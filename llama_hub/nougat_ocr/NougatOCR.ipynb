{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UKZt6CeKFZOX"
      },
      "outputs": [],
      "source": [
        "!pip install -qU nougat-ocr llama-index langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os"
      ],
      "metadata": {
        "id": "RGDlBc96F53C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ZK6hXllMGUbn",
        "outputId": "4260b488-9d28-4e8a-bfb2-51970e280f3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4174bd1a-3b07-451f-9118-1786abf64580\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4174bd1a-3b07-451f-9118-1786abf64580\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving mathpaper.pdf to mathpaper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the ocr\n",
        "!nougat --markdown pdf '/content/mathpaper.pdf' --out 'answer'"
      ],
      "metadata": {
        "id": "lyE4V2kBVX-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edaf780-4e79-42fb-f493-14ec7258d383"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-26 21:41:18.725355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "downloading nougat checkpoint version 0.1.0-small to path /root/.cache/torch/hub/nougat-0.1.0-small\n",
            "config.json: 100% 557/557 [00:00<00:00, 3.41Mb/s]\n",
            "pytorch_model.bin: 100% 956M/956M [00:18<00:00, 54.1Mb/s]\n",
            "special_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 559kb/s]\n",
            "tokenizer.json: 100% 2.04M/2.04M [00:00<00:00, 14.8Mb/s]\n",
            "tokenizer_config.json: 100% 106/106 [00:00<00:00, 725kb/s]\n",
            "INFO:root:Output directory does not exist. Creating output directory.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0% 0/10 [00:00<?, ?it/s]WARNING:root:Found repetitions in sample 2\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "INFO:root:Processing file /content/mathpaper.pdf with 37 pages\n",
            "WARNING:root:Skipping page 3 due to repetitions.\n",
            "100% 10/10 [04:33<00:00, 27.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Define the command as a list of individual arguments\n",
        "cli_command = [\n",
        "    'nougat',\n",
        "    '--markdown',\n",
        "    'pdf',\n",
        "    '/content/mathpaper.pdf',\n",
        "    '--out',\n",
        "    'output'\n",
        "]\n",
        "\n",
        "# Run the command and capture its output\n",
        "try:\n",
        "    result = subprocess.run(cli_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Check the return code to determine success or failure\n",
        "    if result.returncode == 0:\n",
        "        print(\"Command executed successfully\")\n",
        "    else:\n",
        "        print(f\"Command failed with return code {result.returncode}\")\n",
        "\n",
        "    # Print the command's stdout and stderr\n",
        "    print(\"Standard Output:\")\n",
        "    print(result.stdout)\n",
        "\n",
        "    print(\"Standard Error:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Command failed with return code {e.returncode}: {e.stderr}\")\n"
      ],
      "metadata": {
        "id": "-bNmibC4Wqav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df67e295-8b63-4b95-fcf4-5a7818b6019b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Command executed successfully\n",
            "Standard Output:\n",
            "\n",
            "Standard Error:\n",
            "2023-09-26 21:46:41.047597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:root:Output directory does not exist. Creating output directory.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]WARNING:root:Found repetitions in sample 2\n",
            "INFO:root:Processing file /content/mathpaper.pdf with 37 pages\n",
            "WARNING:root:Skipping page 3 due to repetitions.\n",
            "\n",
            " 10%|█         | 1/10 [00:25<03:52, 25.82s/it]\n",
            " 20%|██        | 2/10 [00:52<03:28, 26.11s/it]\n",
            " 30%|███       | 3/10 [01:09<02:33, 21.98s/it]\n",
            " 40%|████      | 4/10 [01:41<02:35, 25.97s/it]\n",
            " 50%|█████     | 5/10 [02:11<02:18, 27.67s/it]\n",
            " 60%|██████    | 6/10 [02:38<01:48, 27.14s/it]\n",
            " 70%|███████   | 7/10 [03:13<01:29, 29.96s/it]\n",
            " 80%|████████  | 8/10 [03:44<01:00, 30.30s/it]\n",
            " 90%|█████████ | 9/10 [04:19<00:31, 31.74s/it]\n",
            "100%|██████████| 10/10 [04:25<00:00, 23.62s/it]\n",
            "100%|██████████| 10/10 [04:25<00:00, 26.52s/it]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now preparing for LlamaHub loader"
      ],
      "metadata": {
        "id": "kAvhtPUIMF3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "from llama_index.readers.base import BaseReader\n",
        "from llama_index.readers.schema.base import Document\n",
        "\n",
        "class PDFNougatOCR(BaseReader):\n",
        "    def nougat_ocr(self, file_path: Path) -> str:\n",
        "        cli_command = [\n",
        "            'nougat',\n",
        "            '--markdown',\n",
        "            'pdf',\n",
        "            str(file_path),\n",
        "            '--out',\n",
        "            'output'\n",
        "            '--pages'\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                cli_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
        "            )\n",
        "            result.check_returncode()\n",
        "            return result.stdout\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            # Log the error message and raise a custom exception\n",
        "            logging.error(f\"Nougat OCR command failed with return code {e.returncode}: {e.stderr}\")\n",
        "            raise RuntimeError(\"Nougat OCR command failed.\") from e\n",
        "\n",
        "    def load_data(\n",
        "        self,\n",
        "        file_path: Path,\n",
        "        extra_info: Optional[Dict] = None\n",
        "    ) -> List[Document]:\n",
        "        try:\n",
        "            # Ensure the 'output' folder exists or create it if not\n",
        "            output_folder = Path('output')\n",
        "            output_folder.mkdir(exist_ok=True)\n",
        "\n",
        "            # Call the method to run the Nougat OCR command\n",
        "            output = self.nougat_ocr(file_path)\n",
        "\n",
        "            # Rest of your code for reading and processing the output\n",
        "            file_path = Path(file_path)\n",
        "            output_path = output_folder / f'{file_path.stem}.mmd'\n",
        "            with output_path.open('r') as f:\n",
        "                content = f.read()\n",
        "\n",
        "\n",
        "            content = (\n",
        "                content.replace(r\"\\(\", \"$\")\n",
        "                .replace(r\"\\)\", \"$\")\n",
        "                .replace(r\"\\[\", \"$$\")\n",
        "                .replace(r\"\\]\", \"$$\")\n",
        "            )\n",
        "\n",
        "            # !!! Need to chunk the before creating Document object here !!!\n",
        "\n",
        "            return [Document(text=content)]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred while processing the PDF: {str(e)}\")"
      ],
      "metadata": {
        "id": "Tq3KuyM3O7Bb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PDFNougatOCR()\n",
        "\n",
        "data = reader.load_data('/content/mathpaper.pdf')"
      ],
      "metadata": {
        "id": "NNIJFZShynFB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n"
      ],
      "metadata": {
        "id": "OGStjdSz1ZL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9311dd8f-428e-4f21-e8b0-9cda56680242"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id_='8c0d2200-4f94-4560-8f41-b431f9c9cbb9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='25ecf3c54a8d35289dffd13db6f86fe31200d7294ae3048d645c315d7bcf8a98', text='# On ranking _via_ sorting by estimated expected utility\\n\\n Clement Calauzenes\\n\\nCriteo AI Lab\\n\\nParis, France\\n\\nc.calauzenes@criteo.com &Nicolas Usunier\\n\\nFacebook AI Research\\n\\nParis, France\\n\\nusunier@fb.com\\n\\n###### Abstract\\n\\nRanking tasks are defined through losses that measure trade-offs between different desiderata such as the relevance and the diversity of the items at the top of the list. This paper addresses the question of which of these tasks are asymptotically solved by sorting by decreasing order of expected utility, for some suitable notion of utility, or, equivalently, _when is square loss regression consistent for ranking_ via _score-and-sort?_ We answer to this question by finding a characterization of ranking losses for which a suitable regression is consistent. This characterization has two strong corollaries. First, whenever there exists a consistent approach based on convex risk minimization, there also is a consistent approach based on regression. Second, when regression is not consistent, there are data distributions for which consistent surrogate approaches necessarily have non-trivial local minima, and for which optimal scoring function are necessarily discontinuous, even when the underlying data distribution is regular. In addition to providing a better understanding of surrogate approaches for ranking, these results illustrate the intrinsic difficulty of solving general ranking problems with the score-and-sort approach.\\n\\n## 1 Introduction\\n\\nThe usual approach in learning to rank is to score each item (e.g., a document) given the input (e.g., a search query), and produce the ranking by sorting in decreasing order of scores. This score-and-sort approach follows the probability ranking principle of information retrieval [29], which stipulates that documents should be rank-ordered according to their estimated probability of relevance to the query.\\n\\nIn practice, the definition of a \"good\" ranking requires more than estimates of relevance. For instance, in scenarios where several users issue the same query \"jaguar\" but with different intended meanings (e.g., the animal or the car brand), it is desirable to produce diverse rankings where each user finds a relevant document as early as possible. While in the probability ranking principle, items are ranked in decreasing order of their _expected utility_ to the users, in sophisticated ranking tasks with a trade-off between relevance and diversity, the definition of a utility per item is not trivial, if not impossible.\\n\\nIn this paper, we study what ranking tasks are solved via sorting by expected utilities, in a general supervised ranking framework that captures different types of ground-truth signal and losses. Since utilities can serve as target values to learn the scoring function through square loss regression, the optimality of sorting by expected utilities is equivalent to the consistency of regression. The main question we address is thus: _When is square loss regression consistent for ranking_ via _score-and-sort?_\\n\\nThe consistency of regression for ranking, and more generally the consistency of convex risk minimization for ranking, are still only partly understood. Existing consistency results only apply to gain-based losses such as the Discounted Cumulative Gain and precision and recall at $K$[12, 28, 6, 19, 14], for which there is an explicit utility function [see e.g., 8, Table 1]. For other losses, only impossibility results are known. Duchi et al. [15], followed by Calauzenes et al. [7] and Ramaswamy et al. [27] proved that convex approaches are, in general, inconsistent with the usual loss when ranking frompairwise preferences, as well as with the Expected Reciprocal Rank [9] and the well-known Average Precision [22], two metrics that are respectively diversity-inducing and diversity-averse [10]. Thus, extending our analysis to general convex risk minimization, two questions remain open: _i) Are there ranking losses for which regression is not consistent, but for which there exists a consistent convex risk minimization approach?_ and _ii) When only non-convex surrogate approaches are consistent, is the learning or optimization problem intrinsically more difficult?_\\n\\nIn Section 3, we describe our main result: sorting by expected utilities is optimal if and only if the sublevel sets of the ranking loss are connected, for a suitable notion of connectedness in the space of permutations. This result identifies the fundamental property of ranking losses that is related to the existence of a consistent regression approach. We give an intuitive interpretation of this result in terms of continuity of optimal scoring functions: when regression is not consistent, there necessarily exist data distributions with continuous conditional distributions $x\\\\mapsto P(y|x)$ but such that all optimal scoring functions are discontinuous. On the other hand, when there is a utility function, expected utilities inherit the regularity properties of $x\\\\mapsto P(y|x)$.\\n\\nIn Section 4, we elaborate on the two types of ranking tasks and discuss surrogate approaches. We first answer question _i)_ and show that for every ranking loss, whenever a convex risk minimization approach is consistent, then a suitable regression is consistent. Second, we give elements of answer to question _ii)_: when surrogate losses need be non-convex to be consistent, we show that every Lipschitz surrogate loss must have bad local minima.\\n\\nOur results establish that the general class of convex surrogate losses cannot solve more tasks than plain regression. This clarifies that sophisticated convex approaches for ranking should be justified by better sample complexity more than \"better fit\" to a specific ranking task, since asymptotically they are either inconsistent or equivalent to a suitable regression. Moreover, the necessary non-global minima of surrogate losses and the discontinuity of optimal scoring are the first formal arguments for the intrinsic difficulties that arise when using score-and-sort for general ranking problems.\\n\\n## 2 Preliminaries: Learning to rank and Consistency\\n\\n### Learning to Rank\\n\\nSupervised learning to rank.We consider a framework of label or subset ranking [13; 12]. The learner predicts rankings over $n$ items based on input features $x\\\\in\\\\mathcal{X}\\\\subseteq\\\\mathbb{R}^{d}$, where $\\\\mathcal{X}$ has nonempty interior. Rankings are represented by permutations, and we denote by $\\\\mathfrak{S}_{n}$ the set of all permutations of $[n]=\\\\{1,\\\\ldots,n\\\\}$. The learner has access to a supervision signal in $\\\\mathcal{Y}$. When the task is fully supervised, $\\\\mathcal{Y}=\\\\mathfrak{S}_{n}$, but we also allow for weakly supervised settings where a supervision $y\\\\in\\\\mathcal{Y}$ is a vector of relevance judgements for each item, a preference graph, or a partial ranking. Our analysis is agnostic to the type of supervision, we only assume that $\\\\mathcal{Y}$ is finite. The task loss $L:\\\\mathcal{Y}\\\\times\\\\mathfrak{S}_{n}\\\\rightarrow\\\\mathbb{R}$ measures the quality of a ranking given the supervision. The goal is to learn, from supervised training data, a ranking function $h:\\\\mathcal{X}\\\\rightarrow\\\\mathfrak{S}_{n}$ with low _task risk_$\\\\mathcal{R}_{L,P}(h)=\\\\mathbb{E}_{P}\\\\big{[}L(Y,h(X))\\\\big{]}$, where the expectation is taken according to the data distribution $P$. We give later examples of usual task losses and their associated $\\\\mathcal{Y}$ in Table 1 (Section 3).\\n\\nThe score-and-sort approach.A usual approach to learning to rank is to sort the items by decreasing order of learnt scores. Given a vector of scores $s\\\\in\\\\mathbb{R}^{n}$, $\\\\operatorname{argsort}(s)$ returns _the set_ of permutations that are compatible with a decreasing order of score:\\n\\n$$\\\\operatorname{argsort}(s)=\\\\left\\\\{\\\\sigma\\\\in\\\\mathfrak{S}_{n}:\\\\forall k\\\\in[n- 1],s_{\\\\sigma(k)}\\\\geq s_{\\\\sigma(k+1)}\\\\right\\\\}.$$\\n\\nWe overload $L$ for a set of rankings $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$, using its average value $L(y,\\\\pi)=\\\\frac{1}{|\\\\pi|}\\\\sum_{\\\\sigma\\\\in\\\\pi}L(y,\\\\sigma)$. Thus, given a _scoring function_$f$, i.e., a measurable function $f:\\\\mathcal{X}\\\\rightarrow\\\\mathbb{R}^{n}$, the task risk for the score-and-sort approach is $\\\\mathcal{R}_{L,P}(\\\\operatorname{argsort}\\\\circ f)=\\\\mathbb{E}_{P}\\\\big{[}L(Y, \\\\operatorname{argsort}(f(X))\\\\big{]}$.\\n\\nSimilarly to previous studies on consistency for ranking [e.g., 12; 15; 28; 7; 8; 26], $x$ contains the information about the input and all items. This is the natural setup when ranking class labels in a multiclass/multilabel setting, since in that case there are only input features (e.g., an image). In recommender systems or search engines, this means that we allos the score of an item to depend on the other available items. This setup makes sure that every ranking function can be implemented by the score-and-sort approach. The difficulty of learning to rank comes from the requirement to learn the scoring function from noisy supervision, instead of from a deterministic ground-truth ranking.\\n\\n[MISSING_PAGE_FAIL:3]\\n\\n* _One distribution over_ $\\\\mathcal{Y}$ _strictly prefers item_ $i$ _over other items:_ $\\\\forall i\\\\in[n],\\\\ \\\\exists q_{\\\\mathrm{top}}^{(i)}\\\\in\\\\mathcal{Q}$ _s.t.:_\\n\\n1. $\\\\ell(q_{\\\\mathrm{top}}^{(i)},.)$ _strictly decreases with the rank of_ $i$_:_ $$\\\\forall\\\\sigma,\\\\nu\\\\in\\\\mathfrak{S}_{n},\\\\left(\\\\sigma^{-1}(i)<\\\\nu^{-1}(i)\\\\Rightarrow \\\\ell(q_{\\\\mathrm{top}}^{(i)},\\\\sigma)<\\\\ell(q_{\\\\mathrm{top}}^{(i)},\\\\nu)\\\\right)\\\\,,$$\\n2. _Ranks of items other than_ $i$ _do not matter:_ $$\\\\forall\\\\sigma\\\\in\\\\mathfrak{S}_{n},\\\\forall j\\\\neq i\\\\in[n],\\\\forall j^{\\\\prime}\\\\neq i \\\\in[n],\\\\ \\\\ell(q_{\\\\mathrm{top}}^{(i)},\\\\sigma)=\\\\ell(q_{\\\\mathrm{top}}^{(i)},\\\\tau_{jj^{ \\\\prime}}\\\\sigma)\\\\,.$$\\n\\nThe first point of the definition is always satisfied in practice since indexes of items do not carry any meaning. In usual ranking tasks, $q_{\\\\mathrm{top}}^{(i)}$ is trivial. For instance, when the supervision is a vector of relevance judgements, $q_{\\\\mathrm{top}}^{(i)}$ is a Dirac on $y$ that gives a high relevance to $i$ and the same lower relevance to all other items. The only practical restriction of this definition is the requirement of _strict_ improvement in the definition of $q_{\\\\mathrm{top}}^{(i)}$. It is satisfied by some notions of DCG, the AP and the ERR, but not by task losses for $K$-subset selection and top-$K$ ranking tasks (in these tasks, ranks of $i$ larger than $K$ lead to the same loss). In the appendix, we extend all our results to a more general case that captures selection and top-$k$ ranking tasks (see Appendix A for the general setup). In the main paper, we focus on ranking only to keep the exposition simple.\\n\\nFor ranking losses, calibration is an \"equality of argmins\" instead of an inclusion:\\n\\n**Theorem 4** ([7, Th. 2]).: _Given a ranking loss $L$, $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to\\\\mathbb{R}_{+}$ is $L$-calibrated if and only if_\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall\\\\delta\\\\in(0,\\\\delta_{0}], \\\\ \\\\ \\\\operatorname*{argmin}_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\ell(q,\\\\sigma)=\\\\bigcup_{s: \\\\phi(q,s)-\\\\underline{\\\\phi}(q)<\\\\delta}\\\\operatorname*{argsort}(s)$$\\n\\nThis equality means that for $L$-calibrated surrogate losses, properties of sublevel sets of the inner risk $isurrogate$ translate into similar properties on $\\\\ell$ and vice-versa. The next section characterizes the sublevel sets of ranking losses that are CEU, while Section 4 focuses on calibrated surrogate losses.\\n\\n## 3 Utility functions and connectedness\\n\\nThis section studies sublevel sets of inner ranking risks in terms of their connectedness:\\n\\n**Definition 5**.: _A set $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ is connected if there is a connected set $S\\\\subseteq\\\\mathbb{R}^{n}$ s.t. $\\\\pi=\\\\bigcup\\\\limits_{s\\\\in S}\\\\operatorname*{argsort}(s)$._\\n\\nThe relationship with topological notion of connectedness is given in Appendix A.\\n\\nIn our main result below, we denote by $\\\\operatorname*{lev}_{\\\\epsilon}\\\\ell(q,.)$ the strict $\\\\epsilon$-sublevel set of the excess inner risk at $q$: $\\\\operatorname*{lev}_{\\\\epsilon}\\\\ell(q,.)=\\\\{\\\\sigma:\\\\ell(q,\\\\sigma)-\\\\min_{\\\\sigma^{ \\\\prime}}\\\\ell(q,\\\\sigma^{\\\\prime})<\\\\epsilon\\\\}$.\\n\\n**Theorem 6**.: _For a ranking loss $L$, the following statements are equivalent:_\\n\\n1. $L$ _is CEU,_\\n2. $\\\\forall q,\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$ _is connected._\\n3. $\\\\forall\\\\epsilon>0,\\\\forall q,\\\\operatorname*{lev}_{\\\\epsilon}\\\\ell(q,.)$ _is connected,_\\n\\n_Moreover, the function $\\\\tilde{u}:\\\\mathcal{Y}\\\\to\\\\mathbb{R}^{n}$ defined as: $\\\\forall i\\\\in[n],\\\\tilde{u}_{i}(y)=-\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}} \\\\mathbbm{1}_{\\\\{\\\\sigma(1)=i\\\\}}L(y,\\\\sigma)$_\\n\\n_is a utility function for $L$ whenever there exists a utility function for $L$ (i.e., whenever $L$ is CEU)._\\n\\nThe proof is given in Appendix C. The theorem has four parts: the existence of the utility function, the connected argmins, and the connected sublevel sets, which in turn give an explicit formula for a utility function for $L$ when one exists. We discuss these four aspects in more detail below.\\n\\n### Disconnected argmins and discontinuity of optimal scoring functions\\n\\nWhile the connectedness of argmins and of sublevel sets is the fundamental concept underlying our result, we first give a more intuitive interpretation of the connectedness of argmins (point _$ii$_) of Th. 6) in terms of the continuity of optimal predictors. The full proof is in Appendix D.\\n\\n**Corollary 7**.: _A ranking loss $L$ is CEU if and only if: for every distribution $P$ over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$ such that $x\\\\mapsto P(.|x)$ is continuous, there is a continuous optimal scoring function for $\\\\mathcal{R}_{L,P}$._\\n\\nThe result follows from the preservation of connectedness by continuous functions. If a ranking loss has a utility function $u$, an optimal scoring function is $x\\\\mapsto\\\\mathbb{E}_{P(.|x)}[u(Y)]$, which inherits all the regularity properties of $x\\\\mapsto P(.|x)$ since $\\\\mathcal{Y}$ is finite. The value of Cor. 7 is thus to show that _only_ CEU losses always have continuous optimal scoring functions when $x\\\\mapsto P(.|x)$ is continuous.\\n\\nSketch of proof.: The \"only if\" direction is straightforward. The full proof of the \"if\" direction is deferred to the appendix. The main line is the following: take a ranking loss $L$ such that there is a continuous optimal scoring function for every distribution such that $P(.|x)$ is continuous. Aiming for a contradiction, assume that $L$ is not CEU. Then, by Theorem 6, there is a $q$ such that $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$ is disconnected. Taking $\\\\mathcal{X}=[0,1]$ without loss of generality and using a uniform marginal distribution over $\\\\mathcal{X}$, we construct a conditional distribution $P(.|x)$ that continuously goes between two distributions $q_{0}$ and $q_{1}$, where each one of $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q_{0},\\\\sigma)$ and $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q_{1},\\\\sigma)$ are included in different connected components of $q$. In that construction, $q$ itself is used as an intermediate point between $q_{0}$ and $q_{1}$ to make sure that $x\\\\mapsto P(.|x)$ is continuous. If this distribution over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$ had a continuous optimal scoring function, then this continuous function would connect two distinct connected components of $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$, which is a contradiction. \\n\\nThis interpretation of Theorem 6 in terms of continuous optimal predictors indicates that CEU and non-CEU losses lead to learning problems of different intrinsic difficulty. We elaborate more on this dichotomy between CEU and non-CEU ranking losses in the next subsection and in Section 4.\\n\\n### Disconnected sublevel sets and bad local minima\\n\\nWe now interpret the connectedness of sublevel sets in terms of local minima of the ranking loss. A permutation is a local minimum if exchanging two adjacent items only increases the loss:\\n\\n**Definition 8**.: _Given a distribution $q\\\\in\\\\mathcal{Q}$ and a loss $L$, a ranking $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$ is a local minimum if for any $r\\\\in[n-1]$, $\\\\ell(q,\\\\sigma)\\\\leq\\\\ell(q,\\\\sigma\\\\tau_{r,r+1})$._\\n\\nThe relationship with the connectedness of sublevel sets is more apparent with this characterization of connectedness in $\\\\mathfrak{S}_{n}$ (the proof is in Appendix C):\\n\\n**Proposition 9**.: _A set $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ is connected if and only if for every $\\\\sigma,\\\\nu\\\\in\\\\pi$ there is path between $\\\\sigma$ and $\\\\nu$ in $\\\\pi$ using transpositions of adjacent items, i.e. $\\\\exists\\\\sigma_{0}=\\\\sigma,\\\\sigma_{1},...,\\\\sigma_{M}=\\\\nu$ where $\\\\forall m,\\\\exists k:\\\\sigma_{m+1}=\\\\sigma_{m}\\\\tau_{kk+1}$ and $\\\\forall m,\\\\sigma_{m}\\\\in\\\\pi$._\\n\\nFigure 1: Graphical representation of level sets of the DCG (left) and the ERR (right) for three items. (normalized in [0,1], lower is better). For the DCG, the distribution over $\\\\mathcal{Y}$ is a Dirac on $(1,2,0)$. For the ERR it is a mixture of a Dirac on $(1,1,0)$ and on $(0,0,1)$ with weights $\\\\frac{7}{12}$ and $\\\\frac{5}{12}$. Each axis is the score of one item, and each region is the corresponding argsort. Two regions share an edge (i.e., are adjacent)if the permutations differ only by a transposition of adjacent items. A subset of regions is _connected_ if it is possible to stay in that subset while moving from one region to the other using a path of adjacent regions. The sublevel sets for the DCG are all connected, but for the ERR, the sublevel set highlighted in dark green has two connected components.\\n\\nThat is, in a connected set $\\\\pi$, we can go from one ranking to another by iteratively swapping adjacent items (as in the bubble sort algorithm), while staying in $\\\\pi$. A graphical representation of connected and disconnected sets is given in Figure 1.\\n\\nWhen $L$ is CEU, then by point _iii)_ of Th. 6, all sublevel sets are conected, which means that for any ranking in a sublevel set, we can find a path using transpositions of adjacent items to an optimal ranking that never increases the loss. Thus, for CEU losses, all local minima are global minima.\\n\\nConversely, when $L$ is not CEU, then for some distributions $q$, the inner ranking risk $\\\\ell$ has disconnected sublevel sets, leading to bad local minima. We show in Appendix E that the set $\\\\{q\\\\in\\\\mathcal{Q}:q\\\\text{ has at least one non-global local minimum}\\\\}$ has non-zero Lebesgue measure for all such $L$. In order to give concrete numbers, we consider the Expected Reciprocal Rank (ERR) [9], which is not CEU as we see later in Section 3.3. By uniformly sampling $q\\\\in\\\\mathcal{Q}$, we empirically estimated proportions of distributions $q$ vs number of local minima for $\\\\ell(q,.)$, for different numbers of items $n$. The results are plotted in Fig. 2_(left)_. The probability that $\\\\ell(q,.)$ has several local minima increases rapidly with $n$, which is expected since when $n$ is small a larger proportion of permutations are adjacent to each other. For $n=8$, we found that the inner risk has up to $23$ local minima, with several local minima for nearly $70\\\\%$ of distributions. Fig. 2_(middle)_ displays the sub-optimality of these local minima, showing that $25\\\\%$ of them are more than $10\\\\%$ sub-optimal. Appendix E contains more details and presents additional results for the Average Precision that are qualitatively similar.\\n\\nThe absence of local minima in $L$ when $L$ is CEU echoes the absence of local minima of the square loss $\\\\Phi_{u}^{\\\\text{suc}}$ that is $L$-calibrated. The result is far from trivial though, because calibration is only a property of minimizers of the inner ranking risk. The strength of our result is to understand that connectedness of argmins actually implies connectedness of all sublevel sets.\\n\\n### Utilities, diverse rankings and disconnected argmins\\n\\nWe now discuss Theorem 6 in light of prior works. First, it is well-known [12; 6; 28] that gain-based metrics of the form $\\\\operatorname{DCG}_{w,u}(y,\\\\sigma)=-\\\\sum_{k=1}^{n}w_{k}u_{\\\\sigma(k)}(y)$, which include the Discounted Cumulative gain or precision/recall at $K$ have $u$ as a utility function. We provide example formulas for well-known ranking losses in Table 1, more examples can be found in [see e.g., 8, Table 1]. We show in Appendix F that $u$ is equal to $\\\\tilde{u}$ given in Th. 6 up to an affine transformation.\\n\\nConversely, it is also known that the ERR and the AP are not CEU, because of their non-neutral behavior with respect to diverse rankings [6; 7; 26]. To illustrate the effect of diversity on connectedness of the argmins, we use the examples of Calauzenes et al. [7, Table 2]. Let us consider a fictional search engine scenario depicted in Figure 2 (right), in which the ambiguous query \"jaguar\" is interpreted differently by two annotators (e.g., as the animal, or as the car brand) leading to noise in the supervision. Optimal rankings for a diversity-inducing loss (e.g., the ERR) alternate items relevant for each interpretation, while for a diversity-averse loss (e.g., the AP), they cluster items relevant to the same interpretation. In both cases, we cannot find a path between optimal rankings by swapping adjacent items without leaving the argmin, so the argmin is not connected.\\n\\nFigure 2: _(left)_ Proportions of distributions $q\\\\in\\\\mathcal{Q}$ ($y$-axis) vs number of local minima (colors) vs number of items ($x$-axis) for the ERR. For $n=8$, nearly $70\\\\%$ of distributions have several local minima. _(middle)_ distribution of sub-optimality of the sub-optimal local minima for the ERR for $n=4$. The sub-optimality of a local minimum is $\\\\frac{\\\\operatorname{value-min}}{\\\\max-\\\\min}$ in $\\\\%$. $0.25\\\\%$ of local minima are more than $90\\\\%$ sub-optimal. _(right)_ Illustration of optimal rankings for the ERR (diversity-inducing) and the AP (diversity-averse), for the fictional search engine scenario with the ambiguous query “jaguar”.\\n\\nThe works above studied convex approaches for ranking. As we explain next, compared to these works, the value of Th. 6 is to show that only CEU ranking losses have a convex, calibrated surrogate.\\n\\n## 4 Calibrated Surrogate Losses\\n\\nWhen a ranking loss $L$ has a utility function $u$, the square loss $\\\\Phi_{u}^{\\\\text{sq}}:y,s\\\\mapsto(s-u(y))^{2}$ is $L$-calibrated. Many other convex surrogate losses are applicable when a utility function is available: losses based on a general Bregman divergence $D$ of the form $y,s\\\\mapsto D(\\\\tilde{u}(y),\\\\psi(s))$ where $\\\\psi$ is a link function [28], pointwise losses such as $y,s\\\\mapsto\\\\sum_{i}\\\\tilde{u}_{i}(y)\\\\log(1+e^{-s_{i}})+\\\\lambda\\\\sum_{i}\\\\log(1+e^ {s_{i}})$ where $\\\\lambda>0$ is a hyperparameter, as well as pairwise losses such as $y,s\\\\mapsto\\\\sum_{i,j}u_{i}(y)\\\\log(1+e^{s_{j}-s_{i}})$[8]. These losses are _convex_, in the sense that $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\mathbb{R}_{+}$ is convex when every $y\\\\in\\\\mathcal{Y}$, the function $s\\\\mapsto\\\\Phi(y,s)$ is convex. Also, for all of them, the minimizers of the inner risk are equal to expected utilities up to a strictly monotonic transform. These losses are well understood, and come with numerous guarantees, such as explicit excess risk bounds for gain-based losses [12, 28, 8]. There are also fast rates of estimation of the scoring function when $x\\\\mapsto P(.|x)$ is smooth [see e.g., 4].\\n\\nOur main result above, Theorem 6,implies that whenever there exists a convex, calibrated loss, then these utility-based surrogate losses are calibrated (if given the suitable utility function):\\n\\n**Theorem 10**.: _Given a ranking loss $L$, there is an $L$-calibrated convex loss if and only if $L$ is CEU._\\n\\nProof of Theorem 10.: The reverse implication is straightforward: if $L$ admits a utility $u$, then $y,s\\\\mapsto(s-u(y))^{2}$ is convex and $L$-calibrated. For the direct implication, let $q\\\\in\\\\mathcal{Q}$. Since $\\\\phi(q,.)$ is convex, its sublevel sets are all convex. Thus, if $\\\\Phi$ is $L$-calibrated, then $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$ is generated by a convex (and thus connected) set of scores by Theorem 4. By the definition of connectedness (Def. 5) $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$ is connected. This holds for all $q$, so $L$ admits a utility function by Theorem 6. \\n\\nThis result establishes the fundamental role of utilities and regression in learning to rank with convex risk minimization: for non-CEU ranking losses, only non-convex surrogate losses are calibrated. For instance, it implies that sophisticated convex approaches such as structural SVMs for ranking [see e.g., 21, 39] do not asymptotically solve more tasks than regression.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l l l l} \\\\hline \\\\hline $\\\\mathcal{Y}$ & Loss & Formula & $u_{i}(y)$ \\\\\\\\ \\\\hline \\\\multirow{3}{*}{$y\\\\in\\\\{0,1\\\\}^{n}$} & Prec@K & $-\\\\sum\\\\limits_{k=1}^{K}\\\\frac{y_{\\\\sigma(k)}}{K}$ & $y_{i}$ \\\\\\\\  & AP & $\\\\frac{-1}{\\\\|y\\\\|_{1}}\\\\sum\\\\limits_{i:y_{i}=1}\\\\operatorname*{Prec@}\\\\sigma^{-1}(i)(y,\\\\sigma)$ & $\\\\times$ \\\\\\\\  & AUC & $\\\\sum\\\\limits_{\\\\begin{subarray}{c}i:y_{i}=1\\\\\\\\ j:y_{j}=0\\\\end{subarray}}\\\\frac{1_{\\\\{\\\\sigma^{-1}(j)\\\\}\\\\sigma^{-1}(j)\\\\}}{\\\\|y\\\\|_{1}(n-\\\\|y\\\\|_{ 1})}$ & $\\\\frac{y_{i}}{\\\\|y\\\\|_{1}(n-\\\\|y\\\\|_{1})}$ \\\\\\\\ \\\\hline \\\\multirow{3}{*}{$y\\\\in\\\\{0,\\\\dots,p\\\\}^{n}$} & DCG@K & $-\\\\sum\\\\limits_{k=1}^{K}\\\\frac{2^{y_{\\\\sigma(k)}}-1}{\\\\log(1+k)}$ & $2^{y_{i}}-1$ \\\\\\\\  & NDCG@K & $\\\\frac{-\\\\operatorname*{DCG@K}(y,\\\\sigma)}{\\\\max\\\\operatorname*{DCG@K}(y,\\\\sigma)}$ & $\\\\frac{2^{y_{i}}-1}{\\\\max\\\\operatorname*{DCG@K}(y,\\\\sigma)}$ \\\\\\\\  & ERR & $-\\\\sum\\\\limits_{k=1}^{K}\\\\frac{R_{\\\\sigma(k)}}{k}\\\\sum\\\\limits_{r=1}^{t-1}(1-R_{ \\\\sigma(r)})\\\\,,\\\\quad R_{i}=\\\\frac{2^{t}-1}{2^{p}}$ & $\\\\times$ \\\\\\\\ \\\\hline $y\\\\in\\\\text{DAG}_{n}$ & PD & $\\\\sum\\\\limits_{i\\\\to j\\\\in y}\\\\mathbbm{1}_{\\\\{\\\\sigma^{-1}(i)>\\\\sigma^{-1}(j)\\\\}}$ & $\\\\times$ \\\\\\\\ \\\\hline \\\\multirow{3}{*}{$y\\\\in\\\\mathfrak{S}_{n}$} & Spearman & $\\\\frac{6}{n(n^{2}-1)}\\\\sum\\\\limits_{i=1}^{n}(\\\\sigma^{-1}(i)-y^{-1}(i))^{2}-1$ & $n-y^{-1}(i)$ \\\\\\\\ \\\\cline{1-1} \\\\cline{2-4}  & & & \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Example of ranking losses with their utilities, if any. We give examples with different types of supervision, including $\\\\text{DAG}_{n}$, which is the set of directed acyclic graphs used in the computation of the pairwise disagreement (PD) studied by Duchi et al. [15].\\n\\n### The case of convex surrogate losses\\n\\nTh. 10 closes the question of the calibration of convex losses for ranking. The question was initiated by Duchi et al. [15, Section 2.1] in the context of learning from pairwise preferences, and then extended to general ranking and classification losses [7, 26].\\n\\nCalauzenes et al. [7] proved that the AP, the ERR and the pairwise disagreement cannot have calibrated, convex surrogate losses because $\\\\operatorname*{argmin}_{\\\\sigma}\\\\ell(q,.,\\\\sigma)$ is not connected for specific choices of $q$. Their result is essentially _i)_$\\\\implies$_ii)_ of our Theorem 6. By proving the reverse implication, we show that _the general class of convex surrogate losses is not more general than square loss regression._ Notice that _i)_$\\\\implies$_ii)_ is an immediate consequence of Th. 4 (also [7, Theorem 2]), while proving the reverse implication is technically much more challenging.\\n\\nRamaswamy and Agarwal [26] defined the concept of _convex calibration dimension_. On the one hand, their approach is more general and they study the minimum number of degrees of freedom that are required for a convex approach to be calibrated. However, they use an unstructured inference procedure, which in general does not correspond to sorting and may be computationally hard. Thus, they do not study when convex losses are calibrated with a score-and-sort approach.\\n\\n### When no convex surrogate loss is calibrated\\n\\nFor non-CEU ranking losses, Th. 10 shows that $L$-calibrated surrogate losses are necessarily non-convex. Additionally, we explained in Sections 3.1 and 3.2 that non-CEU ranking losses have a complex landscape with local minima and discontinuous optimal scoring functions. We now describe analogous undesirable properties for their calibrated (non-convex) surrogate losses.\\n\\nTo illustrate the claims of this section, we perform simulations using a non-convex surrogate loss defined by smoothing the task loss, similarly to [33, 35, 20]. We follow the idea of the probit loss for classification [18], which is a Gaussian smoothing of the 0/1 loss. Here, we use a Gumbel kernel $\\\\kappa$ instead of a Gaussian kernel, because the resulting smoothed loss has a closed form formulation using the Plackett-Luce model [38], and is always $L$-calibrated (a proof is given in Appendix G).\\n\\n**Proposition 11**.: _For any ranking loss $L$, the following surrogate loss $\\\\Phi_{L}^{\\\\text{\\\\sc{NC}}}$ is $L$-calibrated._\\n\\n$$\\\\Phi_{L}^{\\\\text{\\\\sc{NC}}}:y,s\\\\mapsto\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname*{ argsort}(u-s))\\\\kappa(u)\\\\mathrm{d}u=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}L(y, \\\\sigma)\\\\prod_{r=1}^{n}\\\\frac{e^{s_{\\\\sigma(r)}}}{\\\\sum_{k\\\\geq r}e^{s_{\\\\sigma(k)}}}$$\\n\\nExistence of sub-optimal local minima.Th. 12 below is the counterpart of Section 3.2 for surrogate losses. In essence, it shows that when $L$ is not CEU, then bounded, Lipschitz surrogate losses that are $L$-calibrated have bad local minima. In order to deal with surrogate losses like $\\\\Phi_{L}^{\\\\text{\\\\sc{NC}}}$ where critical points are at infinity, we use the following generalization of local minimum. Let $\\\\operatorname*{lev}_{\\\\epsilon}\\\\phi(q,.)$ denote the strict $\\\\epsilon$-sublevel set of the excess inner (surrogate) risk: $\\\\operatorname*{lev}_{\\\\epsilon}\\\\phi(q,.)=\\\\{s\\\\in\\\\mathbb{R}^{n}:\\\\phi(q,s)-\\\\underline {\\\\phi}(q)<\\\\epsilon\\\\}$. Given $\\\\epsilon>0$, a _local valley_ is a connected component of $\\\\operatorname*{lev}_{\\\\epsilon}\\\\phi(q,.)$. A _bad local valley_ is a local valley $C$ such that $\\\\inf_{s\\\\in C}\\\\phi(q,s)>\\\\underline{\\\\phi}(q)$[23, Def. 4.1].\\n\\n**Theorem 12**.: _Let $L$ be a non-CEU ranking loss. Let $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to\\\\mathbb{R}_{+}$ such that $s\\\\mapsto\\\\Phi(y,s)$ is bounded and Lipschitz for every $y\\\\in\\\\mathcal{Y}$. If $\\\\Phi$ is $L$-calibrated, then the set $\\\\{q\\\\in\\\\mathcal{Q}:\\\\phi(q,.)$ has bad local valleys$\\\\}$ has non-zero Lebesgue measure._\\n\\nTo give more precise figures on these local valleys, Fig. 3 (_left and middle_) shows the distribution of sub-optimalities of bad local valleys for the surrogate $\\\\Phi_{L}^{\\\\text{\\\\sc{NC}}}$ applied to the ERR and the AP. These local valleys are found by gradient descent, starting from a relatively large random initialization to analyze the global loss surface. The distributions $q$ are uniformly sampled over $\\\\mathcal{Q}$, rejecting the distributions $q$ where $\\\\ell(q,.)$ does not have any local minima, as for Fig. 2 (_middle_). Fig. 3 (_right_) show the proportions or runs on these distributions that end up stuck in a bad local valley when using an initialization close to 0 (which empirically was best to avoid bad local valleys). As for the ranking losses, we see that the surrogate losses also have \"bad local minima\", but also that gradient descent algorithms might be stuck in them. Details of these experiments are given in Appendix H.\\n\\n(Bad) approximations of optimal scoring functions by Lipschitz functions.In Section 3.1, we showed that optimal scoring functions for non-CEU ranking losses are discontinuous for somedistributions where $x\\\\mapsto P(.|x)$ is continuous. Prop. 13 below is a stronger analogous statement for surrogate losses when $x\\\\mapsto P(.|x)$ is Lipschitz. The proof can be found in Appendix H.\\n\\n**Proposition 13**.: _Let $L$ be a non-CEU ranking loss. Let $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to[0,B_{\\\\Phi}]$ be an $L$-calibrated loss such that $\\\\Phi(y,.)$ is $\\\\beta_{\\\\Phi}$-Lipschitz for all $y\\\\in\\\\mathcal{Y}$._\\n\\n_Then, there exists constants $c,c^{\\\\prime}>0$ and a probability measure $P$ over $[0,1]\\\\times\\\\mathcal{Y}$ where $x\\\\mapsto P(.|x)$ is Lipschitz, such that for all $\\\\beta\\\\geq 0$:_\\n\\n$$\\\\inf_{\\\\begin{subarray}{c}f:[0,1]\\\\to\\\\mathbb{R}^{n}\\\\\\\\ f:\\\\beta-Lipschitz\\\\end{subarray}}\\\\mathcal{R}_{\\\\Phi,P}(f)-\\\\inf_{g:\\\\mathcal{X}\\\\to \\\\mathbb{R}^{n}}\\\\mathcal{R}_{\\\\Phi,P}(g)\\\\geq\\\\min\\\\big{(}c^{\\\\prime},\\\\frac{c}{8B_ {\\\\Phi}+\\\\beta_{\\\\Phi}\\\\beta}\\\\big{)}.$$\\n\\nNotice that for CEU ranking losses with utility function $u$, denoting $\\\\beta_{P}$ the Lipschitz coefficient of $x\\\\mapsto P(.|x)$, the optimal scoring function is $\\\\left\\\\|u\\\\right\\\\|_{\\\\infty}$$\\\\beta_{P}$-Lipschitz. In contrast, for non-CEU losses, the lower bound above shows that for some distributions, the Lipschitz constant of the scoring function $\\\\beta$ needs to grow to infinity to minimize a Lipschitz, calibrated surrogate loss.\\n\\n## 5 Discussion and conclusion\\n\\nFor supervised ranking with the score-and-sort approach, learning the scoring function through regression is consistent for all ranking tasks for which a convex risk minimization approach is consistent. When regression is not consistent, surrogate risks have bad local minima and their minimizers cannot be approximated by regular functions even when $P(.|x)$ is regular. These results demonstrate the fundamental role of regression among convex methods for ranking. They also cast light on the undesirable properties of the score-and-sort approach for non-CEU ranking losses.\\n\\nFor tasks with non-CEU ranking losses, one possible avenue is to develop efficient direct loss minimization approaches, such as approximations of $\\\\Phi_{L}^{\\\\text{sc}}$ above or as proposed by Song et al. [30]. Another direction is to find alternatives to score-and-sort. Ramaswamy and Agarwal [26] developped a more general approach which allows for consistent convex approaches, but in higher dimension than the number of items. Excess risk bounds follow from the work on stuctured prediction [11; 25]. The drawback of these approaches is that inference might be NP-hard [15; 26]. In multilabel classification, in some rare cases, efficient inference procedures have been found [36], but not yet in ranking. A third direction is to relax the requirement of asymptotic optimality. By choosing a sensible but efficient inference procedure and making additional assumptions on the data distribution. Chapelle et al. [10] followed this approach for divesifying search results. For now, the theoretical properties of for such approaches have not been investigated. A possible starting point would be to build on the recent work on excess risk bounds for non-calibrated losses [32].\\n\\nFigure 3: _Left and middle_ Distributions of sub-optimalities of local valleys for the surrogate $\\\\Phi_{L}^{\\\\text{sc}}$ applied to the ERR and the AP on random distributions where the ERR/the AP have bad local minima. _(right)_ percentage of optimization runs based on gradient descent that end up stuck in local minima.\\n\\n## Broader Impact\\n\\nThe framework of ranking we studied plays a fundamental role in information retrieval and information filtering systems. While these systems play an undeniable positive role in society because they increase the efficiency of information access, they also shape the landscape of what their users get to know. This lead to questions regarding equal representation in search engines result [34], how they represent social groups [17], and whether they exacerbate filter bubbles or act as echo chambers [3]. As the most recent trends in learning to rank involve randomized experiments and online learning [16, 1] to improve quality, such user experiments also need be carried out with care [5].\\n\\nCreating a strong theory of learning to rank is important to address the challenges of information access systems. For instance, methods to produce diverse rankings might be part of the solution, and for now the theory of machine learning for diverse rankings is scarse. This paper partly fills this gap.\\n\\n## References\\n\\n* [1] A. Agarwal, K. Takatsu, I. Zaitsev, and T. Joachims. A general framework for counterfactual learning-to-rank. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 5-14, 2019.\\n* [2] F. G. Arenas. Alexandroff spaces. 1999.\\n* [3] E. Bakshy, S. Messing, and L. A. Adamic. Exposure to ideologically diverse news and opinion on facebook. _Science_, 348(6239):1130-1132, 2015.\\n* [4] A. R. Barron. _Complexity Regularization with Application to Artificial Neural Networks_, pages 561-576. Springer Netherlands, Dordrecht, 1991.\\n* [5] S. Bird, S. Barocas, K. Crawford, F. Diaz, and H. Wallach. Exploring or exploiting? social and ethical implications of autonomous experimentation in ai. In _Workshop on Fairness, Accountability, and Transparency in Machine Learning_, 2016.\\n* [6] D. Buffoni, C. Calauzenes, P. Gallinari, and N. Usunier. Learning scoring functions with order-preserving losses and standardized supervision. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, pages 825-832, 2011.\\n* [7] C. Calauzenes, N. Usunier, and P. Gallinari. On the (non-)existence of convex, calibrated surrogate losses for ranking. In _Advances in Neural Information Processing Systems 25_, pages 197-205. 2012.\\n* [8] C. Calauzenes, N. Usunier, and P. Gallinari. Calibration and regret bounds for order-preserving surrogate losses in learning to rank. _Machine learning_, 93(2-3):227-260, 2013.\\n* [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In _Proceedings of the 18th ACM conference on Information and knowledge management_, pages 621-630, 2009.\\n* [10] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: metrics and algorithms. _Information Retrieval_, 14(6):572-592, 2011.\\n* [11] C. Ciliberto, A. Rudi, and L. Rosasco. A consistent regularization approach for structured prediction. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS\\'16, page 4419-4427, 2016.\\n* [12] D. Cossock and T. Zhang. Statistical analysis of bayes optimal subset ranking. _IEEE Transactions on Information Theory_, 54(11):5140-5154, 2008.\\n* [13] O. Dekel, Y. Singer, and C. D. Manning. Log-linear models for label ranking. In _Advances in neural information processing systems_, pages 497-504, 2004.\\n* [14] K. Dembczynski, W. Kotlowski, and E. Hullermeier. Consistent multilabel ranking through univariate losses. _arXiv preprint arXiv:1206.6401_, 2012.\\n* [15] J. C. Duchi, L. W. Mackey, and M. I. Jordan. On the consistency of ranking algorithms. In _Proceedings of the 27th International Conference on International Conference on Machine Learning_, pages 327-334, 2010.\\n* [16] T. Joachims, A. Swaminathan, and T. Schnabel. Unbiased learning-to-rank with biased feedback. In _Proceedings of the Tenth ACM International Conference on Web Search and Data Mining_, pages 781-789, 2017.\\n\\n* [17] M. Kay, C. Matuszek, and S. A. Munson. Unequal representation and gender stereotypes in image search results for occupations. In _Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems_, pages 3819-3828. ACM, 2015.\\n* [18] J. Keshet and D. A. McAllester. Generalization bounds and consistency for latent structural probit and ramp loss. In _Advances in Neural Information Processing Systems 24_, pages 2205-2212. 2011.\\n* [19] W. Kotlowski, K. Dembczynski, and E. Hullermeier. Bipartite ranking through minimization of univariate loss. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, pages 1113-1120, 2011.\\n* [20] J.-W. Kuo, P.-J. Cheng, and H.-M. Wang. Learning to rank from bayesian decision inference. In _Proceedings of the 18th ACM conference on Information and knowledge management_, pages 827-836, 2009.\\n* [21] Q. Le and A. Smola. Direct optimization of ranking measures. _arXiv preprint arXiv:0704.3359_, 2007.\\n* [22] C. D. Manning, P. Raghavan, and H. Schutze. _Introduction to information retrieval_. Cambridge university press, 2008.\\n* [23] Q. Nguyen. On connected sublevel sets in deep learning. _arXiv preprint arXiv:1901.07417_, 2019.\\n* [24] E. A. Ok. _Real Analysis with Economic Applications_. Number mathecon1 in Online economics textbooks. SUNY-Oswego, Department of Economics, January 2004.\\n* [25] A. Osokin, F. Bach, and S. Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In _Advances in Neural Information Processing Systems_, pages 302-313, 2017.\\n* [26] H. G. Ramaswamy and S. Agarwal. Convex calibration dimension for multiclass loss matrices. _The Journal of Machine Learning Research_, 17(1):397-441, 2016.\\n* [27] H. G. Ramaswamy, S. Agarwal, and A. Tewari. Convex calibrated surrogates for low-rank loss matrices with applications to subset ranking losses. In _Advances in Neural Information Processing Systems_, pages 1475-1483, 2013.\\n* [28] P. Ravikumar, A. Tewari, and E. Yang. On ndcg consistency of listwise ranking methods. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 618-626, 2011.\\n* [29] S. E. Robertson. The probability ranking principle in ir. _Journal of documentation_, 33(4):294-304, 1977.\\n* [30] Y. Song, A. Schwing, R. Urtasun, et al. Training deep neural networks via direct loss minimization. In _International Conference on Machine Learning_, pages 2169-2177, 2016.\\n* [31] I. Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26(2):225-287, 2007.\\n* [32] K. Struminsky, S. Lacoste-Julien, and A. Osokin. Quantifying learning guarantees for convex but inconsistent surrogates. In _Advances in Neural Information Processing Systems_, pages 669-677, 2018.\\n* [33] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In _Proceedings of the 2008 International Conference on Web Search and Data Mining_, pages 77-86, 2008.\\n* [34] L. Vaughan and Y. Zhang. Equal representation by search engines? a comparison of websites across countries and domains. _Journal of computer-mediated communication_, 12(3):888-909, 2007.\\n* [35] M. N. Volkovs and R. S. Zemel. Boltzrank: learning to maximize expected ranking gain. In _Proceedings of the 26th Annual International Conference on Machine Learning_, pages 1089-1096, 2009.\\n* [36] W. Waegeman, K. Dembczynski, A. Jachnik, W. Cheng, and E. Hullermeier. On the bayes-optimality of f-measure maximizers. _Journal of Machine Learning Research_, 15:3333-3388, 2014.\\n\\n* [37] R. Wijsman. Continuity of the Bayes risk. _The Annals of Mathematical Statistics_, 41(3):1083-1085, 1970.\\n* [38] J. I. Yellott Jr. The relationship between luce\\'s choice axiom, thurstone\\'s theory of comparative judgment, and the double exponential distribution. _Journal of Mathematical Psychology_, 15(2):109-144, 1977.\\n* [39] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In _Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval_, pages 271-278, 2007.\\n\\nGeneral Setting\\n\\nDisclaimer: As mentioned in the body of the paper, we address, in this Appendix, a setting more general than ranking. We pay attention to often reinterpret the results with the ranking setting described in the paper to ease the reader\\'s task.\\n\\nIn supervised learning, a learner has access to input features $x\\\\in\\\\mathcal{X}\\\\subseteq\\\\mathbb{R}^{d}$ and wants to predict output variables $z\\\\in\\\\mathcal{Z}$ (finite). To do so, she can learn a prediction function $h:\\\\mathcal{X}\\\\rightarrow\\\\mathcal{Z}$, belonging to the space of measurable functions from $\\\\mathcal{X}$ to $\\\\mathcal{Z}$ denoted $\\\\mathcal{H}$, using some feedback $y\\\\in\\\\mathcal{Y}$ (finite) and minimizing a task loss $L:\\\\mathcal{Y}\\\\times\\\\mathcal{Z}\\\\rightarrow\\\\mathbb{R}$ in expectation over some joint distribution $P$ on the observable variables:\\n\\n$$\\\\inf_{h\\\\in\\\\mathcal{H}}\\\\mathcal{R}_{L,P}(h)\\\\qquad\\\\quad\\\\text{where}\\\\;\\\\;\\\\mathcal{ R}_{L,P}(h)=\\\\mathbb{E}_{P}\\\\left[L(Y,h(X))\\\\right]$$\\n\\nNote that the infimum exists because $L$ is bounded (because $\\\\mathcal{Z}$ and $\\\\mathcal{Y}$ being finite). We denote by $\\\\left\\\\|L\\\\right\\\\|_{\\\\infty}=\\\\max_{y,z}|L(y,z)|$.\\n\\nPrediction space: $\\\\mathcal{Z}$ vs $\\\\mathfrak{S}_{n}$.Intuitively, when predictions are ranking $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$, they cannot encode _indifference_ between items (the ordering is strict). However, all the results still holds when items can be indifferent (the ordering is a weak order). Further, moving from total orders to weak orders allows to extends the range of applications. For instance, multiclass classification can be seen as predicting a weak order amongst those that strictly prefers one item (label) to all the other items, that are indifferent one from each other. Top-k ranking and subset selection can be handled similarly. The prediction space $\\\\mathcal{Z}$ allows to handle the additional formalism (detailed below) required to handle this generalization.\\n\\nTwo notes on notation.For $n\\\\in\\\\mathbb{N}$, we denote $[n]$ the set of integers going from $1$ to $n$. The indicator function is denoted $\\\\mathds{1}_{[\\\\cdot]}$. For $s\\\\in\\\\mathbb{R}^{n}$ and $\\\\varepsilon>0$, $\\\\mathcal{B}_{2}(s,\\\\varepsilon)=\\\\{s^{\\\\prime}\\\\in\\\\mathbb{R}^{n},\\\\left\\\\|s^{\\\\prime }-s\\\\right\\\\|_{2}<\\\\varepsilon\\\\}$. Similarly, $\\\\mathcal{B}_{\\\\infty}(s,\\\\varepsilon)$ denotes the open ball of infinity norm. For $q\\\\in\\\\mathcal{Q}$,the $\\\\left\\\\|.\\\\right\\\\|_{1}$ ball in $\\\\mathcal{Q}$ is denoted by $\\\\mathcal{B}_{1}(q,\\\\varepsilon)=\\\\{q^{\\\\prime}\\\\in\\\\mathcal{Q}:\\\\left\\\\|q-q^{\\\\prime} \\\\right\\\\|_{1}<\\\\varepsilon\\\\}$. Random variable are uppercase version of their realisation counterparts. Given a function $g:\\\\mathcal{U}\\\\rightarrow\\\\mathbb{R}$ bounded below and some $\\\\varepsilon\\\\geq 0$, we define the strict $\\\\varepsilon$-optimal (relative) sub-level set of $g$,\\n\\n$$\\\\operatorname{lev}_{\\\\varepsilon}g=\\\\{u\\\\in\\\\mathcal{U}:g(u)-\\\\inf_{u^{\\\\prime}\\\\in \\\\mathcal{U}}g(u^{\\\\prime})<\\\\varepsilon\\\\}\\\\,.$$\\n\\nFurther, we denote $\\\\operatorname{argmin}g=\\\\operatorname{argmin}_{u\\\\in\\\\mathcal{U}}g(u)$.\\n\\n### Score-and-sort for weak orders ($\\\\mathcal{Z}$)\\n\\nScores and argsortWe consider the case of ranking or selection tasks solved by sorting according to predicted scores. The basic operation we consider, called $\\\\operatorname{argsort}$, is the set-value function that associates to a vector of scores given to items the set of total orders compatible with the scores:\\n\\n$$\\\\operatorname{argsort}:\\\\mathbb{R}^{n} \\\\rightarrow\\\\mathfrak{S}_{n} \\\\tag{1}$$ $$s \\\\mapsto\\\\{\\\\sigma\\\\in\\\\mathfrak{S}_{n}:\\\\forall k\\\\in[n-1],s_{\\\\sigma(k) }\\\\geq s_{\\\\sigma(k+1)}\\\\}\\\\,.$$\\n\\nNote that we consider $\\\\operatorname{argsort}$ as a set-valued function (and not as a function with values in $2^{\\\\mathfrak{S}_{n}}$), so that follows the usual convention, for any subset $S\\\\in\\\\mathbb{R}^{n}$, the notation $\\\\operatorname{argsort}(S)$ is defined as:\\n\\n$$\\\\forall S\\\\subseteq\\\\mathbb{R}^{n},\\\\operatorname{argsort}(S)=\\\\bigcup_{s\\\\in S} \\\\operatorname{argsort}(s)\\\\,.$$\\n\\nTranspositionsWe make heavy use of transpositions: given $i,j$ in $[n]$, the transposition of $i$ and $j$ is the permutation $\\\\tau_{ij}$ such that $\\\\tau_{ij}(i^{\\\\prime})=i^{\\\\prime}$ if $i^{\\\\prime}\\\\not\\\\in\\\\{i,j\\\\}$, $\\\\tau_{ij}(j)=i$ and $\\\\tau_{ij}(i)=j$. Given $\\\\sigma$, we denote by $\\\\tau_{ij}\\\\sigma$ the composition of $\\\\tau_{ij}$ and $\\\\sigma$.\\n\\nGiven $z\\\\subseteq\\\\mathfrak{S}_{n}$, we denote by $\\\\tau_{ij}z=\\\\{\\\\tau_{ij}\\\\sigma:\\\\sigma\\\\in z\\\\}$.\\n\\nNotice the following difference between a subset $z\\\\subseteq\\\\mathfrak{S}_{n}$ and a permutation $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$ when it comes to transpositions: the sequential application of transpositions to a permutation commute and it is an associative operation (because they are all members of the symmetric group). Thatis, if $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$ we have $\\\\tau_{ij}\\\\tau_{i^{\\\\prime}j^{\\\\prime}}\\\\sigma=(\\\\tau_{ij}\\\\tau_{i^{\\\\prime}j^{\\\\prime}} )\\\\sigma=\\\\tau_{i^{\\\\prime}j^{\\\\prime}}\\\\tau_{ij}\\\\sigma$. This is not the case in general when applying sequences of transpositions in $\\\\mathcal{Z}$. For instance let $z=\\\\{(123),(213)\\\\}$. Then $\\\\tau_{12}z=z$, so $\\\\tau_{13}\\\\tau_{12}z=\\\\tau_{13}z=\\\\{(321),(231)\\\\}$, while $\\\\tau_{12}\\\\tau_{13}z=\\\\tau_{12}\\\\{(321),(231)\\\\}=\\\\{(312),(132)\\\\}$. Thus, the composition of a permutation and an element of $\\\\mathcal{Z}$ is not defined in general, it is only defined for transpositions, and these should only be read as usual function compositions.\\n\\nThe decision spaceThe final prediction space corresponds to a selection or partial ranking task, generally speaking, a weak order. The decision space $\\\\mathcal{Z}$ should satisfy the following properties:\\n\\n**(A1)**.: _The decision space $\\\\mathcal{Z}\\\\subseteq 2^{\\\\mathfrak{S}_{n}}$ satisfies_\\n\\n$$\\\\text{(Total preorders)}\\\\;\\\\forall z\\\\in\\\\mathcal{Z},\\\\exists s\\\\in \\\\mathbb{R}^{n}:z=\\\\operatorname{argsort}(s)\\\\subseteq\\\\mathfrak{S}_{n}.$$ $$\\\\text{(Partition of }\\\\mathfrak{S}_{n})\\\\bigcup_{z\\\\in\\\\mathcal{Z}}z= \\\\mathfrak{S}_{n}\\\\quad\\\\text{and}\\\\quad\\\\forall z,w\\\\in\\\\mathcal{Z},(z\\\\neq w) \\\\Rightarrow(z\\\\cap w=\\\\emptyset).$$ $$\\\\text{(Closed under transpositions)}\\\\;\\\\forall i,j,\\\\forall z\\\\in\\\\mathcal{Z}, \\\\tau_{ij}z\\\\in\\\\mathcal{Z}.$$\\n\\nSince $\\\\mathcal{Z}$ partitions $\\\\mathfrak{S}_{n}$, elements of $\\\\mathcal{Z}$ can be seen as equivalence classes on rankings in $\\\\mathfrak{S}_{n}$, and we can define the function $\\\\Lambda$ as the quotient map which assigns a permutation to its representative member of $\\\\mathcal{Z}$:\\n\\n$$\\\\Lambda:\\\\mathfrak{S}_{n} \\\\rightarrow\\\\mathcal{Z}$$ $$\\\\sigma \\\\mapsto z\\\\;\\\\;\\\\text{s.t.}\\\\;\\\\;\\\\sigma\\\\in z\\\\,,$$\\n\\nThe prediction function is then the composition $\\\\Lambda\\\\circ\\\\operatorname{argsort}$:\\n\\n$$\\\\forall s\\\\mathbb{R}^{n},\\\\;\\\\;\\\\operatorname{pred}(s)=\\\\Lambda\\\\circ \\\\operatorname{argsort}(s)=\\\\{\\\\Lambda(\\\\sigma):\\\\sigma\\\\in\\\\operatorname{argsort}( s)\\\\}\\\\,.$$\\n\\nA straightforward consequence of Assumption (A1) that we make use of when discussing connectedness later is that $\\\\Lambda$ is equivariant by transposition1:\\n\\nFootnote 1: proof: $\\\\tau_{ij}\\\\Lambda(\\\\sigma)\\\\in\\\\mathcal{Z}$ because $\\\\mathcal{Z}$ is closed by transpositions. By the definition of $\\\\tau_{ij}z$ for $z\\\\in\\\\mathcal{Z}$, $\\\\tau_{ij}\\\\Lambda(\\\\sigma)$ must contain $\\\\tau_{ij}\\\\sigma$. Since $\\\\mathcal{Z}$ is a partition of $\\\\mathfrak{S}_{n}$, there is a unique element of $\\\\mathcal{Z}$ that contains $\\\\tau_{ij}\\\\sigma$, which is, by definition of $\\\\Lambda$, $\\\\Lambda(\\\\tau_{ij}\\\\sigma)$.\\n\\nNotation for orders and preorderGiven $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$, and $i,j\\\\in[n]$, we use the notation\\n\\n$$i\\\\succ_{\\\\sigma}j\\\\;\\\\;\\\\Leftrightarrow\\\\;\\\\;\\\\sigma^{-1}(i)<\\\\sigma^{-1}(j)\\\\qquad \\\\text{\\'\\'$\\\\sigma$ prefers $i$ to $j$\\'\\'}.$$\\n\\nThis is coherent with an ordering in decreasing order of scores in (1), where $\\\\sigma^{-1}(i)$ is the rank of item $i$ and lower ranks are associated with higher scores (i.e., rank 1 is best), the usual convention in learning to rank.\\n\\nWe use the following notation for the preorder induced by $z$ on $[n]$: $\\\\forall z\\\\in\\\\mathcal{Z},\\\\forall i,j\\\\in[n]$:\\n\\n* $i\\\\succeq_{z}j\\\\;\\\\;\\\\Leftrightarrow\\\\;\\\\;\\\\exists\\\\sigma\\\\in z,i\\\\succ_{\\\\sigma}j$,\\n* $i\\\\succ_{z}j\\\\;\\\\;\\\\Leftrightarrow\\\\;\\\\;\\\\forall\\\\sigma\\\\in z,i\\\\succ_{\\\\sigma}j$,\\n* $\\\\succeq_{z}$ induces _indifference classes_: $i\\\\sim_{z}j\\\\quad\\\\Leftrightarrow\\\\quad\\\\exists\\\\sigma,\\\\sigma^{\\\\prime}\\\\in z,i\\\\succ_{ \\\\sigma}j$ and $j\\\\succ_{\\\\sigma^{\\\\prime}}i$.\\n\\nGiven the properties of $\\\\mathcal{Z}$ (Assumption (A1)), the following properties are straightforward to prove: $\\\\forall z\\\\in\\\\mathcal{Z},\\\\forall s\\\\in\\\\mathbb{R}^{n}$ s.t. $z=\\\\operatorname{argsort}(s),\\\\forall i,j\\\\in[n]$:\\n\\n$$a)i\\\\succeq_{z}j\\\\Leftrightarrow s_{i}\\\\geq s_{j}\\\\quad\\\\;\\\\;\\\\;b)i \\\\succ_{z}j\\\\Leftrightarrow s_{i}>s_{j}\\\\quad\\\\;\\\\;c)i\\\\sim_{z}j\\\\Leftrightarrow s_{i}=s_{j} \\\\tag{2}$$ $$d)i\\\\sim_{z}j\\\\Leftrightarrow\\\\tau_{ij}z=z$$\\n\\na), b), c) above simply mean that our definition of $\\\\succeq_{z}$ is coherent with the preorder induced by the scores. d) is a direct consequence of c). Another noticeable implication of (A1) is that the number of indifference classes is the same for all $z$. This can be seen by combining d) above together with the fact that $z$ is both a partition and stable by transposition.\\n\\n### Assumption on task losses\\n\\nLet $\\\\mathcal{Q}$ be the set of probability mass functions over $\\\\mathcal{Y}$. For any $q\\\\in\\\\mathcal{Q}$, we define the _inner risk_ for the task loss $L$ as\\n\\n$$\\\\forall z\\\\in\\\\mathcal{Z},\\\\ell(q,z)=\\\\sum_{y\\\\in\\\\mathcal{Y}}q(y)L(y,z)\\\\qquad\\\\text{ and}\\\\qquad\\\\underline{\\\\ell}(q)=\\\\min_{z\\\\in\\\\mathcal{Z}}\\\\ell(q,z)$$\\n\\nWe now detail our assumptions that subsumes the _ranking task loss_ definition (Def. 3).\\n\\n**(A2)** (Items are equivalent _a priori_).: $\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall i,j\\\\in[n],\\\\exists q^{\\\\prime}\\\\in\\\\mathcal{Q}$ _s.t._$\\\\forall z\\\\in\\\\mathcal{Z},\\\\ell(q,z)=\\\\ell(q^{\\\\prime},\\\\tau_{ij}z)$_._\\n\\nFor the next assumption, for any $z$ and any item $i$, let us denote the rank of $i$ in the preorder $z$ as:\\n\\n$$z^{-1}(i)=|\\\\{j\\\\in[n]:j\\\\succ_{z}i\\\\}|+1.$$\\n\\nThe $+1$ makes sure this definition of rank matches the definition for permutations (first rank is $1$ rather than $0$).\\n\\n**(A3)** (One distribution over $\\\\mathcal{Y}$ strictly prefers item $i$ over other items).: $\\\\forall i\\\\in[n],\\\\exists q^{(i)}_{\\\\mathrm{top}}\\\\in\\\\mathcal{Q}$ _such that_\\n\\n1. $\\\\forall z,w\\\\in\\\\mathcal{Z},\\\\left(z^{-1}(i)<w^{-1}(i)\\\\Rightarrow\\\\ell(q^{(i)}_{ \\\\mathrm{top}},z)<\\\\ell(q^{(i)}_{\\\\mathrm{top}},w)\\\\right)$__\\n2. $\\\\forall z\\\\in\\\\mathcal{Z},\\\\forall k\\\\neq i,\\\\forall l\\\\neq i,\\\\ell(q^{(i)}_{\\\\mathrm{ top}},z)=\\\\ell(q^{(i)}_{\\\\mathrm{top}},\\\\tau_{kl}z)$__\\n\\nThe following lemma is a straightforward consequence of the three assumptions above:\\n\\n**Lemma 14**.: _Under assumptions (A1), (A2) and (A3), we have:_\\n\\n1. $\\\\forall i\\\\in[n],\\\\exists q^{(i)}_{\\\\mathrm{bottom}}\\\\in\\\\mathcal{Q}$ _such that_ 1. $\\\\forall z,w\\\\in\\\\mathcal{Z},\\\\left(z^{-1}(i)<w^{-1}(i)\\\\Rightarrow\\\\ell(q^{(i)}_{ \\\\mathrm{bottom}},z)>\\\\ell(q^{(i)}_{\\\\mathrm{bottom}},w)\\\\right)$__ 2. $\\\\forall z\\\\in\\\\mathcal{Z},\\\\forall k\\\\neq i,\\\\forall l\\\\neq i,\\\\ell(q^{(i)}_{\\\\mathrm{ bottom}},z)=\\\\ell(q^{(i)}_{\\\\mathrm{bottom}},\\\\tau_{kl}z)$__\\n2. $\\\\forall z\\\\in\\\\mathcal{Z},\\\\exists q^{(z)}\\\\in\\\\mathcal{Q}$ _such that_ $\\\\operatorname{argmin}\\\\ell(q^{(z)},.)=\\\\{z\\\\}$_._\\n\\nProof.: Notice that without loss of generality, using (A2), we can assume $\\\\forall z,\\\\ell(q^{(i)}_{\\\\mathrm{top}},z)=\\\\ell(q^{(j)}_{\\\\mathrm{top}},\\\\tau_{ij}z)$\\n\\nThen, define\\n\\n$$q^{(i)}_{\\\\mathrm{bottom}}=\\\\frac{1}{n-1}\\\\sum_{j\\\\neq i}q^{(j)}_{\\\\mathrm{top}}.$$\\n\\n$q^{(i)}_{\\\\mathrm{bottom}}\\\\in\\\\mathcal{Q}$ by convexity of the simplex. It is then easy to check that $q^{(i)}_{\\\\mathrm{bottom}}$ satisfies the two desired conditions.\\n\\nFor $q^{(z)}$, define $\\\\alpha_{i}=n-z^{-1}(i)$ and $\\\\tilde{\\\\alpha}_{i}=\\\\alpha_{i}/\\\\sum_{j}\\\\alpha_{j}$. Then, define\\n\\n$$q^{(z)}=\\\\sum_{i}\\\\tilde{\\\\alpha}_{i}q^{(i)}_{\\\\mathrm{top}}\\\\,.$$\\n\\n$q^{(z)}\\\\in\\\\mathcal{Q}$ by convexity of the simplex and it is easy to check that $\\\\operatorname{argmin}\\\\ell(q^{(z)},.)=\\\\{z\\\\}$. \\n\\nAn immediate consequence of point _ii)_ of Lemma 14 above, is the following straightforward result, which we use throughout the proof of the main results to reduce all cases to cases where argmins of the task loss are unique:\\n\\n**Lemma 15** (Tie-breaking).: _Under (A1), (A2) and (A3), $\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),\\\\ \\\\exists q^{\\\\prime}\\\\in\\\\mathcal{Q}$ s.t._\\n\\n1. $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)=\\\\{z\\\\}$_,_\\n2. $\\\\forall z_{1},z_{2}\\\\in\\\\mathcal{Z},\\\\ell(q,z_{1})>\\\\ell(q,z_{2})\\\\Rightarrow\\\\ell(q^ {\\\\prime},z_{1})>\\\\ell(q^{\\\\prime},z_{2})$__Proof.: Let $q\\\\in\\\\mathcal{Q}$ and $z\\\\in\\\\operatorname*{argmin}\\\\ell(q,.)$. By point _ii)_ of Lemma 14, we know $\\\\exists q^{(z)}\\\\in\\\\mathcal{Q},\\\\operatorname*{argmin}\\\\ell(q^{(z)},.)=\\\\{z\\\\}$. We denote $q_{\\\\alpha}=(1-\\\\alpha)q+\\\\alpha q^{(z)}$.\\n\\n1. As $z$ is the only element of $\\\\mathcal{Z}$ optimal for both $q$ and $q^{(z)}$, we have that $\\\\forall\\\\alpha\\\\in(0,1],\\\\operatorname*{argmin}\\\\ell(p_{\\\\alpha},.)=\\\\{z\\\\}$.\\n2. Denoting $\\\\epsilon=\\\\min_{z_{1},z_{2}:\\\\ell(q,z_{1})\\\\neq\\\\ell(q,z_{2})}|\\\\ell(q,z_{1})-\\\\ell (q,z_{2})|$, Let $0<\\\\alpha<\\\\frac{\\\\epsilon}{\\\\epsilon+\\\\max_{x^{\\\\prime}}(\\\\ell(q^{(z)},z^{\\\\prime})- \\\\underline{\\\\ell}(q^{(z)})}$. Then, for any $z_{1},z_{2}$ such that $\\\\ell(q,z_{1})>\\\\ell(q,z_{2})$: $$\\\\ell(q_{\\\\alpha},z_{1})-\\\\ell(q_{\\\\alpha},z_{2}) =(1-\\\\alpha)(\\\\ell(q,z_{1})-\\\\ell(q,z_{2}))+\\\\alpha(\\\\ell(q^{(z)},z_{1 })-\\\\ell(q^{(z)},z_{2}))$$ $$\\\\geq(1-\\\\alpha)\\\\epsilon-\\\\alpha\\\\max_{z^{\\\\prime}\\\\in\\\\mathcal{Z}}( \\\\ell(q^{(z)},z^{\\\\prime})-\\\\underline{\\\\ell}(q^{(z)}))>0\\\\ \\\\ \\\\text{ for our choice of $\\\\alpha$}.$$\\n\\nThus, for our choice of $\\\\alpha$, $q_{\\\\alpha}$ satisfies both conditions. \\n\\n### Notation\\n\\nFor conciseness, in the following, we may skip the sets that universal quantifiers range over when default values apply and are unambiguous:\\n\\n* $i,j$ always denote items in $[n]$, if the range is not specified, $\\\\forall i$, $\\\\forall j$ or $\\\\forall i,j$ respectively mean $\\\\forall i\\\\in[n]$, $\\\\forall j\\\\in[n]$ and $\\\\forall i,j\\\\in[n]$\\n* $k,m$ denote ranks, also in $[n]$. We use the same shorthands as above\\n* $\\\\mathfrak{S}_{n}$ denotes the set of permutations of $[n]$. $\\\\sigma,\\\\mu,\\\\sigma^{\\\\prime}$ denote permutations. $\\\\forall\\\\sigma$ means $\\\\forall\\\\sigma\\\\in\\\\mathfrak{S}_{n}$, same for $\\\\mu$ and $\\\\sigma^{\\\\prime}$.\\n* $z,w,z^{\\\\prime}$ denote possible predictions, as above, $\\\\forall z$ (without specific range for $z$) means $\\\\forall z\\\\in\\\\mathcal{Z}$\\n* $q,q^{\\\\prime}$ are members of $\\\\mathcal{Q}$, i.e., $\\\\forall q$ is a shorthand for $\\\\forall q\\\\in\\\\mathcal{Q}$.\\n* $s,s^{\\\\prime}$ are vectors of scores that belong to $\\\\mathbb{R}^{n}$, $\\\\forall s$ means $\\\\forall s\\\\in\\\\mathbb{R}^{n}$.\\n\\n### Connectedness in $\\\\mathfrak{S}_{n}$ and $\\\\mathcal{Z}$\\n\\n**Definition 16** (Connectedness in $\\\\mathfrak{S}_{n}$).: _A subset $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ is connected in $\\\\mathfrak{S}_{n}$ if there is a connected subset $S\\\\subseteq\\\\mathbb{R}^{n}$ such that $\\\\pi=\\\\operatorname*{argsort}(S)$._\\n\\n**Definition 17** (Connectedness in $\\\\mathcal{Z}$).: _A subset $\\\\zeta\\\\subseteq\\\\mathcal{Z}$ is connected in $\\\\mathcal{Z}$ if there is a connected subset $S\\\\subseteq\\\\mathbb{R}^{n}$ such that $\\\\zeta=\\\\operatorname{pred}(S)$._\\n\\n**Remark 18** (Link with topological connectedness).: _These notions of connectedness are not stricto sensu corresponding to topological connectedness on $\\\\mathfrak{S}_{n}$ and $\\\\mathcal{Z}$. To derive the topological connectedness that corresponds to these definitions, it is necessary to see $\\\\operatorname*{argsort}$ as a function valued in $2^{\\\\mathfrak{S}_{n}}$ rather than a set-valued function. When doing so, the image of $\\\\mathbb{R}^{n}$ by $\\\\operatorname*{argsort}$ is exactly the set of weak orders, on which it is possible to put the specialization topology, where the specialization corresponds to the inclusion relation. It is also sometimes referred to as the Alexandroff topology[2]. This topology makes $\\\\operatorname*{argsort}$ continuous (as a function valued in $2^{\\\\mathfrak{S}_{n}}$) and open, which explains the use of $\\\\operatorname*{argsort}$ and $\\\\operatorname{pred}$ to have a suitable (but not topological) definition of connectedness on $\\\\mathfrak{S}_{n}$ and $\\\\mathcal{Z}$._\\n\\n**Definition 19** (u.h.c.).: _Let $\\\\mathcal{U}$ and $\\\\mathcal{V}$ be two topological spaces. A set-valued function $g:\\\\mathcal{U}\\\\rightarrow\\\\mathcal{V}$ is upper hemicontinuous (u.h.c.) if for any non-empty open set $O\\\\subseteq\\\\mathcal{V}$, the upper inverse of $O$ by $g$, $\\\\operatorname*{uinv}[g](O)=\\\\{u\\\\in\\\\mathcal{U}:g(u)\\\\subseteq O\\\\}$ is open._\\n\\n**Proposition 20**.: _Given a discrete space $\\\\mathcal{V}$ (the topology is its whole power set), some $m\\\\in\\\\mathbb{N}$, a set-valued function $g:\\\\mathbb{R}^{m}\\\\rightarrow\\\\mathcal{V}$ is u.h.c. if and only if_\\n\\n$$\\\\forall s\\\\in\\\\mathbb{R}^{m},\\\\exists\\\\epsilon>0,g(\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon ))=g(s)$$\\n\\nProof.: Direct implication. Let $s\\\\in\\\\mathbb{R}^{n}$. Because $\\\\mathcal{V}$ is discrete, $g(s)$ is open. Because $g$ is u.h.c. and $g(s)$ is open, $\\\\operatorname*{univ}[g](g(s))$ is open in $\\\\mathbb{R}^{n}$. As it contains $s$ itself, $\\\\exists\\\\epsilon>0,\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon)\\\\subseteq\\\\operatorname*{ univ}[g](g(s))$. Hence $\\\\exists\\\\epsilon>0,g(\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon))\\\\subseteq g(s)$. The equality comes from $s$ being itself in the ball.\\n\\nReverse implication. Let $O\\\\subseteq\\\\mathcal{V}$ non-empty. For any $s\\\\in\\\\operatorname*{uinv}[g](O),\\\\exists\\\\epsilon_{s}>0,g(\\\\mathcal{B}_{\\\\infty}(s, \\\\epsilon_{s}))=g(s)$. Thus $\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon_{s})\\\\subseteq\\\\operatorname*{univ}[g](O)$. Finally, $\\\\operatorname*{univ}[g](O)=\\\\bigcup_{s\\\\in\\\\operatorname*{univ}[g](O)}\\\\mathcal{B} _{\\\\infty}(s,\\\\epsilon_{s})$ which is open.\\n\\nSome basic properties of connectedness are given below. These are useful in general to understand basic properties of $\\\\operatorname{argsort}$ and $\\\\operatorname{pred}$. Point _vii_ allows to focus on connected subsets $S$ that are open, which is convenient since open subsets or $\\\\mathbb{R}^{n}$ are connected if and only if they are path-connected, and path-connectedness is easier to manipulate in our case.\\n\\n**Lemma 21**.:\\n1. $\\\\forall\\\\sigma,\\\\{s:\\\\operatorname{argsort}(s)=\\\\{\\\\sigma\\\\}\\\\}$ _is open and connected._\\n2. $\\\\forall s,\\\\{s^{\\\\prime}:\\\\ \\\\operatorname{argsort}(s^{\\\\prime})=\\\\operatorname{argsort }(s)\\\\}$ _is connected._\\n3. $\\\\forall s,\\\\{s^{\\\\prime}:\\\\ \\\\operatorname{argsort}(s^{\\\\prime})\\\\subseteq \\\\operatorname{argsort}(s)\\\\}$ _is connected._\\n4. $\\\\forall s,\\\\{s^{\\\\prime}:\\\\operatorname{pred}(s)=\\\\operatorname{pred}(s^{\\\\prime})\\\\}$ _is connected._\\n5. $\\\\operatorname{argsort}$ _is u.h.c. with respect to the discrete topology on_ $\\\\mathfrak{S}_{n}$_, i.e._ $$\\\\forall s,\\\\exists\\\\epsilon>0,\\\\ \\\\ \\\\operatorname{argsort}(\\\\mathcal{B}_{\\\\infty}(s, \\\\epsilon))=\\\\operatorname{argsort}(s).$$ (3)\\n6. $\\\\forall s,\\\\forall\\\\epsilon>0,\\\\forall\\\\sigma\\\\in\\\\operatorname{argsort}(s),\\\\ \\\\ \\\\exists s^{\\\\prime}:\\\\left\\\\|s-s^{\\\\prime}\\\\right\\\\|_{\\\\infty}<\\\\epsilon$ _and_ $\\\\operatorname{argsort}(s^{\\\\prime})=\\\\{\\\\sigma\\\\}$_._\\n7. $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ _is connected in_ $\\\\mathfrak{S}_{n}$ _if and only if there is an_ open _connected subset_ $S\\\\subseteq\\\\mathbb{R}^{n}$ _such that_ $\\\\pi=\\\\operatorname{argsort}(S)$_. Similarly,_ $\\\\zeta\\\\subseteq\\\\mathcal{Z}$ _is connected if and only if there is an open subset_ $S$ _such that_ $\\\\zeta=\\\\operatorname{pred}(S)$_._\\n\\nProof.:\\n1. The set $\\\\{s:\\\\operatorname{argsort}(s)=\\\\{\\\\sigma\\\\}\\\\}$ is open and convex (and thus connected) as the intersection of $n-1$ open half spaces $\\\\{s_{\\\\sigma(k)}>s_{\\\\sigma(k+1)}\\\\}$ for $k=1\\\\ldots n-1$.\\n2. As for point _i)_ above, $\\\\{s^{\\\\prime}:\\\\ \\\\operatorname{argsort}(s^{\\\\prime})=\\\\operatorname{argsort}(s)\\\\}$ is an intersection of open half-spaces and hyperplanes: use the half-space $\\\\{s^{\\\\prime}:(s_{i}-s_{j})(s^{\\\\prime}_{i}-s^{\\\\prime}_{j})>0\\\\}$ for $i,j$ such that $s_{i}\\\\neq s_{j}$ and the hyperplane $\\\\{s^{\\\\prime}:s^{\\\\prime}_{i}=s^{\\\\prime}_{j}\\\\}$ for $i,j$ such that $s_{i}=s_{j}$. The intersection is convex and thus connected.\\n3. It is a consequence of points _v)_ and _vi)_, which are proved below, and point _ii)_ above. Fix $s$, denote by $S=\\\\{s^{\\\\prime}:\\\\ \\\\operatorname{argsort}(s^{\\\\prime})\\\\subseteq \\\\operatorname{argsort}(s)\\\\}$ and take an arbitrary $s^{\\\\prime}\\\\in S$. Denote by $\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon)=\\\\{s^{\\\\prime}:\\\\left\\\\|s-s^{\\\\prime}\\\\right\\\\|_{ \\\\infty}<\\\\epsilon\\\\}$. Fix $\\\\epsilon>0$ to satisfy (3) for both $s$ and $s^{\\\\prime}$, i.e., $\\\\operatorname{argsort}(\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon))=\\\\operatorname{argsort }(s)$ and $\\\\operatorname{argsort}(\\\\mathcal{B}_{\\\\infty}(s^{\\\\prime},\\\\epsilon))= \\\\operatorname{argsort}(s^{\\\\prime})$. let $\\\\sigma\\\\in\\\\operatorname{argsort}(\\\\mathcal{B}_{\\\\infty}(s^{\\\\prime},\\\\epsilon))= \\\\operatorname{argsort}(s^{\\\\prime})$. Then $$\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon)\\\\cup\\\\mathcal{B}_{\\\\infty}(s^{\\\\prime},\\\\epsilon) \\\\cup\\\\{s^{\\\\prime\\\\prime}:\\\\operatorname{argsort}(s^{\\\\prime\\\\prime})=\\\\{\\\\sigma\\\\}\\\\}$$ is connected and included in $S$. $S$ is thus a union of connected subsets (of the form above) with non-empty intersection $\\\\{s\\\\}$, and is thus connected.\\n4. Fix $s$ and let $s_{0}$ such that $\\\\operatorname{argsort}(s_{0})=\\\\operatorname{pred}(s)$ ($s_{0}$ exists by definition of $\\\\operatorname{pred}$). The set $\\\\{s^{\\\\prime}:\\\\operatorname{pred}(s)=\\\\operatorname{pred}(s^{\\\\prime})\\\\}$ is then equal to $\\\\{s^{\\\\prime}:\\\\operatorname{argsort}(s)\\\\subseteq\\\\operatorname{argsort}(s_{0})\\\\}$, which is connected by point _iii)_ above.\\n5. To show inclusion, take $\\\\epsilon<\\\\min\\\\{s_{i}-s_{j}:i,j$ such that $s_{i}>s_{j}\\\\}$. The ranking induced by any $s^{\\\\prime}\\\\in\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon)$ is necessarily compatible with a ranking induced by $s$. Equality holds since $s$ is in the open ball.\\n6. Take $\\\\epsilon$ as above, take $s^{\\\\prime}_{i}=s_{i}-\\\\frac{\\\\epsilon\\\\sigma^{-1}(i)}{n}$ (using $-\\\\sigma^{-1}(i)$ because lower ranks are better). by the choice of $\\\\epsilon$, the relative ordering of $s^{\\\\prime}_{i}$ and $s^{\\\\prime}_{j}$ is the same as $s_{i}$ and $s_{j}$ when $s_{i}\\\\neq s_{j}$, and the ordering is exactly the one induced by $\\\\sigma$ on equivalence classes of ties in $s$.\\n7. Let $\\\\pi=\\\\operatorname{argsort}(S)$ for some connected $S\\\\subseteq\\\\mathbb{R}^{n}$. For $s\\\\in S$, let $\\\\epsilon_{s}>0$ such that $\\\\operatorname{argsort}(\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon_{s}))=\\\\operatorname{argsort }(s)$. Then $\\\\pi=\\\\operatorname{argsort}(\\\\bigcup_{s\\\\in S}\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon_{s }))$, and $\\\\bigcup_{s\\\\in S}\\\\mathcal{B}_{\\\\infty}(s,\\\\epsilon_{s})$ is open (as a union of open balls) and connected (as a union of connected sets with a connected set that intersects all of them). The argument is the same for connectedness in $\\\\mathcal{Z}$.\\n\\nThe following lemma makes sure the definitions of connectedness in $\\\\mathcal{Z}$ and $\\\\mathfrak{S}_{n}$ are coherent.\\n\\n**Lemma 22**.: $\\\\zeta\\\\subseteq\\\\mathcal{Z}$ _is connected in $\\\\mathcal{Z}$ if and only if $\\\\bigcup_{z\\\\in\\\\zeta}z$ is connected in $\\\\mathfrak{S}_{n}$._\\n\\nProof.: First point: The if direction holds because $\\\\zeta=\\\\Lambda(\\\\bigcup_{z\\\\in\\\\zeta}z)$, thus if $\\\\bigcup_{z\\\\in\\\\zeta}z=\\\\operatorname{argsort}(S)$ for some connected $S$, then $\\\\zeta=\\\\Lambda\\\\circ\\\\operatorname{argsort}(S)=\\\\operatorname{pred}(S)$. For the only if direction, let us assume $\\\\zeta\\\\subseteq\\\\mathcal{Z}$ is connected, i.e. there is a connected $S\\\\subseteq\\\\mathbb{R}^{n}$ such that $\\\\zeta=\\\\operatorname{pred}(S)$. Then\\n\\n$$\\\\bigcup_{z\\\\in\\\\zeta}z =\\\\bigcup_{s\\\\in S}\\\\operatorname{pred}(s)=\\\\bigcup_{s\\\\in S} \\\\operatorname{argsort}(\\\\{s^{\\\\prime}:\\\\operatorname{argsort}(s^{\\\\prime}) \\\\subseteq\\\\operatorname{pred}(s)\\\\})$$ $$=\\\\operatorname{argsort}(\\\\bigcup_{\\\\underbrace{s\\\\in S}\\\\{s^{ \\\\prime}:\\\\operatorname{argsort}(s^{\\\\prime})\\\\subseteq\\\\operatorname{pred}(s)\\\\} })$$\\n\\nThese set $\\\\tilde{S}$ is the union of connected sets (by Lemma 21 point _iii_)), and there is a connected set (S) that intersects all members of the union. Thus, it is connected. \\n\\n**Remark 23**.: _An analog statement from $\\\\mathfrak{S}_{n}$ to $\\\\mathcal{Z}$ is false in general: there exists $n$ and $\\\\operatorname{pred}$ satisfying all assumptions such that there is $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ not connected but $\\\\Lambda(\\\\pi)$ is connected._\\n\\n_To see this, we anticipate a bit on the idea of paths of adjacent items (Lemma 36 below). Consider a top-2 selection task with $n$=3. Let $\\\\pi=\\\\{2\\\\succ 1\\\\succ 3,3\\\\succ 1\\\\succ 2\\\\}$. $\\\\pi$ is not connected because there is no path between the two permutations. However, $\\\\Lambda(\\\\pi)=\\\\{1\\\\sim 2\\\\succ 3,1\\\\sim 3\\\\succ 2\\\\}$, which is connected because there is a path (transposition of $2$ and $3$)._\\n\\n## Appendix B Calibration and Consistency\\n\\n_Disclaimer: As mentioned in the body of the paper, we address, in these Appendices, a setting more general than ranking. This setting is described in Section A. We restate first the statement of the paper (on ranking), then state the more general statement (on weak orders) and prove the latter._\\n\\n**Subsection B.1**: describes the definition and basic notions of calibration and uniform calibration,\\n**Subsection B.2**: provides the proof that calibration is not just an inclusion of argmins, but rather an equality, stated in Theorem 4 (also [7, Th. 2]),\\n**Subsection B.3**: contains the proof that uniform calibration is equivalent calibration under our assumptions as mentioned in Proposition 2.\\n\\n### Calibration and Consistency\\n\\nFollowing the body of the paper, a surrogate loss $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to\\\\mathbb{R}_{+}$ is a measurable function that aims to be minimized. Similarly to the task loss, we can define the outer and inner risks, given a function $f:\\\\mathcal{X}\\\\to\\\\mathbb{R}^{n}$ belonging to the space of measurable function from $\\\\mathcal{X}$ to $\\\\mathbb{R}^{n}$, denoted $\\\\mathcal{F}$, a distribution $P$ over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$, $s\\\\in\\\\mathbb{R}^{n}$ and a distribution $q\\\\in\\\\mathcal{Q}$,\\n\\n$$\\\\mathcal{R}_{\\\\Phi,P}(f)=\\\\mathbb{E}_{P}[\\\\Phi(Y,f(X))]\\\\,,\\\\qquad\\\\quad\\\\phi(q,s)= \\\\mathbb{E}_{q}[\\\\Phi(Y,s)]\\\\,,\\\\qquad\\\\quad\\\\underline{\\\\phi}(q)=\\\\inf_{s^{\\\\prime}\\\\in \\\\mathbb{R}^{n}}\\\\phi(q,s^{\\\\prime})\\\\,.$$\\n\\nWe can state the definition of calibration in $\\\\mathcal{Z}$,\\n\\n**Definition 24** (calibration).: $\\\\Phi$ _is $L$-calibrated if and only if_\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta>0,\\\\;\\\\operatorname{pred}(\\\\operatorname{ lev}_{\\\\delta}\\\\phi(q,.))\\\\subseteq\\\\operatorname{argmin}\\\\ell(q,.)$$\\n\\nAnd the one of uniform calibration,\\n\\n**Definition 25** (uniform calibration).: $\\\\Phi$ _is $L$-uniformly calibrated if and only if_\\n\\n$$\\\\forall\\\\varepsilon>0,\\\\exists\\\\delta>0,\\\\forall q\\\\in\\\\mathcal{Q},\\\\; \\\\operatorname{pred}(\\\\operatorname{lev}_{\\\\delta}\\\\phi(q,.))\\\\subseteq \\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$$Indeed, uniform calibration leads to the existence of an excess risk bound allowing to control the task risk by the surrogate risk.\\n\\n**Definition 26** (excess risk bound).: _A continuous function $\\\\delta:\\\\mathbb{R}_{+}\\\\to\\\\mathbb{R}_{+}$ is an excess risk bound if_\\n\\n$$\\\\forall P,\\\\forall f\\\\in\\\\mathcal{F},\\\\ \\\\ \\\\ \\\\mathcal{R}_{L,P}(\\\\mathrm{pred} \\\\circ f)-\\\\inf_{h\\\\in\\\\mathcal{H}}\\\\mathcal{R}_{L,P}(h)\\\\leq\\\\delta\\\\left(\\\\mathcal{R} _{\\\\Phi,P}(f)-\\\\inf_{g\\\\in\\\\mathcal{F}}\\\\mathcal{R}_{\\\\Phi,P}(g)\\\\right)$$ _and_ $$\\\\delta(\\\\epsilon)\\\\xrightarrow[\\\\epsilon\\\\to 0]{}0$$\\n\\n**Proposition 27**.: $\\\\Phi$ _is uniformly $L$-calibrated if and only if there exists an excess risk bound._\\n\\nProof.: This result is directly derived from Steinwart [31] Th. 2.13 (direct implication) and Th. 2.17 (reverse implication). \\n\\n### Proof of characterization of Calibration in Theorem 4 (equality of optimal predictions)\\n\\nIn this section, for the sake of completness, we provide a full proof for the generalization of Theorem 4 under our assumptions. First, lets remind the exact statement of the paper in the ranking setting.\\n\\n**Theorem 4** ([7, Th. 2]).: _Given a ranking loss $L$, $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to\\\\mathbb{R}_{+}$ is $L$-calibrated if and only if_\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall\\\\delta\\\\in(0,\\\\delta_{0}], \\\\ \\\\ \\\\operatorname*{argmin}_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\ell(q,\\\\sigma)=\\\\bigcup_{s: \\\\phi(q,s)-\\\\underline{\\\\phi}(q)<\\\\delta}\\\\operatorname*{argsort}(s)$$\\n\\nThen, the version of the same theorem in $\\\\mathcal{Z}$,\\n\\n**Theorem 28**.: _Assuming (A1), (A2) and (A3), $\\\\Phi$ is $L$-calibrated if and only if_\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall 0<\\\\delta<\\\\delta_{0}, \\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))=\\\\operatorname*{argmin}\\\\ell(q,.)$$\\n\\nBefore providing to the proof of Theorem 28, we state a general result of upper hemicontinuity that underlies many of the proofs in these appendices. It is a simplified version of the Berge Maximum Theorem [24]. One application is that $\\\\operatorname*{argmin}\\\\ell(q,.)$ is u.h.c. as a function of $q$.\\n\\n**Lemma 29**.: _Given a function $g:\\\\mathcal{Q}\\\\times\\\\mathcal{Z}\\\\to\\\\mathbb{R}$, continuous in its first argument, the set-valued function_\\n\\n$$\\\\begin{cases}\\\\mathcal{Q}&\\\\to\\\\mathcal{Z}\\\\\\\\ q&\\\\mapsto\\\\operatorname*{argmin}_{z\\\\in\\\\mathcal{Z}}g(q,z)\\\\end{cases}\\\\qquad\\\\text { is u.h.c.}$$\\n\\nProof.: We use the characterization of u.h.c. from Prop.20 as $\\\\mathcal{Q}\\\\subseteq\\\\mathbb{R}^{|\\\\mathcal{Y}|}$. Let $q\\\\in\\\\mathcal{Q}$ be a distribution. We denote $\\\\mathcal{A}_{q}=\\\\operatorname*{argmin}_{z\\\\in\\\\mathcal{Z}}g(q,z)$ and\\n\\n$$\\\\epsilon_{q}=\\\\min_{z\\\\in\\\\mathcal{A}_{q},w\\\\in\\\\mathcal{Z}\\\\setminus\\\\mathcal{A}_{ q}}|g(q,z)-g(q,w)|$$\\n\\nFor some $\\\\delta>0$, and for any $\\\\tilde{q}\\\\in\\\\mathcal{B}_{\\\\infty}(q,\\\\delta)$, we use the following decomposition,\\n\\n$$g(\\\\tilde{q},w)-g(\\\\tilde{q},z)=\\\\underbrace{g(\\\\tilde{q},w)-g(q,w)}_{\\\\begin{subarray} {c}3\\\\delta_{1}>0,\\\\ -\\\\ \\\\zeta_{q}\\\\\\\\ (\\\\text{continuity})\\\\end{subarray}}+\\\\underbrace{g(q,w)-g(q,z)}_{\\\\begin{subarray} {c}>\\\\epsilon_{q}\\\\\\\\ >\\\\epsilon_{q}\\\\end{subarray}}+\\\\underbrace{g(q,z)-g(\\\\tilde{q},z)}_{\\\\begin{subarray} {c}3\\\\delta_{2}>0,\\\\ -\\\\ \\\\zeta_{q}\\\\\\\\ (\\\\text{continuity})\\\\end{subarray}}$$\\n\\nThanks to the continuity of $g$ in its first argument, we have that\\n\\n$$\\\\exists\\\\delta_{q}>0,\\\\forall\\\\tilde{q}\\\\in\\\\mathcal{B}_{\\\\infty}(q,\\\\delta),g( \\\\tilde{q},w)-g(\\\\tilde{q},z)>\\\\frac{\\\\epsilon_{q}}{2}\\\\,.$$\\n\\nThus,\\n\\n$$\\\\exists\\\\delta_{q}>0,\\\\bigcup_{\\\\tilde{q}\\\\in\\\\mathcal{B}_{\\\\infty}(q,\\\\delta)} \\\\operatorname*{argmin}_{z\\\\in\\\\mathcal{Z}}g(\\\\tilde{q},z)\\\\subseteq\\\\operatorname*{ argmin}_{z\\\\in\\\\mathcal{Z}}g(q,z)$$\\n\\nThe equality comes from the fact $q$ itself is in the ball. \\n\\nWe can now proceed to the proof of Theorem 28.\\n\\nProof of Theorem 28.: $(\\\\Phi,\\\\mathrm{pred})$ is $L$-calibrated\\n\\n$$(\\\\Phi,\\\\mathrm{pred})$$\\n\\n is\\n\\n$$L$$\\n\\n-calibrated $$\\\\Leftrightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall 0<\\\\delta< \\\\delta_{0},\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))\\\\subseteq\\\\mathrm{argmin} \\\\,\\\\ell(q,.)$$ (Definition 25)\\n\\nWe now focus on showing\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall 0<\\\\delta\\\\;< \\\\delta_{0},\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))\\\\subseteq\\\\mathrm{argmin} \\\\,\\\\ell(q,.)$$ $$\\\\Leftrightarrow$$ $$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall 0<\\\\delta\\\\;< \\\\delta_{0},\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))=\\\\mathrm{argmin}\\\\, \\\\ell(q,.)$$\\n\\nOr otherwise stated, that\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta} \\\\phi(q,.))\\\\subseteq\\\\mathrm{argmin}\\\\,\\\\ell(q,.)\\\\;\\\\;\\\\Leftrightarrow\\\\;\\\\;\\\\forall q \\\\in\\\\mathcal{Q},\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.) )=\\\\mathrm{argmin}\\\\,\\\\ell(q,.) \\\\tag{4}$$\\n\\nFirst, for any $q\\\\in\\\\mathcal{Q}$, after defining $\\\\tilde{\\\\phi}(q,z)=\\\\inf_{s\\\\in\\\\mathrm{pred}^{-1}(z)}\\\\phi(q,s)$, we have that\\n\\n$$\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))=\\\\mathrm{argmin }\\\\,\\\\tilde{\\\\phi}(q,.)\\\\,.$$\\n\\nNow, thanks to Wijsman [37, Th. 2], we know that $\\\\tilde{\\\\phi}$ is continuous with respect to its first argument, then Lemma 29 gives us that $\\\\mathrm{argmin}\\\\,\\\\tilde{\\\\phi}(q,.)$ is u.h.c. and thanks to Prop. 20, we have\\n\\n$$\\\\exists\\\\delta>0,\\\\bigcup_{\\\\tilde{q}\\\\in B_{\\\\delta}(q)}\\\\mathrm{argmin}\\\\,\\\\tilde{ \\\\phi}(\\\\tilde{q},.)=\\\\mathrm{argmin}\\\\,\\\\tilde{\\\\phi}(q,.)=\\\\bigcap_{\\\\delta>0} \\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))\\\\,. \\\\tag{5}$$\\n\\nLet\\'s take $z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q,.)$. By Lemma 14_ii)_, $\\\\exists q^{z}\\\\in\\\\mathcal{Q},\\\\mathrm{argmin}\\\\,\\\\ell(q^{z}\\\\,,.)=\\\\{z\\\\}$ and we can define $\\\\tilde{q}^{\\\\alpha}=(1-\\\\alpha)q+\\\\alpha q^{z}$ for some $\\\\alpha\\\\in(0,1)$. Because $\\\\mathrm{argmin}\\\\,\\\\ell(q,.)\\\\cap\\\\mathrm{argmin}\\\\,\\\\ell(q^{z},.)=\\\\{z\\\\}$, then $\\\\mathrm{argmin}\\\\,\\\\ell(\\\\tilde{q}^{\\\\alpha},.)=\\\\{z\\\\}$ ($z$ is the only element optimal for both components of the mixture). Finally, we prove (4):\\n\\n$$\\\\forall q\\\\;\\\\in\\\\mathcal{Q},\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{ \\\\delta}\\\\phi(q,.))\\\\subseteq\\\\mathrm{argmin}\\\\,\\\\ell(q,.)$$ $$\\\\Rightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall z\\\\in\\\\mathrm{argmin}\\\\, \\\\ell(q,.),\\\\forall\\\\alpha\\\\in(0,1),\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{ \\\\delta}\\\\phi(\\\\tilde{q}^{\\\\alpha},.))=\\\\{z\\\\}$$ (previous line applied at $$\\\\tilde{q}^{\\\\alpha}$$ ) $$\\\\Rightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall z\\\\in\\\\mathrm{argmin}\\\\, \\\\ell(q,.),\\\\forall\\\\alpha\\\\in(0,1),\\\\mathrm{argmin}\\\\,\\\\tilde{\\\\phi}(\\\\tilde{q}^{ \\\\alpha},.)=\\\\{z\\\\}$$ (from (5)) $$\\\\Rightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall z\\\\in\\\\mathrm{argmin}\\\\, \\\\ell(q,.),\\\\exists\\\\alpha\\\\in(0,1),\\\\{z\\\\}=\\\\mathrm{argmin}\\\\,\\\\tilde{\\\\phi}(\\\\tilde{q}^ {\\\\alpha}\\\\,,.)\\\\subseteq\\\\mathrm{argmin}\\\\,\\\\tilde{\\\\phi}(q,.)$$ (from (5)) $$\\\\Rightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall z\\\\in\\\\mathrm{argmin}\\\\, \\\\ell(q,.),\\\\{z\\\\}\\\\subseteq\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta} \\\\phi(q,.))$$ (from (5) again) $$\\\\Rightarrow\\\\forall q\\\\in\\\\mathcal{Q},\\\\mathrm{argmin}\\\\,\\\\ell(q,.) \\\\subseteq\\\\bigcap_{\\\\delta>0}\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta}\\\\phi(q,.))$$\\n\\n### Equivalence between calibration and uniform calibration\\n\\nIn this section, we prove the equivalence between calibration and the existence of an excess risk bound stated in Proposition 2. First, we remind the exact statement of the paper in the ranking setting.\\n\\n**Proposition 2**.: $\\\\Phi$ _is $L$-calibrated if and only if there is an excess risk bound between $\\\\mathcal{R}_{\\\\Phi}$ and $\\\\mathcal{R}_{L}$._\\n\\nSince we already established in Proposition 27 that uniform calibration is equivalent to the existence of an excess risk bound, we now show the equivalence between calibration and uniform calibration in our general setting:\\n\\n**Theorem 30**.: _Assuming (A1), $\\\\Phi$ is $L$-calibrated if and only if $\\\\Phi$ is $L$-uniformly calibrated._Proof.: If $\\\\Phi$ is $L$-uniformly calibrated, then it is $L$-calibrated, so we just have to prove the only if direction.\\n\\nUsing the notation $\\\\tilde{\\\\phi}(q,z)=\\\\inf_{s\\\\in\\\\operatorname{pred}^{-1}(z)}\\\\phi(q,s)\\\\in\\\\mathbb{R}$ for $z\\\\in\\\\mathcal{Z}$, let us notice that all the following functions are continuous on $\\\\mathcal{Q}$ as $\\\\mathcal{Y}$ is finite [37, Theorem 2]:\\n\\n$$q \\\\mapsto\\\\tilde{\\\\phi}(q,z) q \\\\mapsto\\\\underline{\\\\phi}(q)=\\\\inf_{s\\\\in\\\\mathbb{R}^{n}}\\\\phi(q,s)=\\\\min_{z \\\\in\\\\mathcal{Z}}\\\\tilde{\\\\phi}(q,z)$$ $$q \\\\mapsto\\\\underline{\\\\ell}(q,z) q \\\\mapsto\\\\underline{\\\\ell}(q)=\\\\min_{z\\\\in\\\\mathcal{Z}}\\\\ell(q,z)\\\\,.$$\\n\\nConsequently, for every $z\\\\in\\\\mathcal{Z}$, both functions\\n\\n$$g_{z}:q \\\\mapsto\\\\ell(q,z)-\\\\underline{\\\\ell}(q)\\\\text{ and } h_{z}:q \\\\mapsto\\\\tilde{\\\\phi}(q,z)-\\\\underline{\\\\phi}(q)$$\\n\\nare continuous on $\\\\mathcal{Q}$. Also notice that since both $\\\\mathcal{Y}$ and $\\\\mathcal{Z}$ are finite, both $\\\\ell$ and $\\\\tilde{\\\\phi}$ are bounded on their domain. More formally, denoting $B_{\\\\ell}=\\\\max_{z\\\\in\\\\mathcal{Z}}\\\\max_{y\\\\in\\\\mathcal{Y}}L(y,z)-\\\\min_{z\\\\in \\\\mathcal{Z}}\\\\min_{y\\\\in\\\\mathcal{Y}}L(y,z)$, we have $g_{z}\\\\in[0,B_{\\\\ell}]$.\\n\\nLet us now fix $\\\\epsilon>0$ and denote\\n\\n$$\\\\tilde{\\\\Delta}(z,\\\\epsilon)=\\\\big{\\\\{}q\\\\in\\\\mathcal{Q}:g_{z}(q)\\\\geq\\\\epsilon\\\\big{\\\\}} =g_{z}^{-1}([\\\\epsilon,B_{\\\\ell}])\\\\,.$$\\n\\nSince $g_{z}$ is continuous from a compact set ($\\\\mathcal{Q}$) to $\\\\mathbb{R}$, it is a proper map, which means that preimages of compact subsets of $\\\\mathbb{R}$ are compact. In particular, it implies that $\\\\tilde{\\\\Delta}(z,\\\\epsilon)$ is compact. Since $h_{z}$ is also continuous on $\\\\mathcal{Q}$, it implies that is reaches its minimum on $\\\\tilde{\\\\Delta}(z,\\\\epsilon)$. Let us then denote:\\n\\n$$\\\\delta(\\\\epsilon)=\\\\min_{z\\\\in\\\\mathcal{Z}}\\\\min_{q\\\\in\\\\tilde{\\\\Delta}(z,\\\\epsilon)}h _{z}(q)\\\\,.$$\\n\\nThen, since $\\\\Phi$ is $L$-calibrated, $g_{z}(q)>\\\\epsilon\\\\Rightarrow h_{z}(q)>0$ for all $z,q$ and thus $\\\\delta(\\\\epsilon)>0$.\\n\\nThus, for all $\\\\epsilon>0$, $q\\\\in\\\\mathcal{Q}$ and $s\\\\in\\\\mathbb{R}^{n}$, we have:\\n\\n$$\\\\phi(q,s)-\\\\underline{\\\\phi}(q) <\\\\delta(\\\\epsilon)$$ $$\\\\Rightarrow \\\\forall z\\\\in\\\\operatorname{pred}(s), \\\\tilde{\\\\phi}(q,z)-\\\\underline{\\\\phi}(q) <\\\\delta(\\\\epsilon)$$ (by def. of $$\\\\tilde{\\\\phi}$$ ) $$\\\\Rightarrow \\\\forall z\\\\in\\\\operatorname{pred}(s), \\\\ell(q,z)-\\\\underline{\\\\ell}(q) <\\\\epsilon$$ (by Def of $$\\\\delta(\\\\epsilon)$$ ) $$\\\\Rightarrow \\\\ell(q,s)-\\\\underline{\\\\ell}(q) <\\\\epsilon$$\\n\\nwhich means that $\\\\Phi$ is $L$-uniformly calibrated. \\n\\n## Appendix C Proof of the main result\\n\\n_Disclaimer: As mentioned in the body of the paper, we address, in these Appendices, a setting more general than ranking. This setting is described in Section A. We restate first the statement of the paper (on ranking), then state the more general statement (on weak orders) and prove the latter._\\n\\nThe objective of this section is to prove the following Theorem 6. First we restate the theorem in the ranking setting.\\n\\n**Theorem 6**.: _For a ranking loss $L$, the following statements are equivalent:_\\n\\n1. $L$ _is CEU,_\\n2. $\\\\forall q,\\\\operatorname{argmin}_{\\\\sigma}\\\\ell(q,\\\\sigma)$ _is connected._\\n3. $\\\\forall\\\\epsilon>0,\\\\forall q,\\\\operatorname{lev}_{\\\\epsilon}\\\\ell(q,.)$ _is connected,_\\n\\n_Moreover, the function $\\\\tilde{u}:\\\\mathcal{Y}\\\\to\\\\mathbb{R}^{n}$ defined as: $\\\\forall i\\\\in[n],\\\\tilde{u}_{i}(y)=-\\\\sum_{\\\\sigma\\\\in\\\\mathcal{\\\\mathbb{S}}_{n}} \\\\mathbb{1}_{\\\\{\\\\sigma(1)=i\\\\}}L(y,\\\\sigma)$_\\n\\n_is a utility function for $L$ whenever there exists a utility function for $L$ (i.e., whenever $L$ is CEU)._\\n\\nBefore stating its generalization in $\\\\mathcal{Z}$, we need to note the definition of CEU task loss extends in a strait-forward way to $\\\\mathcal{Z}$. We state it for the sake of completeness.\\n\\n**Definition 31** (Ceu).: _A task loss $L$ is compatible with expected utility (CEU) if there exists a function $u:\\\\mathcal{Y}\\\\to\\\\mathbb{R}^{n}$ such that $\\\\Phi_{u}^{\\\\mathrm{sq}}:y,s\\\\mapsto(u(y)-s)^{2}$ is $L$-calibrated._\\n\\n**Theorem 32**.: _Under assumptions (A1), (A2) and (A3), for a task loss $L$, the following statements are equivalent:_\\n\\n1. $L$ _is CEU,_\\n2. $\\\\forall q,\\\\operatorname*{argmin}\\\\ell(q,.)$ _is connected._\\n3. $\\\\forall\\\\epsilon>0,\\\\forall q,\\\\operatorname{lev}_{\\\\epsilon}\\\\ell(q,.)$ _is connected,_\\n\\n_Moreover, the function $\\\\tilde{u}:\\\\mathcal{Y}\\\\rightarrow\\\\mathbb{R}^{n}$ defined as: $\\\\forall i\\\\in[n],\\\\tilde{u}_{i}(y)=-\\\\sum_{z\\\\in\\\\mathcal{Z}:z-1(i)=1}L(y,z)$_\\n\\n_is a utility function for $L$ whenever there exists a utility function for $L$ (i.e., whenever $L$ is CEU)._\\n\\nWe now proceed with the proof that this whole section is dedicated to. The two main intermediary results of this proof are Corollary 41 and Theorem 42.\\n\\nIn Appendix C.1, we elaborate on more properties of connectedness in $\\\\mathfrak{S}_{n}$ and $\\\\mathcal{Z}$. In particular, we prove Lemma 36 which is the generalized version of Proposition 9 in the main paper regarding the characterization of connectedness with paths of adjacent items. This proposition is a key technical result that is fundamental to the results of Appendices D, E and H.\\n\\nIn Appendix C.2, we give technical lemmas that lead to the main steps in the full proof. The main result of this section, Lemma 38 allows to define sequences of distributions in $cQ$ for which the argmin matches a path of adjacent transposition between permutation. This is the key component to extend properties of argmins to properties of the entire loss.\\n\\nIn Appendix C.3, we put the tools together, and first prove the strict monotonicity of loss with connected argmins (Corollary 41), which means that preferences between items as defined by optimal $z$ are reflected by preferences even for non-optimal $z^{\\\\prime}$. This, in turn, allow us to prove the existence of the utility function and its analytical formula 42. The final proof at the end of that section makes the link with connected sublevel sets.\\n\\n### Properties of connected sets in $\\\\mathfrak{S}_{n}$ and $\\\\mathcal{Z}$\\n\\nThe main technical property of connectedness is the existence of paths with transpositions of adjacent items:\\n\\n**Definition 33** (Adjacent items).: $\\\\forall\\\\sigma\\\\in\\\\mathfrak{S}_{n}$_, items $i$ and $j$ are adjacent2 in $\\\\sigma$ if $|\\\\sigma^{-1}(i)-\\\\sigma^{-1}(j)|\\\\leq 1$. $\\\\forall z\\\\in\\\\mathcal{Z}$, items $i$ and $j$ are adjacent in $z$ if $\\\\exists\\\\sigma\\\\in z$ such that $i$ and $j$ are adjacent in $\\\\sigma$._\\n\\nFootnote 2: In that definition, we consider an item to be adjacent to itself. We could prevent that without altering anything in the proofs.\\n\\n**Definition 34** (paths of adjacent items).: **Path in $\\\\mathfrak{S}_{n}$:** _Let $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$, and $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\pi$. A path of adjacent transpositions of length $M\\\\in\\\\mathbb{N}$ from $\\\\sigma$ to $\\\\sigma^{\\\\prime}$ in $\\\\pi$ is a sequence $(\\\\sigma_{0},...,\\\\sigma_{M})\\\\in\\\\pi^{M}$ such that:_\\n\\n* $\\\\sigma_{0}=\\\\sigma$ _and_ $\\\\sigma_{M}=\\\\sigma^{\\\\prime}$__\\n* $\\\\forall m\\\\in[M]$_, there is_ $(i_{m},i^{\\\\prime}_{m})\\\\in[n]^{2}$ _such that_ $i_{m}$ _and_ $i^{\\\\prime}_{m}$ _are adjacent in_ $\\\\sigma_{m-1}$ _and_ $\\\\sigma_{m}=\\\\tau_{i_{m}i^{\\\\prime}_{m}}\\\\sigma_{m-1}$_._\\n\\n**Path in $\\\\mathcal{Z}$:** _Let $\\\\zeta\\\\subseteq\\\\mathcal{Z}$, and $z,z^{\\\\prime}\\\\in\\\\zeta$. A path of adjacent transpositions of length $M\\\\in\\\\mathbb{N}$ from $z$ to $z^{\\\\prime}$ in $\\\\zeta$ is a sequence $(z_{0},...,z_{M})\\\\in\\\\zeta^{M}$ such that:_\\n\\n* $z_{0}=z$ _and_ $z_{M}=z^{\\\\prime}$__\\n* $\\\\forall m\\\\in[M]$_, there is_ $(i_{m},i^{\\\\prime}_{m})\\\\in[n]^{2}$ _such that_ $i_{m}$ _and_ $i^{\\\\prime}_{m}$ _are adjacent in_ $z_{m-1}$ _and_ $z_{m}=\\\\tau_{i_{m}i^{\\\\prime}_{m}}z_{m-1}$_._\\n\\nWe say that there is a path from $\\\\sigma$ and $\\\\sigma^{\\\\prime}$ in $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ if there is $M\\\\in\\\\mathbb{N}$ such that there is a path of adjacent transpositions of length M from $\\\\sigma$ to $\\\\sigma^{\\\\prime}$ in $\\\\pi$. A similar terminology is used in $\\\\mathcal{Z}$.\\n\\nPaths of adjacent items between two permutations can be found by bubble sort. We describe a naive version of the algorithm and then explain the important property that we use later in the proofs.\\n\\n```\\ninput :$n$ and $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\mathfrak{S}_{n}$ output : sequence of transpositions $\\\\tau^{(1)},\\\\ldots,\\\\tau^{(t)}$ such that $\\\\sigma=\\\\tau^{(t)}\\\\ldots\\\\tau^{(1)}\\\\sigma^{\\\\prime}$ $k=n$; $\\\\nu=\\\\sigma^{\\\\prime}$; $t=1$; for$m\\\\gets 1$to$n-1$do for$k\\\\gets 1$to$m-i$do if$\\\\sigma^{-1}(\\\\nu(k))>\\\\sigma^{-1}(\\\\nu(k+1))$then $\\\\tau^{(t)}=\\\\tau_{\\\\nu(k)\\\\nu(k+1)}$; $\\\\nu=\\\\tau^{(t)}\\\\nu$; $t=t+1$;  end if  end for  end for\\n```\\n\\n**Algorithm 1**(Naive) Bubble Sort\\n\\n**Remark 35**.: _Note the monotonicity property of the bubble sort. We use it later in the proofs. Given two items $i\\\\neq j\\\\in[n]$, if $i\\\\succ_{\\\\sigma}j$ and $i\\\\prec_{\\\\sigma^{\\\\prime}}j$, then either $\\\\tau_{ij}$ or $\\\\tau_{ji}$ appears exactly once in the output sequence $\\\\tau^{(1)},\\\\ldots,\\\\tau^{(t)}$ of bubble sort. Otherwise, neither $\\\\tau_{ij}$ nor $\\\\tau_{ji}$ appears in the output sequence of bubble sort._\\n\\nThe next lemma is the general version of Proposition 9 in the main paper.\\n\\n**Lemma 36** (connectedness and paths in $\\\\mathfrak{S}_{n}$).: $\\\\pi\\\\subseteq\\\\mathfrak{S}_{n}$ _is connected in $\\\\mathfrak{S}_{n}$ if and only if for every $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\pi$, there is a path from $\\\\sigma$ to $\\\\sigma^{\\\\prime}$ in $\\\\pi$._\\n\\n_As a corollary, $\\\\zeta\\\\subseteq\\\\mathcal{Z}$ is connected in $\\\\mathcal{Z}$ if and only if for every $z,z^{\\\\prime}\\\\in\\\\zeta$ there is a path between $z$ and $z^{\\\\prime}$ in $\\\\zeta$._\\n\\nProof.: **\"if\" direction** Let $\\\\sigma$ and $\\\\sigma^{\\\\prime}$ two permutations such that there is a path between them in $\\\\pi$. Denote $(\\\\sigma_{m})_{m=0}^{M}$ the sequence of permutations along that path ($\\\\sigma_{0}=\\\\sigma$ and $\\\\sigma_{M}=\\\\sigma^{\\\\prime}$). We show that there is a connected set $S$ such that $\\\\{\\\\sigma_{0},...,\\\\sigma_{M}\\\\}=\\\\operatorname{argsort}(S)$.\\n\\nThe first step is to notice that if $\\\\sigma_{m}$ and $\\\\sigma_{m+1}$ are two permutations that are equal up to an adjacent transposition, then there is $s^{(m)}$ such that $\\\\operatorname{argsort}(s^{(m)})=\\\\{\\\\sigma_{m},\\\\sigma_{m+1}\\\\}$ (use $s^{(m)}_{i}=-(\\\\sigma^{-1}_{m}(i)+\\\\sigma^{-1}_{m+1}(i))$). We thus have:\\n\\n$$\\\\{\\\\sigma_{0},...,\\\\sigma_{M}\\\\}=\\\\operatorname{argsort}(\\\\bigcup_{m=0..M-1}\\\\underbrace {\\\\{s:\\\\operatorname{argsort}(s)\\\\subseteq\\\\operatorname{argsort}(s^{(m)})\\\\}}_{S _{m}})$$\\n\\nEach $S_{m}$ is connected by Lemma 21 point _iii)_, and $S_{m}\\\\cap S_{m+1}=\\\\{s:\\\\operatorname{argsort}(s)=\\\\sigma_{m+1}\\\\}$ is open and connected. Thus $\\\\bigcup_{m}S_{m}$ is connected. Finally, $\\\\pi$ is the union of all these sets for all possible pairs $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\pi$. The union itself is connected\\n\\n**\"only if\" direction** We prove the result in two steps. First, we prove that if $\\\\pi=\\\\operatorname{argsort}(s)$ for some $s\\\\in\\\\mathbb{R}^{n}$, then any two $\\\\sigma,\\\\sigma^{\\\\prime}$ are connected by a path. We then go to the more general case $\\\\pi=\\\\operatorname{argsort}(S)$ with $S\\\\subseteq\\\\mathbb{R}^{n}$.\\n\\n_Case 1: $\\\\exists s\\\\in\\\\mathbb{R}^{n},\\\\pi=\\\\operatorname{argsort}(s)$._ Let $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\pi$. By definition of $\\\\operatorname{argsort}$, $i\\\\succ_{\\\\sigma}j$ and $j\\\\succ_{\\\\sigma^{\\\\prime}}i$ is only possible for $i,j$ such that $s_{i}=s_{j}$. Apply $\\\\mathtt{bubblesort}($target=$\\\\sigma$, input=$\\\\sigma^{\\\\prime}$). Bubble sort gives a path $(\\\\sigma_{m})_{m=0}^{M}$ between $\\\\sigma$ and $\\\\sigma^{\\\\prime}$; applying recursively the above remark to $i\\\\succ_{\\\\sigma}j$ and $j\\\\succ_{\\\\sigma_{m}}i$, the adjacent transpositions only exchange items $i,j$ that are tied in $s$. Thus, by the definition of $\\\\operatorname{argsort}$, $\\\\forall m\\\\in\\\\{0,...,M\\\\}$, $\\\\sigma_{m}\\\\in\\\\operatorname{argsort}(s)=\\\\pi$.\\n\\n_Step 2: general case: $\\\\pi=\\\\operatorname{argsort}(S)$ for connected $S\\\\subseteq\\\\mathbb{R}^{n}$_ Using Lemma 21 (point _vii)_) we can assume without loss of generality that $S$ is open and thus path-connected. Let $\\\\sigma,\\\\sigma^{\\\\prime}\\\\in\\\\pi$. Let $\\\\gamma:[0,1]\\\\to S$ be a continuous function such that $\\\\sigma\\\\in\\\\operatorname{argsort}(\\\\gamma(0))$ and $\\\\sigma^{\\\\prime}\\\\in\\\\operatorname{argsort}(\\\\gamma(1))$; such a $\\\\gamma$ exists by the assumption $\\\\pi=\\\\operatorname{argsort}(S)$ for path-connected $S$. By definition of $\\\\gamma$, we have $\\\\operatorname{argsort}(\\\\gamma([0,1]))\\\\subseteq\\\\pi$. Now consider the undirected graph $G=(\\\\pi,E)$ with set of nodes $\\\\pi$ and edges $E$, where $(\\\\nu,\\\\nu^{\\\\prime})\\\\in E$ if $\\\\exists t\\\\in[0,1]$ such that $\\\\{\\\\nu,\\\\nu^{\\\\prime}\\\\}\\\\subseteq\\\\operatorname{argsort}(\\\\gamma(t))$. We now prove that there is a path in $G$ (in the usual sense of paths in a graph) between the nodes corresponding to $\\\\sigma$ and $\\\\sigma^{\\\\prime}$. To that end, first notice that by Lemma 21 point _v_), $\\\\operatorname{argsort}(\\\\gamma(t-\\\\epsilon))\\\\subseteq\\\\operatorname{argsort}( \\\\gamma(t))$ for small enough $\\\\epsilon$ and $t\\\\in(0,1]$, since $\\\\gamma$ is continuous (a similar statements holds for $t+\\\\epsilon$ instead of $t-\\\\epsilon$). This means that all permutations in consecutive values of $t\\\\mapsto\\\\operatorname{argsort}(\\\\gamma(t))$ are neighbors in $G$. Now let us consider\\n\\n$$t_{0}=\\\\sup\\\\{t\\\\in[0,1]:\\\\forall\\\\nu\\\\in\\\\operatorname{argsort}(\\\\gamma(t)),\\\\nu \\\\text{ is connected to }\\\\sigma\\\\text{ in }G\\\\}.$$\\n\\n$t_{0}$ is well defined since all permutations of $\\\\gamma(0)$ are connected to $\\\\sigma$ in $G$. First notice that since $\\\\operatorname{argsort}$ is u.h.c. (Lemma 21 point _v_), the $\\\\sup$ above is actually a $\\\\max$. We now prove that it implies that all permutations in $\\\\operatorname{argsort}(\\\\gamma(t))$ are connected to $\\\\sigma$ in $G$. Aiming for a contradiction, assume $t_{0}<1$. Then, by the remark above, there is $\\\\epsilon>0$ such that $\\\\operatorname{argsort}(\\\\gamma(t_{0}+\\\\epsilon))\\\\subseteq\\\\operatorname{argsort }(\\\\gamma(t_{0}))$. Then, all permutations of $\\\\operatorname{argsort}(\\\\gamma(t_{0}+\\\\epsilon))$ are connected in $G$ to all permutations of $\\\\operatorname{argsort}(\\\\gamma(t_{0}))$, and are thus connected to $\\\\sigma$ in $G$. This contradicts the definition of $t_{0}$.\\n\\nThus, all permutations in $\\\\gamma(1)$ are connected in $G$ to all permutations in $\\\\gamma(0)$. To finish the proof, let us remind that by the Step 1 above, for all $t$, all permutations within $\\\\operatorname{argsort}(\\\\gamma(t))$ are connected by a path (of adjacent transpositions) in $\\\\mathfrak{S}_{n}$. Thus, any two permutations connected in $G$ are also connected by a path of adjacent transpositions. Thus, $\\\\sigma$ and $\\\\sigma^{\\\\prime}$ are connected by a path of adjacent transpositions. \\n\\n### Main property of Losses with connected argmins\\n\\nTo clarify the results of the section, we state the connectedness of the argmins of the losses as an assumption:\\n\\n**(A4)**.: $\\\\forall q\\\\in\\\\mathcal{Q},\\\\operatorname{argmin}\\\\ell(q,.)$ _is connected in_ $\\\\mathcal{Z}$_._\\n\\nThe proof of the final result is the combination of the following observation, which is a general property of preorders, together with the lemma that follows.\\n\\nThe general observation is the following: Given two preorders $z$ and $w$ that have the same (strict) preferences between two items $i$ and $j$, then we can find a path of adjacent transpositions between them that never transposes $i$ and $j$. Thus the strict preferences between $i$ and $j$ are kept constant along the path.\\n\\n**Lemma 37**.: $\\\\forall z,w\\\\in\\\\mathcal{Z},\\\\forall i,j\\\\in[n]$ _s.t. $i\\\\succ_{z}j$ and $i\\\\succ_{w}j$, $\\\\exists M\\\\in\\\\mathbb{N},\\\\exists(i_{m},i^{\\\\prime}_{m})_{m=0}^{M}\\\\in[n]^{2M}$ s.t. denoting $z_{0}=z$ and $z_{m}=\\\\tau_{i_{m},i^{\\\\prime}_{m}}z_{m-1}$, we have_\\n\\n1. $\\\\forall m\\\\in[M],i_{q},i^{\\\\prime}_{m}$ _are adjacent in_ $z_{m-1}$_,_\\n2. $z_{M}=w$_,_\\n3. $\\\\forall m\\\\in[M],\\\\{k_{m},k^{\\\\prime}_{m}\\\\}\\\\neq\\\\{i,j\\\\}$_._\\n\\nProof.: For two permutations, the existence of such a path is given by bubble sort: since bubble sort is monotonic (see Remark 35), it never exchanges two items that are in an ordering compatible with the target weak order. The result in $\\\\mathcal{Z}$ follows by compositing with $\\\\Lambda$ the path between $\\\\sigma\\\\in z$ and $\\\\sigma^{\\\\prime}\\\\in z^{\\\\prime}$. \\n\\nThe next lemma is the main technical step in the proof of our final result, and transposes the previous lemma in sequences of argmins for different distributions, when the argmins are always connected. In a similar way to the path between preorders above, the lemma considers a distribution for which the argmin of the loss is a single preorder $z$, and two fixed items $i,j$. Then, it shows that for any two adjacent items $\\\\{k,l\\\\}\\\\in z$, different from $\\\\{i,j\\\\}$, we can find a distribution for which the argmin is $\\\\tau_{kl}z$, which at the same time preserves strict preferences (by the inner risk) between $z^{\\\\prime}$ and $\\\\tau_{ij}z^{\\\\prime}$:\\n\\n**Lemma 38** (Adjacent transpositions of argmin).: _Under assumptions (A1), (A2) and (A3) and (A4): $\\\\forall q,z$ s.t. $\\\\operatorname{argmin}\\\\ell(q,.)=\\\\{z\\\\}$, $\\\\forall i,j,k,l\\\\in[n]$ s.t. $\\\\{i,j\\\\}\\\\neq\\\\{k,l\\\\}$ and $(k,l)$ are adjacent in $z$, we have: $\\\\exists q^{\\\\prime}\\\\in\\\\mathcal{Q}$ s.t._\\n\\n1. $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)=\\\\{\\\\tau_{kl}z\\\\}$_,_2. $\\\\forall z^{\\\\prime},\\\\ell(q,z^{\\\\prime})>\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})\\\\Rightarrow\\\\ell(q^ {\\\\prime},z^{\\\\prime})>\\\\ell(q^{\\\\prime},\\\\tau_{ij}z^{\\\\prime})$__\\n\\nProof.: Note that the result trivially holds with $q^{\\\\prime}=q$ when $k\\\\sim_{z}l$, since in that case $\\\\tau_{kl}z=z$ (by (2) d)). We thus focus on the case $k\\\\not\\\\sim_{z}l$. Without loss of generality, we assume $l\\\\succ_{z}k$.\\n\\n**First case:**$k\\\\neq i$**and**$k\\\\neq j$**.** Consider $q_{\\\\alpha}=(1-\\\\alpha)q+\\\\alpha q_{\\\\mathrm{top}}^{(k)}$ and let $\\\\alpha_{0}=\\\\inf\\\\{\\\\alpha:\\\\{z\\\\}\\\\neq\\\\mathrm{argmin}\\\\,\\\\ell(q_{\\\\alpha},.)\\\\}$. Notice that $\\\\alpha_{0}$ is well defined since all $z^{\\\\prime}\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q_{1},.)$ must have $z^{\\\\prime-1}(k)=1$ by definition of $q_{\\\\mathrm{top}}^{(k)}$, while $z^{-1}(k)>z^{-1}(l)$ by our assumption $l\\\\succ_{z}k$. Moreover, since $\\\\mathrm{argmin}\\\\,\\\\ell(.,.)$ is uc, the infimum is a minimum, and thus $\\\\alpha_{0}>0$ since $\\\\mathrm{argmin}\\\\,\\\\ell(q_{0},.)=\\\\{z\\\\}$.\\n\\nLet $q^{\\\\prime}=q_{\\\\alpha_{0}}$. Since $\\\\mathrm{argmin}\\\\,\\\\ell(.,.)$ is uc, we have that $\\\\exists\\\\epsilon>0$ such that $\\\\mathrm{argmin}\\\\,\\\\ell(\\\\mathcal{B}_{\\\\infty}(q^{\\\\prime},\\\\epsilon),.)=\\\\mathrm{ argmin}\\\\,\\\\ell(q^{\\\\prime},.)$. Taking one such $\\\\epsilon$ with the additional requirement $\\\\epsilon<\\\\alpha_{0}$, we have:\\n\\n1. $z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$, because $\\\\{z\\\\}=\\\\mathrm{argmin}\\\\,\\\\ell(q_{\\\\alpha_{0}-\\\\epsilon},.)\\\\subseteq\\\\mathrm{argmin }\\\\,\\\\ell(q^{\\\\prime},.)$. The first equality comes from the definition of $\\\\alpha_{0}$ and the inclusion comes from the choice of $\\\\epsilon$.\\n2. $\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)\\\\neq\\\\{z\\\\}$, since the infimum in the definiton of $\\\\alpha_{0}$ is a minimum.\\n\\nLet $z^{\\\\prime}\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$ with $z^{\\\\prime}\\\\neq z$. Since $\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$ is connected by (A4), Lemma 36 shows that there is a path of adjacent transpositions between $z$ and $z^{\\\\prime}$ in $\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$. In particular, there exists two items $k^{\\\\prime},l^{\\\\prime}$ adjacent in $z$ such that $\\\\tau_{k^{\\\\prime},l^{\\\\prime}}z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$ and $\\\\tau_{k^{\\\\prime},l^{\\\\prime}}z\\\\neq z$. The result follows from the three additional remarks:\\n\\n1. We necessarily have $k^{\\\\prime}=k$ or $l^{\\\\prime}=k$. Indeed, aiming for a contradiction, assume $k\\\\not\\\\in\\\\{k^{\\\\prime},l^{\\\\prime}\\\\}$. By the definition of $q_{\\\\mathrm{top}}^{(k)}$, we have $\\\\ell(q_{\\\\mathrm{top}}^{(k)},\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z)=\\\\ell(q_{\\\\mathrm{top} }^{(k)},z)$, and thus $$\\\\ell(q^{\\\\prime},\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z) =(1-\\\\alpha_{0})\\\\ell(q,\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z)+\\\\alpha_{0} \\\\ell(q_{\\\\mathrm{top}}^{(k)},z)$$ (6) $$>(1-\\\\alpha_{0})\\\\ell(q,z)+\\\\alpha_{0}\\\\ell(q_{\\\\mathrm{top}}^{(k)},z) \\\\text{because }\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z\\\\not\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q,.)$$ $$=\\\\ell(q^{\\\\prime},z)$$ which contradicts $\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$.\\n2. w.l.o.g., let $k^{\\\\prime}=k$. Then $\\\\ell(q^{\\\\prime},\\\\tau_{kl^{\\\\prime}}z)=\\\\ell(q^{\\\\prime},\\\\tau_{kl}z)$ and thus $\\\\tau_{kl}z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$. Indeed, since $\\\\tau_{k^{\\\\prime}l^{\\\\prime}}z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q^{\\\\prime},.)$, this means $\\\\ell(q_{\\\\mathrm{top}}^{(k)},\\\\tau_{kl^{\\\\prime}}z)<\\\\ell(q_{\\\\mathrm{top}}^{(k)},z)$ and thus $z^{-1}(l^{\\\\prime})<z^{-1}(k)$ by the definition of $q_{\\\\mathrm{top}}^{(k)}$. Since $k$ and $l^{\\\\prime}$ are adjacent in $z$, and the rank of $l^{\\\\prime}$ is strictly smaller than that of $l$, we have $z^{-1}(l^{\\\\prime})=z^{-1}(l)$ and thus $l\\\\sim_{z}l^{\\\\prime}$. This implies $\\\\ell(q^{\\\\prime},\\\\tau_{kl^{\\\\prime}}z)=\\\\ell(q^{\\\\prime},\\\\tau_{kl}z)$.\\n3. $\\\\forall z^{\\\\prime\\\\prime},\\\\ell(q^{\\\\prime},z^{\\\\prime\\\\prime})-\\\\ell(q^{\\\\prime}, \\\\tau_{ij}z^{\\\\prime\\\\prime})=(1-\\\\alpha_{0})\\\\big{(}\\\\ell(q,z^{\\\\prime\\\\prime})-\\\\ell( q,\\\\tau_{ij}z^{\\\\prime\\\\prime})\\\\big{)}$. This follows from the same calculation as (6), using the definition of $q_{\\\\mathrm{top}}^{(k)}$ and $k\\\\not\\\\in\\\\{i,j\\\\}$. This remark implies that strict inequalities between $\\\\ell(q,z^{\\\\prime\\\\prime})$ and $\\\\ell(q,\\\\tau_{ij}z^{\\\\prime\\\\prime})$ are preserved in $q^{\\\\prime}$.\\n\\nThe final result immediately follows, using the tie breaking lemma (Lemma 15) to find some other distribution for which $\\\\tau_{k^{\\\\prime}l}z$ is the unique element of the argmin.\\n\\n**Second case:**$k\\\\in\\\\{i,j\\\\}$**.** Then, by assumption we have $l\\\\neq i$ and $l\\\\neq j$. In that case, we follow similar steps, using $q_{\\\\mathrm{bottom}}^{(l)}$ instead of $q_{\\\\mathrm{top}}^{(k)}$. \\n\\n### Strict monotonicity for losses with connected argmin\\n\\nThis section provides the final proof. The proof of the main result (coonected argms are equivalent to the existence of a utility function) is based on the argument of _strict monotocity_ of task losses with connected argms (Lemma 40 and Corollary 41). In essence, the strict monotonicity extends to the entire task loss the property of its argmin, when these are connected: whenever item $i$ is preferred to item $j$ in the argmin, it is always preferred. The strict monotonicity allows to directly prove the existence of a utility function (Theorem 42). The utility function itself proves the calibration of the square loss, which in turn implies the connectedness of argmins using the characterization of calibration based on equality of argmins (Theorem 28). The strict monotonicity also readily implies connectedness of all sublevel sets, since it shows we can find a path between any permutation and an optimal permutation that never increases the task loss.\\n\\nBefore going to the proofs of strict monotonicity, we give the following alternative to the tie-breaking lemma (Lemma 15)3:\\n\\nFootnote 3: Note that even though tie breaking lemmas are purely technical, they are very important because connectedness is materialized by ties. There are two ways to break ties: the tie breaking lemma chooses the argmin, but cannot guarantee that other ties outside the argmin are suitably broken. On the other hand, Lemma 39 breaks a specific tie, but cannot choose precisely the resulting argmin.\\n\\n**Lemma 39**.: _Under assumptions (A1), (A2) and (A3):_\\n\\n_Let $q\\\\in\\\\mathcal{Q}$ and $i,j,z^{\\\\prime}$ such that $i\\\\succ_{z^{\\\\prime}}j$. If $\\\\ell(q,z^{\\\\prime})=\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})$, then $\\\\exists q^{\\\\prime}\\\\in\\\\mathcal{Q}$, such that $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)\\\\subseteq\\\\operatorname{argmin}\\\\ell(q,.)$ and $\\\\ell(q^{\\\\prime},z^{\\\\prime})>\\\\ell(q^{\\\\prime},\\\\tau_{ij}z^{\\\\prime})$._\\n\\nProof.: Let $\\\\epsilon=\\\\min_{z_{1},z_{2}:\\\\ell(q,z_{1})\\\\neq\\\\ell(q,z_{2})}|\\\\ell(q,z_{1})-\\\\ell (q,z_{2})|$, take $\\\\alpha$ such that $0<\\\\alpha<\\\\frac{\\\\epsilon}{\\\\epsilon+\\\\max_{z_{0}}\\\\ell(q,z_{0})}$, and define $q^{\\\\prime}=(1-\\\\alpha)q+\\\\alpha q^{(\\\\tau_{ij}z^{\\\\prime})}$ (as defined in Lemma 14). The choice of $\\\\alpha$ makes sure that any strict preferences between any $z_{1}$ and $z_{2}$ by $\\\\ell(q,.)$ are preserved in $\\\\ell(q^{\\\\prime},.)$. Only ties in $\\\\ell(q,.)$ can be changed in strict inequalities in $\\\\ell(q^{\\\\prime},.)$. Thus $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)\\\\subseteq\\\\operatorname{argmin}\\\\ell(q,.)$. Moreover, since $\\\\ell(q^{(\\\\tau_{ij}z^{\\\\prime})},\\\\tau_{ij}z^{\\\\prime})<\\\\ell(q^{(\\\\tau_{ij}z^{ \\\\prime})},,z^{\\\\prime})$ by definition of $q^{(\\\\tau_{ij}z^{\\\\prime})}$, we have $\\\\ell(q,z^{\\\\prime})>\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})$, which is the desired result. \\n\\nWe decribe the _strict monotonicity_ property of losses with connected argmins (A4) in the next results.\\n\\n**Lemma 40** (Strict monotonicity, base case).: _Under assumptions (A1), (A2) and (A3) and (A4), we have:_\\n\\n$$\\\\forall q,z\\\\text{ s.t. }\\\\operatorname{argmin}\\\\ell(q,.)=\\\\{z\\\\}\\\\text{: }\\\\forall i,j\\\\text{ such that }i\\\\succ_{z}j\\\\text{, }\\\\forall z^{\\\\prime}\\\\text{ such that }i\\\\succ_{z^{\\\\prime}}j\\\\text{, }\\\\ell(q,z^{\\\\prime})<\\\\ell(q,\\\\tau_{ij}z^{\\\\prime}).$$\\n\\nProof.: Let $i,j$ such that $i\\\\succ_{z}j$ and $z^{\\\\prime}$ such that $i\\\\succ_{z^{\\\\prime}}j$. By Lemma 37, there is a path of adjacent transpositions between $z$ and $z^{\\\\prime}$ that never swaps $i$ and $j$. Let us denote by $z_{0},...,z_{M}$ such a path (with $z_{0}=z$ and $z_{M}=z^{\\\\prime}$). Since we never swap $i$ and $j$, denoting $k_{m},k_{m^{\\\\prime}}$ the adjacent items of $z_{m-1}$ that are swapped between $z_{m-1}$ and $z_{m}$, we have $\\\\{i,j\\\\}\\\\neq\\\\{k_{m},k_{m}^{\\\\prime}\\\\}$. We can thus apply Lemma 38, and find $q_{0}=q,q_{1},...,q_{M}$ such that:\\n\\n1. $\\\\forall m\\\\in[M],\\\\ \\\\operatorname{argmin}\\\\ell(q_{m},.)=\\\\{z_{m}\\\\}$\\n2. $\\\\forall m\\\\in[M],\\\\ \\\\ \\\\forall z^{\\\\prime\\\\prime}\\\\text{ such that }\\\\ell(q_{m-1},z^{\\\\prime\\\\prime})>\\\\ell(q_{m-1},\\\\tau_{ij}z^{\\\\prime\\\\prime})$, we have $\\\\ell(q_{m},z^{\\\\prime\\\\prime})>\\\\ell(q_{m},\\\\tau_{ij}z^{\\\\prime\\\\prime})$\\n\\nAn immediate consequence is that\\n\\n$$\\\\ell(q,z^{\\\\prime})\\\\leq\\\\ell(q,\\\\tau_{ij}z^{\\\\prime}).$$\\n\\nIndeed, aiming for a contradiction, assume that $\\\\ell(q,z^{\\\\prime})>\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})$. Then by point 2. above, the sign of the difference would be kept along the path, all the way to $q_{M}$, i.e. we should have $\\\\ell(q_{M},z^{\\\\prime})>\\\\ell(q_{M},\\\\tau_{ij}z^{\\\\prime})$. But $z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{M},.)$, so this impossible.\\n\\nNow, by Lemma 39, if we had $\\\\ell(q,z^{\\\\prime})=\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})$, we could find $q^{\\\\prime\\\\prime}$ with $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime\\\\prime},.)=\\\\{z\\\\}$ and $\\\\ell(q^{\\\\prime\\\\prime},z^{\\\\prime})>\\\\ell(q^{\\\\prime\\\\prime},\\\\tau_{ij}z^{\\\\prime})$, which is impossible as we just stated. Thus, we have $\\\\ell(q,z^{\\\\prime})<\\\\ell(q,\\\\tau_{ij}z^{\\\\prime})$. \\n\\nThe lemma above extends to all possible $q$, not only those for which the argmin is a single element, in the following way:\\n\\n**Corollary 41** (Strict monotonicity).: _Under the assumptions of Lemma 40, let $q\\\\in\\\\mathcal{Q}$ and $i,j\\\\in[n]$. There are three cases:_1. _if_ $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z}j$_, then_ $\\\\forall z^{\\\\prime}:i\\\\succ_{z}j,\\\\ell(q,z^{\\\\prime})<\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$_;_\\n2. _if_ $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succeq_{z}j$ _and_ $\\\\exists z_{0}\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z_{0}}j$_, then:_ $\\\\forall z^{\\\\prime}:i\\\\succ_{z^{\\\\prime}}j,\\\\ell(q,z^{\\\\prime})\\\\leq\\\\ell(q,\\\\tau_{i, j}z^{\\\\prime})$_._ _(Notice in that case_ $\\\\ell(q,z_{0})<\\\\ell(q,\\\\tau_{i,j}z_{0})$_.)_\\n3. _if_ $\\\\exists z_{0},z_{1}\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ _such that_ $i\\\\succ_{z_{0}}j$ _and_ $j\\\\succ_{z_{1}}i$_, then_ $\\\\forall z^{\\\\prime}:\\\\ell(q,z^{\\\\prime})=\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$_._\\n\\nProof.:\\n1. Let $q$ such that $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z}j$. Aiming for a contradiction, assume $\\\\ell(q,z^{\\\\prime})>\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$. Let $z\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$. By the tie-breaking lemma (Lemma 15), we could find $q^{\\\\prime}$ with $\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)=\\\\{z\\\\}$ and $\\\\ell(q^{\\\\prime},z^{\\\\prime})>\\\\ell(q^{\\\\prime},\\\\tau_{i,j}z^{\\\\prime})$, but since $i\\\\succ_{z}j$ this contradicts the base case (Lemma 40). We thus have $\\\\ell(q,z^{\\\\prime})\\\\leq\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$. However, by Lemma 39, $\\\\ell(q,z^{\\\\prime})=\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$ would also yield a contradiction. We thus have $\\\\ell(q,z^{\\\\prime})<\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$, which is the desired result.\\n2. $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succeq_{z}j$ and $\\\\exists z_{0}\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z_{0}}j$. With the same arguments as above, taking $z\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ such that $i\\\\succ_{z}j$ (which exists by assumption) when using the tie-breaking lemma, we obtain $\\\\forall z^{\\\\prime},\\\\ell(q,z^{\\\\prime})\\\\leq\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$.\\n3. Let $q$ such that $\\\\exists z_{0},z_{1}\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ such that $i\\\\succ_{z_{0}}j$ and $j\\\\succ_{z_{1}}i$. If there is $z^{\\\\prime}$ such that $i\\\\succ_{z}j,\\\\ell(q,z^{\\\\prime})>\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$ then we can use the tie-breaking lemma with $z_{0}$ and that would contradicts the base case Lemma 40. Likewise, a $z^{\\\\prime}$ such that $i\\\\succ_{z}j,\\\\ell(q,z^{\\\\prime})<\\\\ell(q,\\\\tau_{i,j}z^{\\\\prime})$ would contradict the base case after applying the tie breaking lemma with $z_{1}$. Thus, equality must hold for all $z^{\\\\prime}$.\\n\\nThe following result is a direct consequence of the strict monotonicity, and is the main result of the paper: there is a utility function wuch that the expected utility gives an optimal scoring function:\\n\\n**Theorem 42**.: _Let $\\\\tilde{u}:\\\\mathcal{Y}\\\\to\\\\mathbb{R}^{n}$ defined as: $\\\\tilde{u}_{i}(y)=-\\\\sum_{z\\\\in\\\\mathcal{Z}:z^{-1}(i)=1}L(y,z)$._\\n\\n_For $q\\\\in\\\\mathcal{Q}$, denote the expected utility by $\\\\tilde{U}(q)=\\\\sum_{y\\\\in\\\\mathcal{Y}}q_{y}\\\\tilde{u}(y)$._\\n\\n_Under assumptions (A1), (A2) and (A3) and (A4), we have:_\\n\\n$$\\\\operatorname{argmin}\\\\ell(q,.)=\\\\operatorname{pred}(\\\\tilde{U}(q))$$\\n\\nObviously, utilities are cardinal, in the sense that any affine transformation of $u$ is also a utility.\\n\\nProof.: Let $q\\\\in\\\\mathcal{Q}$ and $i,j\\\\in[n]$. From the definition of $u$ and the linearity (w.r.t. $q$) of $\\\\tilde{U}$, we have:\\n\\n$$\\\\tilde{U}_{i}(q)-\\\\tilde{U}_{j}(q) =\\\\sum_{z\\\\in\\\\mathcal{Z}:z^{-1}(j)=1}\\\\ell(q,z)-\\\\sum_{z\\\\in\\\\mathcal{Z }:z^{-1}(j)=1}\\\\ell(q,z)$$ $$=\\\\sum_{z\\\\in\\\\mathcal{Z}:z^{-1}(i)=1}\\\\left(\\\\ell(q,\\\\tau_{ij}z)-\\\\ell( q,z)\\\\right).$$\\n\\nbecause $\\\\{\\\\tau_{ij}z\\\\in\\\\mathcal{Z}:z^{-1}(j)=1\\\\}=\\\\{z\\\\in\\\\mathcal{Z}:z^{-1}(i)=1\\\\}$. Thus, coming back to the three cases of Lemma 41, we immediately have:\\n\\n1. if $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z}j$, then $\\\\tilde{U}_{i}(q)>\\\\tilde{U}_{j}(q)$;\\n2. if $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succeq_{z}j$ and $\\\\exists z_{0}\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z_{0}}j$, then $\\\\tilde{U}_{i}(q)>\\\\tilde{U}_{j}(q)$; (The strict inequality comes from $\\\\ell(q,z_{0})<\\\\ell(q,\\\\tau_{i,j}z_{0})$.)* if $\\\\exists z_{0},z_{1}\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ such that $i\\\\succ_{z_{0}}j$ and $j\\\\succ_{z_{1}}i$, then $\\\\tilde{U}_{i}(q)=\\\\tilde{U}_{j}(q)$\\n\\nThe second and main step of the theorem is to show $\\\\operatorname{argsort}(\\\\tilde{U}(q))\\\\subseteq\\\\bigcup_{z\\\\in\\\\operatorname{ argmin}\\\\ell(q,.)}z$. The main arguments consist in the following disjunction of cases. Denoting $\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)=\\\\bigcup_{z\\\\in\\\\operatorname{argmin}\\\\ell(q,. )}z$, we have:\\n\\n* if $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z}j$, then $\\\\forall\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{\\\\sigma}j$;\\n* if $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succeq_{z}j$ and $\\\\exists z_{0}\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\succ_{z_{0}}j$, then $\\\\forall\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$ we either have $i\\\\succ_{\\\\sigma}j$ or $\\\\tau_{i,j}\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$.\\n* if $\\\\exists z_{0},z_{1}\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ such that $i\\\\succ_{z_{0}}j$ and $j\\\\succ_{z_{1}}i$, then $\\\\forall\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$, we have $\\\\tau_{ij}\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$ (because $\\\\ell(q,\\\\Lambda(\\\\sigma))=\\\\ell(q,\\\\Lambda(\\\\tau_{ij}\\\\sigma))$ by _iii)_ of Corollary 41).\\n* if $\\\\forall z\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\sim_{z}j$, then $\\\\forall\\\\sigma\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$, we have $\\\\tau_{ij}\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$ (by definition of $\\\\sim_{z}$).\\n\\nThe critical implication of points _a-d)_ above is that for all $\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$, if $\\\\tilde{U}_{i}(q)>\\\\tilde{U}_{i}(q)$ then $\\\\sigma$ either gives a relative ordering of $i,j$ properly, or $\\\\tau_{ij}\\\\sigma\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$. If $\\\\tilde{U}_{i}(q)=\\\\tilde{U}_{i}(q)$, which happens only in cases _c)_ and _d)_ above, then $\\\\{\\\\sigma,\\\\tau_{ij}\\\\sigma\\\\}\\\\subseteq\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$. Successive applications of this remark allows us to construct $\\\\sigma\\\\in\\\\operatorname{argsort}(\\\\tilde{U}(q))\\\\cap\\\\bigcup_{z\\\\in\\\\operatorname{ argmin}\\\\ell(q,.)}$: start from an arbitrary $\\\\sigma\\\\in\\\\bigcup_{z\\\\in\\\\operatorname{argmin}\\\\ell(q,.)}$, and take any $\\\\sigma^{\\\\prime}\\\\in\\\\operatorname{argsort}(\\\\tilde{U}(q))$. Apply bubble sort starting from $\\\\sigma$ with target $\\\\sigma^{\\\\prime}$. By the remark above, every step of bubble sort stays in $\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$, which proves $\\\\sigma^{\\\\prime}\\\\in\\\\bigcup\\\\operatorname{argmin}\\\\ell(q,.)$, and thus $\\\\operatorname{argsort}(\\\\tilde{U}(q))\\\\subset\\\\bigcup\\\\operatorname{argmin}\\\\ell (q,.)$.\\n\\nWe thus proved $\\\\forall q\\\\in\\\\mathcal{Q},\\\\operatorname{pred}(\\\\tilde{U}q)\\\\subset\\\\operatorname{ argmin}\\\\ell(q,.)$. Equality is proved since this inclusion proves the square loss $y,s\\\\mapsto(s-\\\\tilde{u}(z))^{2}$ is $L$-calibrated, so equality follows from equality of argmins (Theorem 28).\\n\\nNow, we have all the tools to wrap up the proof of Theorem 32.\\n\\nProof of Theorem 32.:\\n\\n**(i) $\\\\Rightarrow$ (ii).** As $L$ is CEU, there exists $u$ such that $\\\\Phi_{u}^{\\\\text{sq}}$ is $L$-calibrated. Thanks to Th. 28, we know\\n\\n$$\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\delta_{0}>0,\\\\forall 0<\\\\delta<\\\\delta_{0}, \\\\operatorname{pred}(\\\\operatorname{lev}_{\\\\delta}\\\\phi_{u}^{\\\\text{sq}}(q,.))= \\\\operatorname{argmin}\\\\ell(q,.)$$\\n\\nAs $\\\\Phi_{u}^{\\\\text{sq}}$ is convex in $s$ and thus $\\\\operatorname{lev}_{\\\\delta}\\\\phi_{u}^{\\\\text{sq}}(q,.)$ is connected, then $\\\\forall q\\\\in\\\\mathcal{Q},\\\\ \\\\operatorname{argmin}\\\\ell(q,.)$ is connected.\\n\\n**(ii) $\\\\Rightarrow$ (i).** This is exactly Th. 42.\\n\\n**(ii) $\\\\Rightarrow$ (iii).** By Cor. 41, we have the property we referred to as _strict monotonicity_ of the loss $L$. We use Lemma 36 to prove the connectedness by using path of adjacent items. Let us take $q\\\\in\\\\mathcal{Q}$, $\\\\varepsilon>0$ and $z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$ and $\\\\sigma\\\\in z$. We know there exists $s\\\\in\\\\mathbb{R}^{n}$ such that $\\\\operatorname{argmin}\\\\ell(q,.)=\\\\operatorname{pred}(s)$ (from (ii) $\\\\Rightarrow$ (i)), thus we can find $\\\\nu\\\\in\\\\mathfrak{S}_{n}$ such that $\\\\operatorname{pred}(\\\\nu)\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ and for any $i,j$ such that $\\\\tau_{ij}\\\\in\\\\mathtt{bubblesort}($target$=$$\\\\nu$, input=$\\\\sigma$), we have $\\\\forall z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q,.),i\\\\sim_{z}j$. Indeed, if there were $\\\\tau_{ij}\\\\in\\\\mathtt{bubblesort}($target$=$$\\\\nu$, input=$\\\\sigma$) such that $\\\\exists z^{\\\\prime}\\\\operatorname{argmin}\\\\ell(q,.),i\\\\geq_{z}j$ because $\\\\operatorname{argmin}\\\\ell(q,.)=\\\\operatorname{pred}(s)$ then $\\\\tau_{ij}\\\\nu\\\\in\\\\operatorname{argsort}(s)$ and we could choose $\\\\tau_{ij}\\\\nu$ instead of $\\\\nu$.\\n\\nDenoting $(\\\\tau^{(m)})_{m=1}^{M}$ the output of $\\\\mathtt{bubblesort}($target$=$$\\\\nu$, input=$\\\\sigma$$)$, we just prove $\\\\tau^{(1)}z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$, and then finish by induction.\\n\\n_Case 1: $i\\\\sim_{z}j$_. In this case $\\\\tau_{ij}z=z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$.\\n\\n_Case 2: $i\\\\succ_{z}j$_. By Cor. 41 (_i)_, $\\\\ell(q,\\\\tau_{ij}z)>\\\\ell(q,z)$. Hence $\\\\tau_{ij}z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$. Then, by induction, for any $m\\\\in[M]$ we have $\\\\tau^{(m)}\\\\ldots\\\\tau^{(1)}z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$.\\n\\nFinally, any $z\\\\in\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$ is connected to an element of $\\\\operatorname{argmin}\\\\ell(q,.)$ which is itself a connected set. Thus, $\\\\operatorname{lev}_{\\\\varepsilon}\\\\ell(q,.)$ is connected.\\n\\n**(iii) $\\\\Rightarrow$ (ii).** Because $\\\\forall q\\\\in\\\\mathcal{Q},\\\\exists\\\\varepsilon>0,\\\\operatorname{lev}_{\\\\varepsilon} \\\\ell(q,.)=\\\\operatorname{argmin}\\\\ell(q,.)$Proof of Corollary 7\\n\\n_Disclaimer: As mentioned in the body of the paper, we address, in these Appendices, a setting more general than ranking. This setting is described in Section A. We restate first the statement of the paper (on ranking), then state the more general statement (on weak orders) and prove the latter._\\n\\nThis section provides the proof of Corollary 7. First, we remind the exact statement of the paper in the ranking setting.\\n\\n**Corollary 7**.: _A ranking loss $L$ is CEU if and only if: for every distribution $P$ over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$ such that $x\\\\mapsto P(.|x)$ is continuous, there is a continuous optimal scoring function for $\\\\mathcal{R}_{L,P}$._\\n\\nThen, the version of the same statement in $\\\\mathcal{Z}$,\\n\\n**Corollary 43**.: _Under (A1), (A2) and (A3), the task loss $L$ is CEU if and only if for every distribution $P$ over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$ such that $x\\\\mapsto P(.|x)$ is continuous, there is a continuous optimal scoring function for $\\\\mathcal{R}_{L,P}$ - i.e. a function $f:\\\\mathcal{X}\\\\to\\\\mathbb{R}^{n}$ continuous such that $\\\\forall x\\\\in\\\\mathcal{X}$, $\\\\operatorname{pred}(f(x))\\\\subseteq\\\\operatorname{argmin}\\\\ell(P(.|x),.)$._\\n\\nThe proof of that corollary, as well as all proofs regarding local minima when there are disconnected argmins rely on the following path between probability distributions $\\\\bar{q}$. The proof is straightforward.\\n\\n**Lemma 44**.: _Let $q_{0}$ such that $|\\\\operatorname{argmin}\\\\ell(q_{0},.)|>1$. Let $z,z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$ with $z\\\\neq z^{\\\\prime}$. Define the following path between probability distributions in $\\\\mathcal{Q}$:_\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\ \\\\ \\\\bar{q}(\\\\alpha)=\\\\begin{cases}(1-2\\\\alpha)q^{(z)}+2 \\\\alpha q_{0}&\\\\text{if }\\\\alpha\\\\in[0,\\\\frac{1}{2}]\\\\\\\\ (2-2\\\\alpha)q_{0}+(2\\\\alpha-1)q^{(z^{\\\\prime})}&\\\\text{if }\\\\alpha\\\\in[\\\\frac{1}{2},1] \\\\end{cases}.$$\\n\\n_Then, let_\\n\\n$$\\\\epsilon_{0}=\\\\min_{q^{\\\\prime}\\\\in\\\\{q_{0},q^{(z)},q^{(z^{\\\\prime})}\\\\}\\\\,z^{\\\\prime \\\\prime}\\\\not\\\\in\\\\operatorname{argmin}\\\\ell(q^{\\\\prime},.)}\\\\big{(}\\\\ell(q^{\\\\prime},z^{\\\\prime\\\\prime})-\\\\underline{\\\\ell}(q^{\\\\prime})\\\\big{)}$$\\n\\n_Then, $\\\\epsilon>0$ and we have:_\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall z^{\\\\prime\\\\prime}\\\\in\\\\operatorname{lev}_{\\\\epsilon _{0}}\\\\ell(\\\\bar{q}(\\\\alpha),.),z^{\\\\prime\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_ {0},.).$$\\n\\nProof.: We prove the case $\\\\alpha\\\\in[0,\\\\frac{1}{2}]$, the other case is similar. Let $\\\\alpha\\\\in[0,\\\\frac{1}{2}]$. We first notice $z\\\\in\\\\operatorname{argmin}\\\\ell(\\\\bar{q}(\\\\alpha),.)$. Thus, by developing $\\\\bar{q}(\\\\alpha)$ we have, for $z^{\\\\prime\\\\prime}\\\\neq z$:\\n\\n$$\\\\ell(\\\\bar{q}(\\\\alpha),z^{\\\\prime\\\\prime})-\\\\underline{\\\\ell}(\\\\bar{q}(\\\\alpha))=(1-2 \\\\alpha)\\\\big{(}\\\\ell(q^{(z)},z^{\\\\prime\\\\prime})-\\\\ell(q^{(z)},z)\\\\big{)}+2\\\\alpha \\\\big{(}\\\\ell(q_{0},z^{\\\\prime\\\\prime})-\\\\ell(q_{0},z)\\\\big{)}\\\\geq\\\\epsilon.\\\\qed$$\\n\\nProof of Corollary 43.: The direct implication is straightforward: if $L$ is CEU, we can choose $f:x\\\\mapsto\\\\mathbb{E}_{P(.|x)}[u(Y)]$ as optimal scoring function by Theorem 32, which is continuous whenever $x\\\\mapsto P(.|x)$ is continuous.\\n\\nFor the reverse implication, let $q_{0}\\\\in\\\\mathcal{Q}$. We have to prove that $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ is connected. Notice that if $|\\\\operatorname{argmin}\\\\ell(q_{0},.)|=1$ it is connected, so we focus on the case $|\\\\operatorname{argmin}\\\\ell(q_{0},.)|>1$. Let $z,z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$ with $z\\\\neq z^{\\\\prime}$. Let $\\\\mathcal{X}=[0,1]$, and take the distribution $P$ over $\\\\mathcal{X}\\\\times\\\\mathcal{Y}$ such that the marginal distribution over $\\\\mathcal{X}$ is uniform and $x\\\\mapsto P(.|x)$ is the path $\\\\bar{q}$ constructed as in Lemma 44. If there is a continuous optimal scoring function $f:[0,1]\\\\to\\\\mathbb{R}^{n}$, it means that $\\\\forall\\\\alpha\\\\in[0,1]$, $\\\\operatorname{pred}(f(\\\\alpha))\\\\in\\\\operatorname{argmin}\\\\ell(\\\\bar{q}(\\\\alpha),.)$ and thus $\\\\operatorname{pred}(f([0,1]))\\\\subseteq\\\\operatorname{argmin}\\\\ell(q_{0},.)$ by Lemma 44.\\n\\nNotice that $f$ is continuous and thus preserves connectedness, so $\\\\operatorname{pred}(f([0,1]))$ is connected in $\\\\mathcal{Z}$ by definition. By the characterization of connectedness in $\\\\mathcal{Z}$ through paths of adjacent transpositions (Theorem 36), there is a path between $z$ and $z^{\\\\prime}$ in $\\\\operatorname{argmin}\\\\ell(q_{0},.)$. The construction above can be repeated for every $z,z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$, which implies that $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ is connected. \\n\\n## Appendix E Local Minima of the Task Loss\\n\\n### Proof of Theorem 46\\n\\n**Definition 8**.: _Given a distribution $q\\\\in\\\\mathcal{Q}$ and a loss $L$, a ranking $\\\\sigma\\\\in\\\\mathfrak{S}_{n}$ is a local minimum if for any $r\\\\in[n-1]$, $\\\\ell(q,\\\\sigma)\\\\leq\\\\ell(q,\\\\sigma\\\\tau_{r,r+1})$._The definition of local minima for ranking losses 8 is extended to $\\\\mathcal{Z}$ in a straightforward manner:\\n\\n**Definition 45**.: _Given a loss $L$ and $q\\\\in\\\\mathcal{Q}$, a prediction $z\\\\in\\\\mathcal{Z}$ is a local minimum if, for any pair of adjacent items $(i,j)$, we have $\\\\ell(q,z)\\\\leq\\\\ell(q,\\\\tau_{ij}z)$._\\n\\n**Theorem 46**.: _Under (A1), (A2) and (A3), if a task loss $L$ is not CEU, then the subset of distribution for which $\\\\ell$ has non-global local minima has non-zero measure._\\n\\nProof.: We use the following property of the excess inner risk, where we denote $\\\\left\\\\|L\\\\right\\\\|_{\\\\infty}=\\\\max_{y,z}\\\\left|L(y,z)\\\\right|$. The calculation is similar to the one later in Lemma 48 for surrogate losses.\\n\\n$$\\\\forall q,q^{\\\\prime},\\\\forall z,\\\\ \\\\ \\\\left|\\\\ell(q^{\\\\prime},z)-\\\\underline{\\\\ell}(q^{ \\\\prime})-\\\\left(\\\\ell(q,z)-\\\\underline{\\\\ell}(q)\\\\right)\\\\right|\\\\leq 2\\\\|q-q^{\\\\prime} \\\\|_{1}\\\\|L\\\\|_{\\\\infty}.$$\\n\\nLet us take $q_{0}$ such that $\\\\operatorname*{argmin}\\\\ell(q_{0},.)$ is not connected, and $z\\\\in\\\\operatorname*{argmin}\\\\ell(q_{0},.)$.\\n\\nLet $\\\\bar{q}(\\\\alpha)=(1-\\\\alpha)q_{0}+\\\\alpha q^{(z)}$. We define the _gap_ with respect to $q_{0}$ of $q\\\\in\\\\mathcal{Q}$ as:\\n\\n$$G(q)=\\\\min_{z^{\\\\prime}\\\\not\\\\in\\\\operatorname*{argmin}\\\\ell(q_{0},.)}\\\\big{(}\\\\ell(q,z^{\\\\prime})-\\\\underline{\\\\ell}(q)\\\\big{)}.$$\\n\\nWe use $\\\\epsilon_{0}=G(q_{0})$.\\n\\nThe important aspect of this gap is that if $z$ and $z^{\\\\prime}$ are both in $\\\\operatorname*{argmin}\\\\ell(q_{0},.)$ but not connected in $\\\\operatorname*{argmin}\\\\ell(q_{0},.)$, this disconnectedness remains in sublevel sets of $\\\\ell(q,.)$ for other distributions $q$: If, for some $\\\\epsilon>0$, $z$ and $z^{\\\\prime}$ are not connected in $\\\\operatorname*{lev}_{\\\\epsilon}\\\\ell(q,.)$ with $\\\\epsilon\\\\leq G(q)$, then $z$ and $z^{\\\\prime}$ are in different connected components of $\\\\operatorname*{lev}_{\\\\epsilon}\\\\ell(q,.)$. We prove the existence of bad local minima by showing that there are suboptimal connected components smaller than the gap.\\n\\nLet $\\\\alpha_{0}\\\\in(0,1]$. We then have $\\\\{z\\\\}=\\\\operatorname*{argmin}\\\\ell(\\\\bar{q}(\\\\alpha_{0}),.)$. Moreover, let $\\\\tilde{\\\\epsilon}=\\\\min_{z^{\\\\prime}\\\\neq z}(\\\\ell(q^{(z)},z^{\\\\prime})-\\\\underline{ \\\\ell}(q^{(z)}))$. Notice that $\\\\tilde{\\\\epsilon}>0$ by definition of $q^{(z)}$. We have4\\n\\nFootnote 4: The last equality comes from noticing $\\\\underline{\\\\ell}(\\\\bar{q}(\\\\alpha_{0}))=\\\\ell(\\\\bar{q}(\\\\alpha_{0}),z)$ and decomposing\\n\\n$$\\\\ell(\\\\bar{q}(\\\\alpha_{0}),z^{\\\\prime})-\\\\underline{\\\\ell}(\\\\bar{q}(\\\\alpha_{0}))= \\\\alpha_{0}\\\\big{(}\\\\ell(q^{(z)},z^{\\\\prime})-\\\\ell(q^{(z)},z)\\\\big{)}+(1-\\\\alpha_{0} )\\\\big{(}\\\\ell(q_{0},z^{\\\\prime})-\\\\ell(q_{0},z))\\\\big{)}.$$\\n\\n where the term in $q_{0}$ vanishes because both $z$ and $z^{\\\\prime}$ are optimal for $q_{0}$.\\n\\n$$G(\\\\bar{q}(\\\\alpha_{0}))\\\\geq\\\\epsilon_{0}-4\\\\alpha_{0}\\\\|L\\\\|_{\\\\infty},$$ $$\\\\forall z^{\\\\prime}\\\\in\\\\operatorname*{argmin}\\\\ell(q_{0},.),z^{\\\\prime }\\\\neq z,\\\\quad 4\\\\alpha_{0}\\\\|L\\\\|_{\\\\infty}\\\\geq\\\\ell(\\\\bar{q}(\\\\alpha_{0}),z^{\\\\prime})- \\\\underline{\\\\ell}(\\\\bar{q}(\\\\alpha_{0}))\\\\geq\\\\alpha_{0}\\\\tilde{\\\\epsilon}.$$\\n\\nAnd thus, given $\\\\eta>0$, for every $q^{\\\\prime}$ such that $\\\\|q^{\\\\prime}-\\\\bar{q}(\\\\alpha_{0})\\\\|_{1}\\\\leq\\\\eta$, we have\\n\\n$$G(q^{\\\\prime})\\\\geq\\\\epsilon_{0}-(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{\\\\infty},$$\\n\\nand $\\\\forall z^{\\\\prime}\\\\in\\\\operatorname*{argmin}\\\\ell(q_{0},.),z^{\\\\prime}\\\\neq z$:\\n\\n$$(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{\\\\infty}\\\\geq\\\\ell(q^{\\\\prime},z^{\\\\prime})-\\\\underline{ \\\\ell}(q^{\\\\prime})\\\\geq\\\\alpha_{0}\\\\tilde{\\\\epsilon}-2\\\\eta\\\\|L\\\\|_{\\\\infty}.$$\\n\\nLet $\\\\alpha_{0}\\\\in(0,1)$ and $\\\\eta\\\\in(0,1)$ such that\\n\\n$$0<\\\\alpha_{0}\\\\tilde{\\\\epsilon}-2\\\\eta\\\\|L\\\\|_{\\\\infty}<(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{ \\\\infty}<\\\\epsilon_{0}-(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{\\\\infty}.$$\\n\\nThese exist since $\\\\epsilon_{0}>0$. Then, for any $q^{\\\\prime}$ such that $\\\\|\\\\bar{q}(\\\\alpha_{0})-q^{\\\\prime}\\\\|<\\\\eta$, for any $z^{\\\\prime}\\\\in\\\\operatorname*{argmin}\\\\ell(q_{0},.)$ such that $z^{\\\\prime}$ is not in the same connected component as $z$, with such values of $\\\\alpha_{0}$ and $\\\\eta$, $z^{\\\\prime}$ is in a connected component of $\\\\operatorname*{lev}_{\\\\epsilon_{0}-(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{\\\\infty}}\\\\ell(q^{ \\\\prime},.)$ that is suboptimal (because $\\\\alpha_{0}\\\\tilde{\\\\epsilon}-2\\\\eta\\\\|L\\\\|_{\\\\infty}>0$ ) and disconnected from $z$ (because $G(q^{\\\\prime})\\\\geq\\\\epsilon_{0}-(4\\\\alpha_{0}+2\\\\eta)\\\\|L\\\\|_{\\\\infty}$). Thus, all such $q^{\\\\prime}$ have a bad local minimum. The result follows, since the measure of the $\\\\left\\\\|.\\\\right\\\\|_{1}$-ball of radius $\\\\eta$ is non-zero. \\n\\n## Appendix F Proof of Utility Computation on Generalized DCG\\n\\nThe expression of the utility $u$ from Theorem 6 may not be efficient to compute a priori. As it happens, for many evaluation metrics, the expression of $u$ does simplify for numerous common tasks losses.\\n\\n**Proposition 47**.: _We assume here a ranking task loss $L$ of the form $L(y,\\\\sigma)=\\\\operatorname{DCG}_{w,u}(y,\\\\sigma)=-\\\\sum_{k=1}^{n}w_{k}u_{\\\\sigma(k)}(y )-b(y)$ where $\\\\forall i\\\\in[n],u_{i}$ is increasing and $w$ is decreasing. Then, there exists $B:\\\\mathcal{Y}\\\\rightarrow\\\\mathbb{R}_{*}^{+}$ and $A>0$ such that, $\\\\forall i\\\\in[n],\\\\forall y\\\\in\\\\mathcal{Y}$,_\\n\\n$$\\\\tilde{u}_{i}(y)=B(y)+Au_{i}(y)$$\\n\\nProof.: By definition, we have,\\n\\n$$u_{i}(y) =-\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\mathds{1}_{[\\\\sigma(1)=i]}L(y,\\\\sigma)$$ $$=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\mathds{1}_{[\\\\sigma(1)=i]}\\\\left( b(y)+\\\\sum_{r=1}^{n}u_{\\\\sigma(r)}(y)w_{r}\\\\right)$$ $$=b(y)(n-1)!+\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\sum_{r=1}^{n} \\\\mathds{1}_{[\\\\sigma(1)=i]}u_{\\\\sigma(r)}(y)w_{r}$$ $$=b(y)(n-1)!+u_{i}(y)w_{1}(n-1)!+\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}} \\\\sum_{r=2}^{n}\\\\mathds{1}_{[\\\\sigma(1)=i]}u_{\\\\sigma(r)}(y)w_{r}$$ $$=b(y)(n-1)!+u_{i}(y)w_{1}(n-1)!+\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}} \\\\sum_{r=2}^{n}\\\\sum_{k\\\\neq i}\\\\mathds{1}_{[\\\\sigma(r)=k]}\\\\mathds{1}_{[\\\\sigma(1)= i]}u_{k}(y)w_{r}$$ $$=b(y)(n-1)!+u_{i}(y)w_{1}(n-1)!+\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k} (y)w_{r}\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\mathds{1}_{[\\\\sigma(r)=k]}\\\\mathds{1}_ {[\\\\sigma(1)=i]}$$ $$=b(y)(n-1)!+u_{i}(y)w_{1}(n-1)!+\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k} (y)w_{r}\\\\mathds{1}_{[k\\\\neq i]}(n-2)!$$ $$=b(y)(n-1)!+u_{i}(y)w_{1}(n-1)!-\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k} (y)w_{r}\\\\mathds{1}_{[k=i]}(n-2)!+\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k}(y)w_{r} \\\\Big{(}\\\\mathds{1}_{[k\\\\neq i]}+\\\\mathds{1}_{[k=i]}\\\\Big{)}(n-2)!$$ $$=b(y)(n-1)!+\\\\sum_{r=2}^{n}\\\\Big{(}u_{i}(y)w_{1}-u_{i}(y)w_{r}\\\\Big{)} (n-2)!+\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k}(y)w_{r}(n-2)!$$ $$=\\\\underbrace{b(y)(n-1)!+\\\\sum_{k=1}^{n}\\\\sum_{r=2}^{n}u_{k}(y)w_{r} (n-2)!}_{B(y)}+u_{i}(y)\\\\underbrace{\\\\sum_{r=2}^{n}\\\\Big{(}w_{1}-w_{r}\\\\Big{)}(n-2 )!}_{A}$$ $$=B(y)+Au_{i}(y)$$\\n\\n## Appendix G Gumbel Smoothing is a Calibrated Surrogate Loss\\n\\n_Disclaimer: While this section presents a way to build, for any $L$, a non-convex surrogate loss $L$-calibrated, it is by no means one to use in practice. Its interest is mostly theoretical as its computational complexity is generally prohibitive._\\n\\n_Disclaimer: For this section, we keep the specific case of the ranking where $\\\\mathcal{Z}=\\\\mathfrak{S}_{n}$._\\n\\nWe examine here a convolution with a well-behaved kernel to smooth $L(.,\\\\operatorname{argsort}(.))$. We obtain the following surrogate loss,\\n\\n$$\\\\Phi_{L}^{\\\\mathrm{sc}}:y,s\\\\mapsto\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname{argsort }(u-s))\\\\kappa(u)\\\\mathrm{d}u$$\\n\\nThe particular kernel $\\\\kappa$ we choose here is a Gumbel density as it can easily be reformulated in the ranking space using a Plackett-Luce model. Indeed, for any $s\\\\in\\\\mathbb{R}^{n}$ and $u\\\\sim\\\\operatorname{Gumbel}(0,1)^{n}$we have that $\\\\operatorname{argsort}(s+u)$ follows a Plackett-Luce distribution[38], allowing to express $\\\\Phi_{L}^{\\\\text{SC}}$ in closed-form (with prohibitive computational cost for large values of $n$):\\n\\n$$\\\\Phi_{L}^{\\\\text{NC}}(y,s) =\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname{argsort}(s-u))\\\\kappa(u) \\\\mathrm{d}u$$ $$=\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname{argsort}(s-u))k(u) \\\\mathrm{d}u$$ $$=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}L(y,\\\\sigma)\\\\underbrace{\\\\int_{ \\\\mathbb{R}^{n}}\\\\mathds{1}_{[\\\\operatorname{argsort}(s-u)=\\\\sigma]}k(u)\\\\mathrm{ d}u}_{\\\\mathbb{P}(\\\\sigma|e^{\\\\sigma})\\\\text{ of a Plackett-Luce model}}$$ $$=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}L(y,\\\\sigma)\\\\prod_{r=1}^{n} \\\\frac{e^{s_{\\\\sigma(r)}}}{\\\\sum_{k\\\\geq r}e^{s_{\\\\sigma(k)}}}$$\\n\\n**Proposition 11**.: _For any ranking loss $L$, the following surrogate loss $\\\\Phi_{L}^{\\\\text{NC}}$ is $L$-calibrated._\\n\\n$$\\\\Phi_{L}^{\\\\text{NC}}:y,s\\\\mapsto\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname{argsort }(u-s))\\\\kappa(u)\\\\mathrm{d}u=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}L(y,\\\\sigma)\\\\prod_{ r=1}^{n}\\\\frac{e^{s_{\\\\sigma(r)}}}{\\\\sum_{k\\\\geq r}e^{s_{\\\\sigma(k)}}}$$\\n\\nProof.: Considering the surrogate $\\\\Phi=\\\\Phi_{L}^{\\\\text{NC}}$, let $q\\\\in\\\\mathcal{P}$, to show the calibration, we show that\\n\\n1. $\\\\underline{\\\\phi}(q)=\\\\underline{\\\\ell}(q)$.\\n2. $\\\\forall s\\\\in\\\\mathbb{R}^{n}$ such that $\\\\exists\\\\nu\\\\in\\\\operatorname{argsort}(s),\\\\ell(q,\\\\nu)>\\\\underline{\\\\ell}(q)$ we have $\\\\phi(q,s)\\\\geq\\\\frac{n!-1}{n!}\\\\underline{\\\\ell}(q)+\\\\frac{1}{n!}\\\\ell(q,\\\\nu)> \\\\underline{\\\\ell}(q)$\\n\\nLet\\'s prove 1. first. We can choose $\\\\sigma\\\\in\\\\operatorname{argmin}\\\\ell(q,.)$ and $s\\\\in\\\\mathbb{R}^{n}$ such that $\\\\operatorname{argsort}(s)=\\\\{\\\\sigma\\\\}$. Taking $\\\\alpha>0$, we have $\\\\mathbb{P}(\\\\sigma|e^{\\\\alpha s})\\\\xrightarrow[\\\\alpha\\\\to\\\\infty]{}1$, meaning that $\\\\phi(q,\\\\alpha s)\\\\xrightarrow[\\\\alpha\\\\to\\\\infty]{}\\\\ell(q,\\\\sigma)=\\\\underline{\\\\ell} (q)$.\\n\\nLet\\'s prove 2. now and consider $s\\\\in\\\\mathbb{R}^{n},\\\\exists\\\\nu\\\\in\\\\operatorname{argsort}(s),\\\\ell(q,\\\\nu)> \\\\underline{\\\\ell}(q)$. Because $\\\\nu\\\\in\\\\operatorname{argsort}(s)$ it is an event of maximal probability in the Plackett-Luce model $\\\\mathbb{P}(.|e^{s})$. Hence $\\\\mathbb{P}(\\\\nu|e^{s})>\\\\frac{1}{n!}$.\\n\\n$$\\\\phi(q,s) =\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}\\\\ell(q,\\\\sigma)\\\\mathbb{P}(\\\\sigma| e^{s})$$ $$\\\\geq\\\\frac{1}{n!}\\\\ell(q,\\\\nu)+\\\\left(\\\\mathbb{P}(\\\\nu|e^{s})-\\\\frac{1}{ n!}\\\\right)\\\\ell(q,\\\\nu)+\\\\sum_{\\\\sigma\\\\neq\\\\nu}\\\\ell(q,\\\\sigma)\\\\mathbb{P}(\\\\sigma|e^{s})$$ $$\\\\geq\\\\frac{1}{n!}\\\\ell(q,\\\\nu)+\\\\left(\\\\mathbb{P}(\\\\nu|e^{s})-\\\\frac{1}{ n!}\\\\right)\\\\underline{\\\\ell}(q)+\\\\sum_{\\\\sigma\\\\neq\\\\nu}\\\\underline{\\\\ell}(q)\\\\mathbb{P}( \\\\sigma|e^{s})$$ $$\\\\geq\\\\frac{1}{n!}\\\\ell(q,\\\\nu)+\\\\frac{n!-1}{n!}\\\\underline{\\\\ell}(q)$$\\n\\n## Appendix H Local Minima of Surrogate Loss\\n\\n### Simulations to Analyze Surrogate Bad Local Valleys\\n\\nWe describe here the simulations ran to analyze the loss surface of the $\\\\Phi_{L}^{\\\\text{NC}}$. We remind its definition:\\n\\n$$\\\\Phi_{L}^{\\\\text{NC}}:y,s\\\\mapsto\\\\int_{\\\\mathbb{R}^{n}}L(y,\\\\operatorname{argsort}(u-s ))\\\\kappa(u)\\\\mathrm{d}u=\\\\sum_{\\\\sigma\\\\in\\\\mathfrak{S}_{n}}L(y,\\\\sigma)\\\\prod_{r=1}^ {n}\\\\frac{e^{s_{\\\\sigma(r)}}}{\\\\sum_{k\\\\geq r}e^{s_{\\\\sigma(k)}}}.$$\\n\\nLet us first make a few remarks on $\\\\Phi_{L}^{\\\\text{NC}}$ to explain the intention and the design of the experiments. First, about the smoothing. It is common, when smoothing a function with a convolution, to have the strength of the smoothing controlled by the bandwidth of the convolution kernel (e.g., variance of a Gaussian kernel). This is unnecessary here: the function $s\\\\mapsto L(y,\\\\operatorname{argsort}(s))$ is invariant by re-scaling of the scores $s$ (contrarily to the convolution kernel). Thus, the loss $L$ is more smoothed towards vectors $s$ of small norm (up to removing a translation) and almost not smoothed when the norm of $s$ goes towards infinity.\\n\\nSecond, because the task loss is more smoothed towards scores of low norm, initializing optimization algorithms close to 0 (all items have the same score) proves to be an empirically good heuristic.\\n\\nWe performed two experiments, similarly to the simulation for the task losses described in Section 3. Given $n$ items, we ran simulations where each simulation consists in sampling a distribution $q$ then, for each $q$, we randomly choose a starting point $s_{0}$ for the optimization and run a gradient descent on $\\\\Phi_{L}^{\\\\text{NC}}$ with a basic line search to handle the poor conditioning of $\\\\Phi_{L}^{\\\\text{NC}}$. To handle bad valleys where the infimum is not reached while avoiding numerical issues, we project the optimization to stay inside a $\\\\|.\\\\|_{2}$ ball of radius 100. This is sufficient to ensure that the local minimum reached in the $\\\\|.\\\\|_{2}$ ball generates the same ranking $\\\\sigma$ as the scores at infinity. We then compute the sub-optimality of this local valley as $\\\\eta=\\\\frac{\\\\ell(q,\\\\sigma)-\\\\min\\\\ell(q,.)}{\\\\max\\\\ell(q,.)-\\\\min\\\\ell(q,.)}$.\\n\\nSimulation 3: Distribution of sub-optimality of bad local valleys.Here, $q$ is sampled uniformly on $\\\\mathcal{Q}$ and $s_{0}$ is sampled from a $\\\\mathcal{N}(0,\\\\mathrm{Id}_{n})$, where $\\\\mathrm{Id}_{n}$ is the $n\\\\times x$ identity matrix. Figure 3 (_left and middle_) (reproduced in Figure 4) illustrates the results of the distribution of $\\\\eta$ given that $\\\\eta>0$. In these plots, the suboptimality of _bad_ local valleys are considered in the distribution.\\n\\nSimulation 4: Percentage of optimization runs stuck in bad local valleys.Here, we want to answer the question: _How often can we expect the optimization of the surrogate to be stuck in a bad local valley when the task loss has several local minima?_ To answer this, $q$ is sampled uniformly amongst distributions for which $\\\\ell(q,.)$ has at least two local minima, and $s_{0}$ is set to 0 as it proved an empirically good initialization. Figure 3 (right) (reproduced here as Figure 5) illustrates the percentage of runs that are stuck in local minima as a function of the number of items $n$ for the ERR and the AP. For $n=3$ 5-10% of the runs are stuck in local minima, and is growing with the number of items $n$ until $n=6$. Moreover, the bad local valleys of $\\\\Phi_{L}^{\\\\text{NC}}$ found, actually corresponds to rankings that are bad local minima for the task loss itself. This suggests that these local minima of the surrogate loss reflect an intrinsic difficulty of the optimization for calibrated surrogate losses rather than an artifact of $\\\\Phi_{L}^{\\\\text{NC}}$. We thus conjecture that optimization is difficult for other surrogate losses as well.\\n\\n### Preliminaries for the study of surrogate losses\\n\\nThe two subsequent sections study properties of surrogate losses calibrated with non-CEU losses. We summarize here basic results that the two sections use.\\n\\nWe first summarize the assumptions we use:\\n\\n**(A5).**: _The four following statements hold:_\\n\\n1. _(A1), (A2) and (A3) hold,_\\n\\nFigure 4: Distribution of sub-optimality of bad local valleys. Left: ERR. Right:AP.\\n\\n_\\n2. $\\\\exists q_{0}$ _such that_ $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ _is not connected,_\\n3. $\\\\Phi:\\\\mathcal{Y}\\\\times\\\\mathbb{R}^{n}\\\\to\\\\mathbb{R}$ _is such that_ $\\\\forall y\\\\in\\\\mathcal{Y}$_,_ $s\\\\mapsto\\\\Phi(y,s)$ _is_ $\\\\beta_{\\\\Phi}$_-Lipschitz and uniformly bounded by_ $B_{\\\\Phi}$_,_\\n4. $\\\\Phi$ _is_ $L$_-calibrated._\\n\\nThe assumption above implies Lipschitzness of the excess inner risk $\\\\phi(q,s)-\\\\underline{\\\\phi}(q)$ with respect to both $q\\\\in\\\\mathcal{Q}$ and $s\\\\in\\\\mathbb{R}^{n}$:\\n\\n**Lemma 48**.: _If $\\\\Phi$ is bounded and Lipschitz as defined in Assumption (A5) iii), then:_\\n\\n$$\\\\forall q,q^{\\\\prime},\\\\forall s,s^{\\\\prime},\\\\quad|\\\\phi(q,s)-\\\\underline{\\\\phi}(q) -\\\\phi(q^{\\\\prime},s^{\\\\prime})-\\\\underline{\\\\phi}(q^{\\\\prime})|\\\\leq 2\\\\|q-q^{ \\\\prime}\\\\|_{1}B_{\\\\Phi}+\\\\beta_{\\\\Phi}\\\\|s-s^{\\\\prime}\\\\|_{2}$$\\n\\nProof.: First consider $A=\\\\phi(q,s)-\\\\underline{\\\\phi}(q)-\\\\phi(q^{\\\\prime},s^{\\\\prime})-\\\\underline{\\\\phi} (q^{\\\\prime})$. Let $\\\\epsilon>0$ and $s^{*}$ such that $\\\\underline{\\\\phi}(q)\\\\geq\\\\phi(q,s^{*})-\\\\epsilon$. We have\\n\\n$$A \\\\leq\\\\phi(q,s)-\\\\phi(q,s^{*})+\\\\epsilon-\\\\phi(q^{\\\\prime},s)+\\\\phi(q^{ \\\\prime},s^{*})$$ $$\\\\leq\\\\|q-q^{\\\\prime}\\\\|_{1}\\\\max_{y}|\\\\Phi(y,s)-\\\\Phi(y,s^{*})|+\\\\phi(q^ {\\\\prime},s)-\\\\phi(q^{\\\\prime},s)+\\\\epsilon$$ $$\\\\leq 2\\\\|q-q^{\\\\prime}\\\\|_{1}B_{\\\\Phi}+\\\\beta_{\\\\Phi}\\\\|s-s^{\\\\prime}\\\\| _{2}+\\\\epsilon\\\\,,$$\\n\\nwhere the first inequality holds since $\\\\phi(q^{\\\\prime},s^{*})\\\\geq\\\\underline{\\\\phi}(q^{\\\\prime})$ and the second from Holder\\'s inequality. Following the same steps, we find the same upper bound for $\\\\phi(q^{\\\\prime},s^{*})-\\\\underline{\\\\phi}(q^{\\\\prime})-\\\\phi(q,s)-\\\\underline{\\\\phi }(q)$. The final result is obtained in the limit $\\\\epsilon\\\\to 0$. \\n\\nThe following result give the main implications of calibration of $\\\\Phi$. It are extended version of Lemma 44 for surrogate losses instead of task losses. It gives basic properties of segments joining a distribution $q_{0}$ with a disconnected argmin, with distributions of type $q^{(z)}$ where $z\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.,.)$. The lemma has several parts that are used in different proofs.\\n\\n**Lemma 49**.: _Under Assumption (A5), let $q_{0}\\\\in\\\\mathcal{Q}$ such that $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ is disconnected._\\n\\n1. _Let_ $z,z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$ _such that_ $z$ _and_ $z^{\\\\prime}$ _are not in the same connected components of_ $\\\\operatorname{argmin}\\\\ell(q_{0},.)$_. Let_ $\\\\bar{q}$ _be defined as in Lemma_ 44_. Then_ $\\\\exists\\\\delta_{0}$ _such that for every continuous function_ $f:[0,1]\\\\to\\\\mathbb{R}^{n}$ _such that_ $\\\\{z,z^{\\\\prime}\\\\}\\\\subseteq\\\\operatorname{pred}(f([0,1]))$_, we have:_ $$\\\\max_{\\\\alpha\\\\in[0,1]}\\\\left(\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}( \\\\bar{q}(\\\\alpha))\\\\right)\\\\geq\\\\delta.$$\\n2. _Let_ $\\\\bar{q}:[0,1]\\\\to\\\\mathcal{Q}$ _defined by:_ $$\\\\forall\\\\alpha\\\\in[0,1],\\\\ \\\\ \\\\bar{q}(\\\\alpha)=(1-\\\\alpha)q_{0}+\\\\alpha q^{(z)}.$$\\n\\nFigure 5: Percentage of optimizations runs on $\\\\Phi_{L}^{\\\\text{sc}}$ that end up stuck in a bad local valley over distribution for which the task loss as several local minima.\\n\\n_Let_ $G_{0}=\\\\min_{z\\\\not\\\\in\\\\operatorname{argmin}_{q_{0}}}\\\\min_{q\\\\in\\\\{\\\\vartheta_{0},q^{(z) }\\\\}}\\\\big{(}\\\\ell(q,z)-\\\\underline{\\\\ell}(q)\\\\big{)}$_. Then_ $G_{0}>0$_._ _Moreover, let_ $\\\\eta_{0}\\\\in(0,\\\\frac{G_{0}}{2\\\\|L\\\\|_{\\\\infty}})$_. Then_ $\\\\exists\\\\delta_{0}>0$ _such that:_ $$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta),\\\\;\\\\; \\\\;\\\\forall s\\\\in\\\\operatorname{lev}_{\\\\delta_{0}}\\\\phi(q,.),\\\\operatorname{pred}(s) \\\\subseteq\\\\operatorname{argmin}\\\\ell(q_{0},.).$$\\n* _For any_ $q\\\\in\\\\mathcal{Q}$ _such that_ $\\\\operatorname{argmin}\\\\ell(q,.)=\\\\{z\\\\}$_, there is_ $\\\\delta_{0}>0$ _and_ $\\\\eta_{0}>0$ _such that:_ $$\\\\forall s\\\\in\\\\operatorname{lev}_{\\\\delta_{0}}\\\\phi(q,.),\\\\forall q^{\\\\prime}\\\\in \\\\mathcal{B}_{1}(q,\\\\eta_{0}),\\\\;\\\\;\\\\operatorname{pred}(s)=\\\\{z\\\\}.$$\\n\\nProof.: Point _i)_. Using uniform calibration (Theorem 30, let $\\\\epsilon_{0}$ be defined as in Lemma 44, and let $\\\\delta_{0}$ such that $\\\\forall q\\\\in\\\\mathcal{Q},s\\\\in\\\\operatorname{lev}_{\\\\delta_{0}}\\\\ell(q,.), \\\\operatorname{pred}(s)\\\\subseteq\\\\operatorname{lev}_{\\\\epsilon_{0}}\\\\ell(q,.)$. Combining with Lemma 44, we have:\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall s\\\\in\\\\operatorname{lev}_{\\\\delta_{0}}\\\\phi(\\\\bar{q}( \\\\alpha),.),\\\\operatorname{pred}(s)\\\\subseteq\\\\operatorname{argmin}\\\\ell(q_{0},.). \\\\tag{7}$$\\n\\nSince connectedness is preserved by continuous functions, any continuous function $f:[0,1]\\\\to\\\\mathbb{R}^{n}$ such that $\\\\{z,z^{\\\\prime}\\\\}\\\\subseteq\\\\operatorname{pred}(f([0,1]))$ must have $\\\\operatorname{pred}(f([0,1]))\\\\not\\\\subseteq\\\\operatorname{argmin}\\\\ell(q_{0},.)$. Using (7), this means $\\\\exists\\\\alpha,f(\\\\alpha)\\\\not\\\\in\\\\operatorname{lev}_{\\\\delta_{0}}\\\\phi(\\\\bar{q}( \\\\alpha),.)$, which is equivalent to the desired result.\\n\\nPoint _ii)_. With the same arguments as Lemma 44, for every $\\\\alpha\\\\in[0,1]$, if $z\\\\in\\\\operatorname{lev}_{G_{0}}\\\\ell(\\\\bar{q}(\\\\alpha),.)$ then $z\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$.\\n\\nNow, let us take $\\\\eta\\\\in(0,\\\\frac{G_{0}}{2\\\\|L\\\\|_{\\\\infty}})$. For every $\\\\alpha\\\\in[0,1]$, the Lipschitzness of $q\\\\mapsto\\\\ell(q,z)-\\\\underline{\\\\ell}(q)$, which follows from similar arguments as Lemma 48, gives:\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta), \\\\forall z\\\\not\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.),\\\\;\\\\;\\\\ell(q,z)-\\\\underline{ \\\\ell}(q)\\\\geq G_{0}-2\\\\eta\\\\|L\\\\|_{\\\\infty}>0 \\\\tag{8}$$\\n\\nBy uniform calibration (Theorem 30), there is $\\\\delta_{0}>0$ such that $\\\\forall\\\\delta\\\\in(0,\\\\delta_{0}),\\\\forall q\\\\in\\\\mathcal{Q},\\\\forall s\\\\in \\\\operatorname{lev}_{\\\\delta}\\\\phi(q,.),\\\\operatorname{pred}(s)\\\\subseteq \\\\operatorname{lev}_{G_{0}-2\\\\eta\\\\|L\\\\|_{\\\\infty}}\\\\ell(q,.)$. With this choice of $\\\\delta_{0}$, using (8) gives the result.\\n\\nPoint _iii)_. The proof is exactly the same a before: Let $\\\\epsilon=\\\\min_{z^{\\\\prime}\\\\notin z}(\\\\ell(q,z^{\\\\prime})-\\\\underline{\\\\ell}(z))$. We have $\\\\epsilon>0$ by the assumption $\\\\operatorname{argmin}\\\\ell(q,.)=\\\\{z\\\\}$. Let $\\\\eta_{0}\\\\in(0,\\\\frac{\\\\epsilon}{2\\\\|L\\\\|_{\\\\infty}})$. We have\\n\\n$$\\\\forall q^{\\\\prime}\\\\in\\\\mathcal{B}_{1}(q,\\\\eta),\\\\min_{z^{\\\\prime}\\\\neq z}(\\\\ell(q^{ \\\\prime},z^{\\\\prime})-\\\\underline{\\\\ell}(z))\\\\geq\\\\epsilon-2\\\\eta\\\\|L\\\\|_{\\\\infty}>0.$$\\n\\nAnd thus $\\\\forall q^{\\\\prime}\\\\in\\\\mathcal{B}_{1}(q,\\\\eta),\\\\{z\\\\}=\\\\operatorname{argmin}\\\\ell (q,.)^{\\\\prime}$. By uniform calibration (Th. 30), using $\\\\delta_{0}>0$ such that $\\\\forall q^{\\\\prime}\\\\in\\\\mathcal{Q},\\\\forall s\\\\in\\\\operatorname{lev}_{\\\\delta_{0}} \\\\phi(q^{\\\\prime},.),s\\\\in\\\\operatorname{lev}_{\\\\epsilon-2\\\\eta\\\\|L\\\\|_{\\\\infty}}\\\\ell( q^{\\\\prime},.)$ gives the result. \\n\\n### Lower bound on approximation error of surrogate loss by Lipschitz functions\\n\\nThe proposition below is a stronger version of Proposition 13. It makes it precise that the conditional distribution over $\\\\mathcal{Y}$ is infinitely many times differentiable with bounded derivatives at any order (denoted $x\\\\mapsto P(.|x)\\\\in\\\\mathcal{W}^{\\\\infty}([0,1]^{\\\\mathcal{Q}})$), and that there is a single point in $[0,1]$ on which the argmin of $\\\\ell$ is not a singleton (and thus disconnected) (condition $|\\\\{\\\\alpha\\\\in[0,1]:|\\\\operatorname{argmin}\\\\ell(p_{Y|X=\\\\alpha}|,.)|>1\\\\}|=1$). This makes sure that the distibution is not too peculiar.\\n\\n**Proposition 50**.: _Under Assumption (A5), there is a probability measure $P$ over $[0,1]\\\\times\\\\mathcal{Y}$ where the marginal distribution over $[0,1]$, and $x\\\\mapsto P(.|x)\\\\in\\\\mathcal{W}^{\\\\infty}([0,1]^{\\\\mathcal{Q}})$ such that $|\\\\{\\\\alpha\\\\in[0,1]:|\\\\operatorname{argmin}\\\\ell(p_{Y|X=\\\\alpha}|,.)|>1\\\\}|=1$, and constants $c,c^{\\\\prime}>0$, such that for all $\\\\beta\\\\geq 0$_\\n\\n$$\\\\inf_{\\\\begin{subarray}{c}f:[0,1]\\\\to\\\\mathbb{R}^{n}\\\\\\\\ f:\\\\beta-Lipschitz\\\\end{subarray}}\\\\mathcal{R}_{\\\\Phi,P}(f)-\\\\inf_{g:\\\\mathcal{X}\\\\to \\\\mathbb{R}^{n}}\\\\mathcal{R}_{\\\\Phi,P}(g)\\\\geq\\\\min\\\\big{(}c^{\\\\prime},\\\\frac{c}{8B_{ \\\\Phi}+\\\\beta_{\\\\Phi}\\\\beta}\\\\big{)}.$$\\n\\nProof.: Let $q_{0}$ such that $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ is disconnected and let $z,z^{\\\\prime}\\\\in\\\\operatorname{argmin}\\\\ell(q_{0},.)$ such that $z$ and $z^{\\\\prime}$ belong to two different connected components of $\\\\operatorname{argmin}\\\\ell(q_{0},.)$. Let $\\\\bar{q}(\\\\alpha)$ defined as in Lemma 44. Clearly, $\\\\bar{q}\\\\in\\\\mathcal{W}^{\\\\infty}([0,1]^{|\\\\mathcal{Y}|})$, i.e., $\\\\bar{q}$ has bounded derivatives of any order. Also there is only one value of $\\\\alpha$ ($\\\\alpha=\\\\frac{1}{2}$) for which $\\\\operatorname{argmin}\\\\ell(\\\\bar{q}(\\\\alpha),.)$ is not a singleton.\\n\\nLet $f$ be $\\\\beta$-Lipschitz. We consider two cases:* If $\\\\{z,z^{\\\\prime}\\\\}\\\\subset\\\\mathrm{pred}(f([0,1]))$:  First, we show that $\\\\alpha\\\\mapsto\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q}(\\\\alpha))$ is Lipschitz. Notice that $\\\\left\\\\|\\\\bar{q}(\\\\alpha)-\\\\bar{q}(\\\\alpha^{\\\\prime})\\\\right\\\\|_{1}\\\\leq 2|\\\\alpha- \\\\alpha^{\\\\prime}|$ when $\\\\alpha$ and $\\\\alpha^{\\\\prime}$ are in the same help-segment of $[0,1]$. When they are not, say $\\\\alpha\\\\in[0,\\\\frac{1}{2})$ and $\\\\alpha^{\\\\prime}\\\\in[\\\\frac{1}{2},1]$, we have $$\\\\left\\\\|\\\\bar{q}(\\\\alpha)-\\\\bar{q}(\\\\alpha^{\\\\prime})\\\\right\\\\|_{1} \\\\leq\\\\left\\\\|(2-\\\\alpha-\\\\alpha^{\\\\prime})q_{0}\\\\right\\\\|_{1}+2\\\\left\\\\|( \\\\frac{1}{2}-\\\\alpha)q^{(z^{\\\\prime})}\\\\right\\\\|_{1}$$ $$\\\\leq 4\\\\big{(}\\\\big{\\\\lfloor}\\\\frac{1}{2}-\\\\alpha|+\\\\big{\\\\lfloor} \\\\frac{1}{2}-\\\\alpha^{\\\\prime}|\\\\big{)}=4(\\\\alpha^{\\\\prime}-\\\\alpha).$$ We thus have that $\\\\alpha\\\\mapsto\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q}(\\\\alpha))$ is $8B_{\\\\Phi}+\\\\beta_{\\\\Phi}\\\\beta$-Lipschitz by Lemma 48. Moreover, by point _i_) of Lemma 49, choose $\\\\delta>0$ such that $\\\\max_{\\\\alpha\\\\in[0,1]}\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q} (\\\\alpha))\\\\geq\\\\delta$. The minimum of the integral $\\\\int_{[0,1]}\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q}(\\\\alpha))d\\\\alpha$ is attained for a trapezoidal or triangle shape, with the maximum attained at a bound (0 or 1), of length $\\\\min(1,\\\\frac{\\\\delta}{8B_{\\\\Phi}+\\\\beta_{\\\\Phi}\\\\beta})$ (because the domain is of length 1). We can conclude $$\\\\mathcal{R}_{\\\\Phi,P}(f)-\\\\inf_{g:\\\\mathcal{X}\\\\to\\\\mathbb{R}^{n}}\\\\mathcal{R}_{\\\\Phi,P}(g)=\\\\int_{[0,1]}\\\\big{(}\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}( \\\\bar{q}(\\\\alpha))\\\\big{)}d\\\\alpha\\\\geq\\\\frac{1}{2}\\\\min(\\\\delta,\\\\frac{\\\\delta^{2}}{8B _{\\\\Phi}+\\\\beta_{\\\\Phi}\\\\beta})$$\\n* If $\\\\{z,z^{\\\\prime}\\\\}\\\\not\\\\subseteq\\\\mathrm{pred}(f([0,1]))$: Assume for instance $z\\\\not\\\\in\\\\mathrm{pred}(f([0,1]))$. Let $\\\\epsilon=\\\\min_{z^{\\\\prime\\\\prime}\\\\neq z}\\\\ell(q^{(z)},z^{\\\\prime\\\\prime})-\\\\underline {\\\\ell}(q^{(z)})$. For $\\\\alpha<1/4$, using $\\\\underline{\\\\ell}(\\\\bar{q}(\\\\alpha))=\\\\ell(\\\\bar{q}(\\\\alpha),z)$, we have: $$\\\\ell(\\\\bar{q}(\\\\alpha),\\\\mathrm{pred}(f(\\\\alpha)))-\\\\underline{\\\\ell}(\\\\bar{q}( \\\\alpha))\\\\geq(1-2\\\\alpha)\\\\big{(}\\\\ell(q^{(z)},\\\\mathrm{pred}(f(\\\\alpha)))-\\\\ell(q^{( z)},z)\\\\big{)}\\\\geq\\\\frac{1}{2}\\\\epsilon.$$ By uniform calibration (Theorem 30), there is $\\\\delta^{\\\\prime}_{z}$ such that $\\\\forall\\\\alpha\\\\in[0,\\\\frac{1}{4})$, $\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q}(\\\\alpha))\\\\geq\\\\delta^{ \\\\prime}_{z}$ (or we would have $\\\\mathrm{pred}(f(\\\\alpha))=\\\\{z\\\\}=\\\\mathrm{argmin}\\\\,\\\\ell(\\\\bar{q}(\\\\alpha),.)$). We then have $$\\\\int_{\\\\alpha\\\\in[0,1]}\\\\big{(}\\\\phi(\\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}( \\\\bar{q}(\\\\alpha))\\\\big{)}d\\\\alpha\\\\geq\\\\int_{\\\\alpha\\\\in[0,\\\\frac{1}{4}]}\\\\big{(}\\\\phi( \\\\bar{q}(\\\\alpha),f(\\\\alpha))-\\\\underline{\\\\phi}(\\\\bar{q}(\\\\alpha))\\\\big{)}d\\\\alpha \\\\geq\\\\frac{1}{4}\\\\delta^{\\\\prime}_{z}$$ Similarly if $z^{\\\\prime}\\\\not\\\\in\\\\mathrm{pred}(f([0,1]))$.\\n\\nTaking $c^{\\\\prime}=\\\\min(\\\\frac{1}{4}\\\\delta^{\\\\prime}_{z},\\\\frac{1}{4}\\\\delta^{\\\\prime}_{z^{ \\\\prime}},\\\\frac{\\\\delta}{2})$ and $c=\\\\frac{\\\\delta^{2}}{2}$ gives the result. \\n\\n### Local valleys of calibrated surrogate losses\\n\\nWe now prove that when the task loss does not have connected argmins, the set of distributions in $\\\\mathcal{Q}$ for which $\\\\phi(q,.)$ has bad local valleys has non-zero measure.\\n\\n**Proposition 51**.: _Under Assumption (A5), the set $\\\\{q\\\\in\\\\mathcal{Q}:\\\\phi(q,.)$ has bad local valleys$\\\\}$ has non-zero Lebesgue measure._\\n\\nProof.: Let $q_{0}$ be a distribution such that $\\\\mathrm{argmin}\\\\,\\\\ell(q_{0},.)$ is disconnected, and let $z\\\\in\\\\mathrm{argmin}\\\\,\\\\ell(q_{0},.)$. Let $\\\\bar{q}$, $\\\\delta_{0}$ and $\\\\eta_{0}$ be defined as in point _ii_) of Lemma 49, i.e., $\\\\bar{q}$ is a segment between $q_{0}$ and $q^{(z)}$.\\n\\nFirst let $\\\\delta_{1}\\\\in(0,\\\\delta_{0})$, such that $\\\\mathrm{pred}(\\\\mathrm{lev}_{\\\\delta_{1}}\\\\phi(q_{0},.))=\\\\mathrm{argmin}\\\\,\\\\ell(q_{0 },.)$. Such a $\\\\delta_{1}$ exists using the equality of argmins for calibrated surrogate losses (Theorem 28).\\n\\nLet $s^{\\\\prime}\\\\in\\\\mathrm{lev}_{\\\\delta_{1}}\\\\phi(q_{0},.)$ such that $\\\\mathrm{pred}(s^{\\\\prime})$ is not in the same connected component of $\\\\mathrm{argmin}\\\\,\\\\ell(q_{0},.)$ as $z$. Using the Lipschitz property of $q\\\\mapsto\\\\phi(q,s)-\\\\underline{\\\\phi}(q)$ (Lemma 48), we have:\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta),\\\\;\\\\; \\\\phi(q,s^{\\\\prime})-\\\\underline{\\\\phi}(q)<\\\\delta_{1}+(2\\\\eta+4\\\\alpha)B_{\\\\Phi}. \\\\tag{9}$$\\n\\nMoreover, for every $\\\\alpha>0$, using point _iii_) of Lemma 49 with $q:=\\\\bar{q}(\\\\alpha)$, we can find $\\\\delta_{\\\\alpha}\\\\in(0,\\\\delta_{1})$ and $\\\\eta_{\\\\alpha}>0$ such that:\\n\\n$$\\\\forall s\\\\in\\\\mathrm{lev}_{\\\\delta_{\\\\alpha}}\\\\phi(\\\\bar{q}(\\\\alpha),.),\\\\;\\\\;\\\\forall q \\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta_{\\\\alpha}),\\\\mathrm{pred}(s)=\\\\{z\\\\}.$$Thus, for any $s^{\\\\prime}\\\\in\\\\operatorname{lev}_{\\\\delta_{1}}\\\\phi(q_{0},.)$ such that $\\\\operatorname{pred}(s^{\\\\prime})$ is not in the same connected component as $z$ is not in $\\\\operatorname{lev}_{\\\\delta_{\\\\alpha}}\\\\ell(\\\\bar{q}(\\\\alpha),.)$, we have\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta_{ \\\\alpha}),\\\\ \\\\ \\\\ \\\\phi(q,s^{\\\\prime})-\\\\underline{\\\\phi}(q)\\\\geq\\\\delta_{\\\\alpha}+(2\\\\eta_{\\\\alpha}+4 \\\\alpha)B_{\\\\Phi}. \\\\tag{10}$$\\n\\nFinally, since $z$ and $z^{\\\\prime}$ are not connected in $\\\\operatorname{argmin}\\\\ell(q_{0},.)$, for every continuous function $f:[0,1]\\\\to\\\\mathbb{R}^{n}$ with $z\\\\in\\\\operatorname{pred}(f(0))$ and $z^{\\\\prime}\\\\in\\\\operatorname{pred}(f(1))$, we must have $\\\\operatorname{pred}(f([0,1]))\\\\not\\\\subseteq\\\\operatorname{argmin}\\\\ell(q_{0},.)$ (since continuous functions preserve connectedness), and thus, by our choice of $\\\\delta_{0}$ from Lemma 49, we have:\\n\\n$$\\\\forall\\\\alpha\\\\in[0,1],\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta_{0}) \\\\max_{t\\\\in[0,1]}\\\\big{(}\\\\phi(q,f(t))-\\\\underline{\\\\phi}(q)\\\\big{)}\\\\geq\\\\delta_{0}. \\\\tag{11}$$\\n\\nThe proof finishes by taking $\\\\alpha=\\\\frac{\\\\delta_{0}-\\\\delta_{1}}{12B_{\\\\Phi}}$, $\\\\eta=\\\\min(\\\\eta_{\\\\alpha},\\\\eta_{0},\\\\alpha)$. With these values, we have that $\\\\forall q\\\\in\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta)$:\\n\\n1. Using (9), there exists $s^{\\\\prime}\\\\in\\\\operatorname{lev}_{\\\\delta_{1}+(2\\\\eta+4\\\\alpha)B_{\\\\Phi}}\\\\phi(q,.)$ such that $\\\\operatorname{pred}(s^{\\\\prime})$ is in $\\\\operatorname{argmin}\\\\ell(q_{0},.)$ but not connected to $z$ in $\\\\operatorname{argmin}\\\\ell(q_{0},.)$.\\n2. Using $\\\\delta_{1}+(2\\\\eta+4\\\\alpha)B_{\\\\Phi}<\\\\delta_{0}$ and (11), $s^{\\\\prime}$ above and its connected component $C$ in $\\\\operatorname{lev}_{\\\\delta_{1}+(2\\\\eta+4\\\\alpha)B_{\\\\Phi}}\\\\phi(q,.)$ are not connected to any $s$ such that $\\\\operatorname{pred}(s)=\\\\{z\\\\}$. Thus, the connected component $C$ is a local valley of $\\\\ell(q,.)$,\\n3. Using (10), the infimum over $C$ is suboptimal. Thus $C$ is a bad local valley.\\n\\nThus, the measure of bad local valleys is at least the measure of $\\\\mathcal{B}_{1}(\\\\bar{q}(\\\\alpha),\\\\eta)$, and is thus $>0$.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_P-mZ2QYSgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}